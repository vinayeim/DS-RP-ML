{
    "collab_server" : "",
    "contents" : "#Data Science With R ##########\n#'-------------------------'#####\n\n#1.\tIntroduction to R Programming#####\n#'--------------------------------------'\n\n# Author         : Selva Prabhakaran\n# Course Name    : Introduction to R Programming Course\n# Section        : Installation and Setup\n# Compiled On    : 2nd July 2016.\n# Course URL     : https://www.packtpub.com/application-development/introduction-r-programming-video\n# Info \n# - Author website : http://r-statistics.co and http://rstatistics.net\n# - Follow         : www.twitter.com/r_programming\n# - Youtube        : http://youtube.com/user/thelearnr \n# - Email          : selva86@gmail.com\n\n\n#################################################################################\n#What You Will Learn                                                            #\n#\t\t\t\t\t\t\t\t\t\t\t\t                                                        #\n#Create and master the manipulation of vectors, lists, dataframes, and matrices #\n#Write conditional control structures, and debug and handle errors for efficient error handling\t#\n#See how to use the apply family of functions and write functions used within the apply function#\n#Handle dates using lubridate and manipulate strings with stringr package       #\n#Melt, reshape, aggregate, and cross-tabulate with dcast from dataframes\t\t\t  #\n#Make and customize various types of charts in base graphics for exceptional data representation#\n#Perform univariate and bivariate analysis and do statistical tests\t\t\t\t      #\n#Work with databases without having to write SQL using the dplyr package\t\t\t  #\n#Write readable and expressive code using pipes from magrittr and dplyrâs verbs\t#\n#Perform efficient, high-speed data munging with data.table\t\t\t\t\t            #\n#Work on a full-scale data analysis / data munging project\t\t\t\t\t            #\n#################################################################################\n\n#1.1.\tINSTALLATION AND SETUP#####\n\n#R Base Software\n#R Studio\n#R Toos software\n\n#1.2.\tThe Course Overview#####\n\n1 + 1\nprint(\"Hello World\")\n5 / 2 * 3\n10 %% 3  # get the remainder \n10 %/% 3  # get the quotient\n\n#?print\n#??print \n\n#1.3.\tInstalling R#####\n\n# Goto the 'https://cran.r-project.org/' Download the R-Base software.\n\n#1.4.\tInstalling RStudio#####\n\n# Goto the 'https://www.rstudio.com/' Download the RStudio software.\n\n#1.5.\tInstalling Packages#####\n\n#Source of R Packages\n\n#CRAN\n\n\"https://cran.r-project.org/\"\n\n#GitHub\n\n\"github.com\"\n\n#Bioconductor\n\n\"https://bioconductor.org/packages/\"\n\n######\n#2.\tWORKING WITH VECTORS#####\n#-----------------------------\n\n#2.1.\tData Types and Data Structures#####\n\na <- \"abc\"\n\n\"abc\" -> a\n\nclass(a)\n\na <- 10\n\nclass(a)\n\na <- as.character(10)\n\nclass(a)\n\n\n#------------------------------#\n#       Data Types             #\n#------------------------------#\n# Number    - Whole numbers    #\n# Integer   - Integers         #\n# Character - Strings          #\n#Factor - Categorical variables#\n# Logical - T or F             #\n#------------------------------#\n\n\n#------------------------------------------------#\n#           Data Structures                      #\n#------------------------------------------------#\n#       Homogeneous     |   Hetrogeneous         #\n#------------------------------------------------#\n#           Vector      |     Data frame         #\n#           Matrix      |     List               #\n#------------------------------------------------#\n\n#2.2.\tVectors#####\n\na <- numeric(10)\na\n\nb <- character(10)\nb\n\na[10] <- 10\na\n\na[2] <- 2\na\n\na <- c(1,2,3,4,5,6,7,8,9,10)\na\n\na <- 1:10\na\nb <- 11:15\nb\n\nout <- c(a,b)\nout\n\nclass(out)\n\nc <- rep(1, 5)\nc\n\nd <- rep(c(1,2), 5)\nd\n\ne <- seq(1, 10, by=2)\ne\n\nf <- seq(1, 10, length=20)\nf\n\ng <- seq(1, by=2, length=10)\ng\n\nout[5]\n\nout[-5]\n\nout[c(5, 6)]\n\nfac <- factor(c(\"red\", \"blue\", \"red\", \"blue\", \"blue\", \"blue\"))\nfac\n\nlevels(fac) \n\nlevels(fac) <- c(\"purple\", 'blue')\nlevels(fac)\n\nlength(a)\n\nnames(a) <- c(\"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\")\nnames(a)\na\n\n#Challenge\n#---------\n\n\"Get the first item and the secound last item in the vector 'd'\"\n\na <- 1:10\nd <- c(a, 2*a, 3*a)\na\nd\nd[c(1, length(d)-1)]\n\n\n#2.3.\tRandom Numbers, Rounding, and Binning#####\n\na <- runif(10)\na\n\na <- runif(10, 100, 150)\na\n\nrunif(10, 100, 150)\nrunif(10, 100, 150)\n\n\nset.seed(100)\nrunif(10, 100, 150)\nset.seed(100)\nrunif(10, 100, 150)\n\nset.seed(100)\na <- runif(10, 100, 150)\nb <- trunc(a)\nb\nb <- floor(a)\nb\nc <- ceiling(a)\nc\nd <- round(a)\nd\nd2 <- round(a, 2)\nd2\n\nbins <- c(100, 110, 120, 130, 140, 150)\nbins\n\nc1 <- cut(d2, bins)\nc1\nclass(c1)\n\nbin2 <- pretty(d2, 5)\nbin2  \n\nc2 <- cut(d2, bin2)\nc2\n\n#Challenge\n#---------\n\n\"Generate 25 Random numbers between 1 to 10 with the seed 100 and remove all the decimals \nafter the decimal point\"\n\nset.seed(100)\na <- runif(25, 1, 10)\ntrunc(a)\n\n\n#2.4.\tMissing Values#####\n\na <- 21:30\na\n\na[9] <- NA\na\n\nanyNA(a)\n\nis.na(a)  \n\na==NA\n\na[!is.na(a)]\n\na[is.na(a)]\n\na[is.na(a)] <- mean(a, na.rm=T)\n\n\n#2.5.\tThe which() Operator#####\n\nset.seed(100)\na <- round(runif(25, 1, 100))\na\n\npos <- which(a > 25)\npos\n\na[pos]\n\npos <- which(a%%3==0 & a%%4==0)\na[pos]\n\npos <- which(a%%3==0 | a%%4==0)\npos\na[pos]\n\na[a%%3==0 | a%%4==0]\n\nset.seed(100)\na <- round(runif(25, 1, 100))\n\npos <- which(a%%2==0)\na[pos]\n\n\n###\n#3.\tR ESSENTIALS#####\n#'--------------------'\n\n#3.1.\tLists#####\n\nset.seed(100)\nx <- letters\ny <- 1:26\nz <- round(runif(26, 1, 26))\nx\ny\nz\n\nm <- list(x, y, z)\nm\nclass(m)\n\nm[1]\n\nm[[1]]\n\nm[[1]][4]\n\nm1 <- unlist(m)\n\nm1[4]\n\nn <- list(x,m)\n\nn\n\nclass(n)\n\nn[[2]][[1]][4]\n\n#Challenge\n#---------\n\n\"Create the list 'm' shown below. From list'm' get the number from object 'y' \nthat corresponding to the letter 'r' in the 'x' object\" \n\nset.seed(100)\nx <- letters\ny <- 1:26\nz <- round(runif(26, 1, 26))\nm <- list(x, y, z)\n\nm\n\npos <- which(m[[1]] == 'r')\nm[[2]][pos]\n\n#3.2.\tSet Operations#####\n\nx <- c(1:7)\ny <- c(4:10)\n\nout <- x %in% y\nout\n\nx[out]\n\nunique(y)\n\nunion(x, y)\nintersect(x, y)\n\nsetdiff(y, x)\n\n#Challenge\n#---------\n\n\"Find the items that are not common between x and y using R\"\n\nx <- c(1:7)\ny <- c(4:10)\n\nsetdiff(union(x,y), intersect(x,y))\n\n#3.3.\tSampling and Sorting#####\n\na <- 1:100\n\nset.seed(100)\nsample(a, 10)\na\n\nset.seed(100)\nb <- sample(a, 30, replace=T)\nb\n\nsort(b)\n\nsort(b, decreasing=T)\n\no <- order(b)\no\n\nb[o]\n\nrev(b[o])\n\n#Challenge\n#---------\n\n\"Sample 20 numbers with replacement from the numbers 1 to 100 using a seed value of 100.\nthen sort it in decreasing order using the order() function.\"\n\nset.seed(100)\na <- sample(1:100, 20, replace = T)\na\n\na[order(-a)]\n\n#3.4.\tCheck Conditions#####\n\na <- 2\nb <- 5\n\nif(a < b){\n  print(\"less\")\n}else{\n  print(\"more\")\n}\n\na <- 1:10\n\nb <- ifelse(a%%2 == 0, 'even', 'odd')\nb\n\nifelse(a%%2 == 0, {}, {})\n\nifelse(sum(a)<5, x <- 'more', x <- 'less')\nx\n\n#Challenge\n#-----------\n\n\"Create a character vector of same length as 'a' that has 'yes' in positions where the \nnumber is a multipul of three and 'no' otherwise\" \n\nset.seed(100)\na <- round(runif(15, 1, 100))\n\na1 <- ifelse(a%%3 == 0, 'Yes', 'No')\na1\n\n#3.5.\tFor Loops#####\n\nfor(i in 1:10){\n  print(i)\n}\n\nfor(i in 1:10){\n  if(i ==5){\n    next\n  }\n  print(i)\n}\n\nfor(i in 1:10){\n  if(i ==5){\n    break\n  }\n  print(i)\n}\n\n\nsystem.time({\n  out <- numeric()\n  for(i in 1:10000){\n    out <- c(out, 1)\n  }\n})\n\nsystem.time({\n  out <- numeric(10000)\n  for(i in 1:10000){\n    out[i] <- i\n  }\n})\n\n\n######\n#4.\tDATAFRAMES AND MATRICES#####\n#--------------------------------\n#4.1.\tDataframes#####\n\nset.seed(100)\na <- c(1:17, NA)\nb <- c(\"a\", \"b\", \"c\", \"d\", NA, \"f\", \"a\", \"b\", \"c\", \"d\", NA, \"f\", \"a\", \"b\", \"c\", \"d\", NA, \"f\")\nc <- factor(sample(c(\"red\", \"blue\", \"green\"), 18, replace=T))\na\nb\nc\n\ndf1 <- data.frame(v1=a, v2=b, v3=c)\ndf1\n\nclass(df1)\n\ndim(df1) \n\nnrow(df1)\n\nncol(df1) \n\nrownames(df1)\n\ncolnames(df1)\n\ncolnames(df1) <- c(\"A\", \"B\", \"C\")\n\ndf1\n\nhead(df1)\n\nhead(df1, 3)\n\n\ntail(df1)\n\n\nView(df1)\n\n\nmat <- t(df1)\nmat\nclass(mat)\n\nsummary(df1)\n\nstr(df1)\n\noptions(stringsAsFactors=F)\n\ndf1 <- data.frame(v1=a, v2=b, v3=c)\nstr(df1)\n\noptions(stringsAsFactors=T)\n\ndf1 <- data.frame(v1=a, v2=b, v3=c)\ndf1\nstr(df1)  \n\ndf1$v2 <- as.character(df1$v2) # or\ndf1$v2 <- as.character(df1[, 2]) #or\ndf1$v2 <- as.character(df1[, \"v2\"])\nstr(df1)\n\ndf1[, c(1, 2)] # or\ndf1[, c(\"v1\", \"v2\")]\n\ndf1[, 1, 2]  # \n\ndf1[c(5:10), c(1,2)]  # \n\ndf1[which(df1$v1 > 5), c(1, 2)]\n\nindex <- which(df1$v1 > 5)  # \nindex\n\ndf1[index, c(1, 2)]\n\ndf1[df1$v1 > 5, c(1, 2)]\n\nsubset(df1, select=c(1, 2), subset=v1>5)\n\ndf2 <- na.omit(df1)\ndf2\n\nindex <- !is.na(df1$v1) \nindex\ndf1[index, ]  # \n\ndf1[!is.na(df1$v1) , ]\n\n#Challenge\n\"Remove all missing values and keep only those rows that contains either 'green' or 'blue'\nin column 'c' \"\n\na <- c(1:17, NA)\nb <- c(\"a\", \"b\", \"c\", \"d\", NA, \"f\", \"a\", \"b\", \"c\", \"d\", NA, \"f\", \"a\", \"b\", \"c\", \"d\", NA, \"f\")\nc <- factor(sample(c(\"red\", \"blue\", \"green\"), 18, replace=T))\ndf1 <- data.frame(a, b, c, stringsAsFactors = F)\n\n# Answer :\n\nindex <- df1$c %in% c(\"green\", \"blue\")\ndf2 <- df1[index, ]\nna.omit(df2)\n\n\n#4.2.\tImporting and Exporting Data#####\n\nlibrary(help=\"datasets\") # and you can see the description of this package that displays all the dataset names.\n\nhead(mtcars)\ngetwd()\n\nwrite.csv(mtcars, \"mtcars.CSV\", row.names = F)\n\ngetwd()\n\ndir()\n\npath <- file.path(getwd(), \"Datafiles/mtcars.csv\")\npath\n\nwrite.csv(mtcars, path, row.names = T)\n\npath1 <- file.path(getwd(), \"Datafiles/mtcars.RDS\")\n\nsaveRDS(mtcars, path1, compress = T)\n\nx <- readRDS(path1)\n\nhead(x)\n\nfpath <- file.path(getwd(), \"/Datafiles/mtcars.Rdata\")\n\nsave(mtcars, cars, file = fpath)\n\nrm(list = ls())\nrm(imagepath)\nload(fpath)\n\nfile.remove(imagepath)\n\nall_obj <- file.path(getwd(), \"Datafiles/all_obj.Rdata\")\nsave.image(all_obj)\n\nrm(list = ls())\nall_obj <- file.path(getwd(), \"Datafiles/all_obj.Rdata\")\nload(all_obj)\n\npath <- file.path(getwd(), \"Datafiles/mtcars.csv\")\npath\n\nmydata <- read.table(path, sep = ',')\n\nhead(mydata)\n\n# If the file is pipe delimeted set the sep was '|'\nmydata <- read.table(path, sep = '|', header = T)\n\n# If the file is tab delimeted set the sep was '/t'\nmydata <- read.table(path, sep = '/t', header = T)\n\nmydata <- read.csv(path)\n\nhead(mydata)\n\ninstall.packages('readxl')\nlibrary(readxl)\n\n\nxlpath <- file.path(getwd(),'Datafiles/mtcars.xlsx')\n\nread_excel(xlpath, sheet = 1)\n# or if you think the sheet number could change, you can enter the sheetname.\nexceldata <- read_excel(xlpath, sheet = 'mtcars')\n\nexceldata\n\nhead(exceldata)\n\n#Install 'haven' package to read the sas files.\ninstall.packages('haven')\nlibrary(haven)\n\nsasdata <- read_sas(\"http://crn.cancer.gov/resources/ctcodes-procedures.sas7bdat\")\nsasdata\n\nsasfile <- file.path(getwd(), 'Datafiles/ctcodes-procedures.csv')\nwrite.csv(sasdata, sasfile, row.names = T)\n\n# For SPSS files you can use the read_sav or read_por\nspss_dat <- read_sav('filepath.sav')\nspss_dat1 <- read_por('filepath.por')\n# Likewise, for stata files, use the read_dta function\nstata_dat <- read_dta(\"filepath.dta\")\n\n\n#4.3.\tMatrices and Frequency Tables#####\n\nobject.size(mtcars)\n\nmtcars_mat <- as.matrix(mtcars)\n\nobject.size(mtcars_mat)\n\nm1 <- matrix(1:100, nrow = 10)\nm1\n\nm1 <- matrix(1:100, nrow = 10, byrow = T)\nm1\n\ndim(m1)\n\nm1[, c(1,2)]\nm1[1:5, c(1,2)]\n\ndiag(m1)\nrowSums(m1)\nrowMeans(m1)\ncolSums(m1)\ncolMeans(m1)\ncumsum(m1)\n\n#Creating frequency table\ntable(mtcars$cyl)\n# And if you want to create counts against 2 vectors, \n# specify the vector that goes in row first follow by the column vector\ntb <- table(mtcars$cyl, mtcars$vs)\ntb\nclass(tb)\n\nas.data.frame(tb)\n\ntb_m <- as.matrix(tb)\ntb_m\nclass(tb_m)\n\ntb_bf <- as.data.frame.matrix(tb)\nclass(tb_bf)\ntb_bf\n\ntb_bf[1]\n\n#Challenge\n\n\"Get the counts of 'mtcars$gear' for each value of 'mtcars$cyl'\nthen compute how many car makes have 4 gares of 6 cylenders\"\n\ntb1 <- table(mtcars$cyl, mtcars$gear)\ntb1\ntb2 <- as.data.frame.matrix(tb1)\ntb2\ntb2[row.names(tb2) == '6', '4']\n\n\n#4.4.\tMerging Dataframes#####\n\nset.seed(100)\ndf1 = data.frame(FruitId = c(1:10),\n                 Subject = sample(c(\"Apple\", \"Banana\", \"Mango\"), 10, replace=T))\n\ndf2 = data.frame(FruitNum = c(2, 4, 6, 12),\n                 Cuisine = sample(c(\"Chinese\", \"Italian\", \"Mexican\"), 4, replace=T))\ndf1\ndf2\n\nmerge(df1, df2, by.x = 'FruitId', by.y = 'FruitNum') # by.x refers to the column name in df1 and by.y refers to the column name in df2.\n\nmerge(df1, df2, by.x=\"FruitId\",by.y=\"FruitNum\", all=T) # by settin all=T, we can retain all the observations from both dataframes.\n# And if you want to retain only the observations in df1, then do:\nmerge(df1, df2, by.x=\"FruitId\",by.y=\"FruitNum\", all.x=T)\n# Else, if you want to retain observations from right dataframe df2, then:\nmerge(df1, df2, by.x=\"FruitId\",by.y=\"FruitNum\", all.y=T)\n\n\n#4.5.\tAggregation#####\n\nhead(ChickWeight)\n# This dataset shows the weight of chicks that were given different diets at various points in time.\n\naggregate(weight ~ Diet, data=ChickWeight, FUN=mean)\n# You can read it as, Get the weight as a function of Diet from the dataset ChickWeight. \n# By setting the FUN to `mean`, you get the mean weights for each of the diets.\n\n# Suppose if you want the sum instead of mean, you can write:\naggregate(weight ~ Diet, data=ChickWeight, FUN=sum)\n\n# Of if you are interested in how many chicks were given each diet, use the length function instead.\naggregate(weight ~ Diet, data=ChickWeight, FUN=length)\n\n# Also, it is possible to aggregate by a combination of variables\naggregate(weight ~ Chick + Diet, data=ChickWeight, FUN=mean)\n\naggregate(weight ~ Time, data=ChickWeight, FUN=mean)\n\na <- aggregate(weight ~ Chick + Diet, data=ChickWeight, FUN=mean)\n\na[order(a$Chick, a$Diet), ]\nstr(a)\n\na$Chick <- as.numeric(as.character(a$Chick))\nstr(a)\n\na[order(a$Chick, a$Diet), ]\n\n#challenge\n\n\"Form 'Chickweight' built in database, get the mean chick wight at each time period \ndoes the mean weight exceed 150\"\n\nexceed.150 <- aggregate(weight ~ Time, data=ChickWeight, FUN=mean)\nstr(exceed.150)\nexceed.150[exceed.150$weight > 150, ][1, ]\n\n\n#4.6.\tMelting and Cross Tabulations with dcast()#####\n\n#install.packages(\"reshape2\", repos=\"http://cran.rstudio.com\")\ninstall.packages(c(\"reshape2\", \"ggplot2\"))\nlibrary(reshape2)\nlibrary(ggplot2)\n\n\nhead(economics)\nmelted <- melt(economics, id=\"date\")\n# This brings the dataset to the level of \"date\" variable. All other features are stacked \n# under the 'value' variable and the column called 'variable' shows what feature the \n# respective values belong.\nhead(melted)\n\nhead(french_fries)\n\n\na <- dcast(french_fries, treatment ~ subject, value.var=\"potato\", fun.aggregate=mean) \na\n# The dcast takes in the dataframe as the first argument. Then a formula to define what goes \n# in the rows and columns of the output, then the 'value.var' argument takes which variable \n# to use to fill up the table's values and finally the function by which 'value.var' has to be \n# aggregated.\n# This is a very useful function often used in exploratory data analysis.\n\na <- dcast(french_fries, treatment ~ subject, value.var = \"potato\",\n           fun.aggregate = function(x){mean(x, na.rm = T)})\na\n\n#Challenge\n\n\"From reshape2::french_fries datasets, what is the median rating of 'rancid' variable for \nall subjects of each values of 'treatment'\"\n\na <- dcast(french_fries, treatment ~ subject, value.var=\"rancid\", fun.aggregate=median) \na\n\na <- dcast(french_fries, treatment ~ subject, value.var=\"rancid\", \n           fun.aggregate = function(x){median(x, na.rm = T)}) \na\n\n\n######\n#5.\tCORE PROGRAMMING#####\n#--------------------------------\n\n#5.1.\tDates#####\n\n#Date Classes\n# - Date\n# - POSIXct and \n# - POSIXlt\n\ninstall.packages(\"lubridate\", repos = \"http://cran.rstudio.com\")\nlibrary(lubridate)\nSys.time()\nclass(Sys.time())\n\nd1 <- \"2014-11-28\"\nd2 <- \"2-19-11\"\nd3 <- \"04 April 2016\"\nd4 <- \"October 19th 2016\"\nd5 <- \"2016-06-25 04:30:00\"\n\nclass(d1)\nclass(d2)\nclass(d3)\nclass(d4)\nclass(d5)\n\nd1 <- ymd(d1, tz=\"UTC\")\nd2 <- mdy(d2, tz=\"UTC\")\nd3 <- dmy(d3, tz=\"UTC\")\nd4 <- mdy(d4, tz=\"UTC\")\nd5 <- ymd_hms(d5, tz=\"UTC\")\n\nclass(d1)\nclass(d2)\nclass(d3)\nclass(d4)\nclass(d5)\n\n\nday(d1)  # the 28th day of month\nmonth(d1)  # falls on the 11th month of the year.\nmonths(d1)  # if you want month in character.\nweek(d1)  # falls on the 48th week\nweekdays(d1)  # is a friday.\nquarter(d1)  # falls on the 4th quarter\nquarters(d1)\n\n# If you wish to find out the number of days in the month of `d1`, use the Hmisc package.\ninstall.packages('Hmisc')\nlibrary(Hmisc)\nmonthDays(d1)\n# or to find the number of days in the year\nlibrary(Hmisc)\nyearDays(d1)\n\n\nd <- d1 - d2\nd\n# That gives a text string, whose class is:\nclass(d) # difftime\n# If you want just the number, do:\nas.numeric(d)  # and you will get just the number.\n# Using the difftime function, you can find the time difference in any time unit.\ndifftime(d1, d2, unit=\"sec\") \ndifftime(d1, d2, unit=\"week\") \n\n#Challenge\n\n\"Pass this date string into POSIXct object '04/1981/20'\nFind out what day of week it was and how many days where there in that month\"\n\nlibrary(lubridate)\nd1 <- \"04/1981/20\"\nd1 <- myd(d1, tz=\"UTC\")\nweekdays(d1)  #  so that was a monday.\nHmisc::monthDays(d1)  # and the month had 30 days.\n\n\n#5.2.\tString Manipulation#####\n\npaste(\"txt1\", \"txt2\")\n# By default, the strings will have a space between them. If you dont want that space, then explicitly set the 'sep' argument.\npaste(\"txt1\", \"txt2\", sep=\"\")\n# likewise you can set any character or word as separator.\npaste(\"txt1\", \"txt2\", \"txt3\", sep=\"------------\")\n\npaste(c(\"a\", \"b\"),  c(\"d\", \"e\"), sep=\"\")\n# if you want further collapse them into a single string, then set the collapse argument\npaste(c(\"a\", \"b\"),  c(\"d\", \"e\"), sep=\"\", collapse=\"-\")\n\na <- \"The lord of the rings and harry potter are the most fascinating tales ever told in English. 12.3.\"\n\nb <- \"@r_programming  \"\n\n# Lets load up the stringr package\n# install.packages(\"stringr\", repos = \"http://cran.rstudio.com\")\nlibrary(stringr)\nstr_count(b)\n\n# To upper case\nstr_to_upper(a)\n# To lower case\nstr_to_lower(a)\n# to title case\nstr_to_title(a)\n\nstr_split(a, \" \")\n\nunlist(str_split(a, \" \"))\n\nsubstr(b, 2, 13)\n\nword(a, c(2:4))\n\nstr_detect(b, \"[:punct:]\")\n\nstr_detect(b, \"@\")\n\nstr_replace_all(a, \"t\", \"L\")\n\nstr_trim(b, side=\"both\")\n\nstr_replace_all(b, \"[:punct:]\", \"\")\n\nstr_replace_all(a, \"[0-9]\", \"\")\nstr_replace_all(a, \"[a-z A-Z]\", \"\")\n\n#Challenge\n\"Extract the names of the two books and put it in title case form this phrase\"\n\na <- \"The lord of the rings and harry potter are the most fascinating tales ever told in English.\"\n\ny <- word(a, c(1:8))\ny\nbook1 <- paste(y[1:5], collapse=\" \")\nstr_to_title(book1)\nbook2 <- paste(y[7:8], collapse=\" \")\nstr_to_title(book2)\n\n\n#5.3.\tFunctions#####\n\n#Tips of best Practices\n# - Use of missing()\n# - Use of stop()\n# - Use of stopifnot()\n# - Use of invisible()\n\nhypotenuse <- function(side1, side2){\n  return(sqrt(side1^2 + side2^2))\n}\n\nhypotenuse(10, 15)  \n#or\nhypotenuse(side1=10, side2=15)\n\nhypotenuse(10)\n\nhypotenuse <- function(side1=10, side2=15){\n  return(sqrt(side1^2 + side2^2))\n}\n\n# so if one of the arguments is missing, it will still hold the default value.\nhypotenuse(12)\n\nhypotenuse <- function(side1=NULL, side2=NULL){\n  if(any(is.null(side1), is.null(side2))){\n    stop(\"one of the sides is missing!\")\n  }\n  return(sqrt(side1^2 + side2^2))\n}\nhypotenuse(12, 13) # This works fine. \n# But it any of the arguments is missing. it throws the error message.\nhypotenuse(12)\n\nhypotenuse <- function(side1, side2){\n  if(missing(side2) | missing(side1)){\n    stop(\"one of the sides is missing!\")\n  }\n  return(sqrt(side1^2 + side2^2))\n}\nhypotenuse(12)\n# That stops the function if one of the arguments was missing.\n# or you can use stopifnot.\nhypotenuse <- function(side1, side2){\n  stopifnot(!missing(side2), !missing(side1))\n  return(sqrt(side1^2 + side2^2))\n}\nhypotenuse(12, 12)\n\nhypotenuse <- function(side1, side2, round=F, ...){\n  if(missing(side2) | missing(side1)){\n    stop(\"one of the sides is missing!\")\n  }\n  if(round){\n    return(round(sqrt(side1^2 + side2^2), ...))\n  }else{\n    return(sqrt(side1^2 + side2^2))\n  }\n}\n\nhypotenuse(side1=12, side2=15)\nhypotenuse(side1=12, side2=15, round=T)  # by default it rounded to 0 digits.\n# If you want to define the number of rounding digits you can pass that as well.\nhypotenuse(side1=12, side2=15, round=T, digits=2)  # Great!\n\n'+'(1, 2)\n'*'(2, 3)\n\n'%***%' <- function(x, y){\n  return(sqrt(x^2 + y^2))\n}\n# Now we have a symbol function for hypotenuse.\n10%***%10\n\n#Challenge\n\n\"Create your symbol function that returns the minimum of two numbers\"\n\n'%mi%' <- function(x, y){\n  return(min(x, y))\n}\n\n\n#5.4.\tDebugging and Error Handling#####\n\n# 3 Ways to debug errors\n\ntraceback()\ndebugonce()\noptions(error = utils::recover)\n\n#How to handle unavoidable error using\n\ntry()\ntryCatch()\n\nhypo <- function(side1=10, side2=15){\n  if(missing(side2)){\n    stop(\"Somethings wrong!\")\n  }else if(missing(side1)){\n    stop(\"Somethings wrong!\")\n  }\n  return(sqrt(side1^2 + side2^2))\n}\n\nhypotenuse <- function(side1, side2){\n  hypo(side1, side2)\n}\n\nhypotenuse(10)\n\ntraceback()\n\ndebugonce(hypotenuse)\nhypotenuse(10)\n# that shows the error happens within 'hypo' function. So lets debug that once\ndebugonce(hypo)\nhypotenuse(10)\n\noptions(error = utils::recover)\nhypotenuse(10)\n# You can switch off this by setting\noptions(error = NULL)\n\nout <- try(hypotenuse(10), silent=T) # by setting silent=TRUE, no error will be printed on the console. \nout # Now, the object out contains information about the error msg.\nclass(out)  # and the class of such an object will always be try-error.\nout <- try(hypotenuse(10, 10), silent=T) # but in case the code did return a legit value, the output will have that\nout\n\ntryCatch({hypotenuse(10)}, error=function(x){print(x)}, finally=print(\"Finally, print this!\"))\n\ntryCatch({hypotenuse(10, 10)}, error=function(x){print(x)}, finally=print(\"Finally, print this!\"))\n\n\n#5.5.\tFast Loops with apply()#####\n\n#How to write loops using apply()\n# - Row wise\n# - Column wise\n\n\"How the 'apply' function will work\n\nThe first argument is the data frame witch you want to loop through.\nThe secound argument you tell 'apply' to loop 'column' wise or 'row' wise\n- for 'row' wise write '1' or 'column' wise write '2'.\nThe thired argument is function that specify to execute on each itaraction.\nAfter this we have to write code for this function separately\"\n\n#Example: 1 for Row wise execution\napply(mtcars, 1, FUN = ss)\n#Example: 2 for Column wise execution\napply(mtcars, 2, FUN = ss)\n\noutput <- numeric(nrow(mtcars))  # initialise the output vector\nfor(i in 1:nrow(mtcars)){\n  output[i] <- (mtcars[i, 1])^2 + (mtcars[i, 2])^2 + mtcars[i, 3]^2\n}\noutput\n\n\nss <- function(mpg, cyl, disp){\n  (mpg^2) + (cyl^2) + (disp^2)\n}\napply(mtcars, 1, FUN = ss)\n\n## \nss <- function(x){\n  \n}\n\n##\nss <- function(x){\n  print(x)\n}\ndebugonce(ss)\napply(mtcars, 1, FUN = ss)\n\nls()\nx\n\nx[1] #or\nx['mpg']\n\n##\nss <- function(x){\n  return((x['mpg']^2) + (x['cyl']^2) + (x['disp']^2) )\n}\n\n##\nss <- function(x){\n  return((x[1]^2) + (x[2]^2) + (x[3]^2) )\n}\noutput1 <- apply(mtcars, 1, FUN = ss)\noutput1\n\n## Challenge\n\n\"Get the sum of squares row wise for the last 3 columns of 'mtcars' built-in dataset\"\n\nlast3cols <- function(x)\n{\n  return((x[9]^2) + (x[10]^2) + (x[11]^2))\n}\n\noutput2 <- apply(mtcars, 2, FUN = last3cols)\noutput2\n\n#5.6.\tFast Loops with sapply(), lapply() and vapply()#####\n\n## lapply()\nx <- list(a1 = 1:10, a2 = 100:105, a3 = 10:20)\nout_l <- lapply(x, mean)\nout_l\nclass(out_l)\n\n## sapply()\nout_s <- sapply(x, mean)\nout_s\nclass(out_s)\n\nsapply(mtcars, class)\n\n## vapply()\nvapply(mtcars[, 1:4], mean, FUN.VALUE = numeric(1))\n\n\n######\n#6.\tMAKING PLOTS WITH BASE GRAPHICS#####\n#----------------------------------------\n\n#6.1.\tCreating and Customizing an R Plot#####\n\nhead(mtcars)\n\nplot(mtcars$wt, mtcars$mpg)\n\n##\nabline(lm(mpg ~ wt, data=mtcars))\n\n##\nplot(mtcars$wt, mtcars$mpg, main=\"Mpg vs Wt\")\n\n##\nplot(mtcars$wt, mtcars$mpg, main=\"Mpg vs Wt\", xlab=\"Weight (1000 lbs)\", ylab=\"Miles per gallon\")\n\n##\nplot(mtcars$wt, mtcars$mpg, main=\"Mpg vs Wt\", xlab=\"Weight (1000 lbs)\", ylab=\"Miles per gallon\", pch=4)\n\n##\nplot(mtcars$wt, mtcars$mpg, main=\"Mpg vs Wt\", xlab=\"Weight (1000 lbs)\", ylab=\"Miles per gallon\", \n     pch=4, col=\"red\")\n\n##\nplot(mtcars$wt, mtcars$mpg, main=\"Mpg vs Wt\", xlab=\"Weight (1000 lbs)\", ylab=\"Miles per gallon\", pch=4, col=\"red\", col.main=\"blue\", col.lab=\"brown\", col.axis=\"orange\")\n\n##\ncolors()\n\n##\nplot(mtcars$wt, mtcars$mpg, main=\"Mpg vs Wt\", xlab=\"Weight (1000 lbs)\", ylab=\"Miles per gallon\", pch=4, col=\"red\", col.main=\"blue\", col.lab=\"brown\", col.axis=\"orange\", cex=1.5, cex.axis=1.5, cex.lab=1.5)\n\n##\ntext(mtcars$wt, mtcars$mpg+1, labels=rownames(mtcars), cex=0.75)\n\n##\ntext(4.5, 30, labels=\"Car Makes\", cex=2, col=\"red\")\n\n##\nmtext(\"Right side margin\", side=4, cex=1.5, col=\"green4\")\n\n##\nlegend(\"bottomright\", inset=.01, title=\"mgp vs wt\", legend=c(\"mgp vs wt\"), fill=\"blue\", horiz=TRUE) \ngrid()\n\n##\npng(\"myplot.png\", height=300, width=300)\nplot(mtcars$wt, mtcars$mpg, main=\"Mpg vs Wt\", xlab=\"Weight (1000 lbs)\", ylab=\"Miles per gallon\", pch=4, col=\"red\", col.main=\"blue\", col.lab=\"brown\", col.axis=\"orange\", cex=1.5, cex.axis=1.5, cex.lab=1.5)\ndev.off() # turn off device\n\n##\npdf(\"mypdfplot.pdf\", height=6, width=6)\nplot(mtcars$hp, mtcars$mpg, main=\"Mpg vs hp\", xlab=\"HP\", ylab=\"Miles per gallon\", pch=4, col=\"red\", col.main=\"blue\", col.lab=\"brown\", col.axis=\"orange\", cex=1.5, cex.axis=1.5, cex.lab=1.5)\ndev.off() # turn off device\n\n\n#6.2.\tDrawing Plots with 2 Y Axes#####\n\nset.seed(1001)\nyear <- 1901:1920\npopulation <- c(1:20)\ngdp <- sample(40:50, 20, replace = T)\n\n## \nplot.new()\npar(mar=c(5, 4, 4, 5) + 0.1)\nplot(year, population, type=\"b\", ylim=c(0,21), lwd=2, main=\"Multiple Y axes\", col=\"green3\", pch=16)\n\n## \npar(new=T)  \nplot(year, gdp, type=\"b\", axes=F, xlab=\"\", ylab=\"\", lwd=2, col=\"red\", pch=15, ylim=c(30,60))\naxis(4, ylim=c(30,60))  # draw second y axis.\nmtext(\"gdp\", side=4, line=3)\nlegend(\"bottomright\", inset=0.05, col=c(\"green3\",\"red\"),lty=1,legend=c(\"pop\",\"gdp\"))\n\n\n#6.3.\tMultiplots and Custom Layouts#####\n\npar(mfrow=c(2, 2))\nplot(mtcars$wt, mtcars$mpg, xlab=\"Car Weight\", ylab=\"Miles Per Gallon\", main=\"wt\")\nplot(mtcars$qsec, mtcars$mpg, xlab=\"Qsec\", ylab=\"Miles Per Gallon\", main=\"qsec\")\nplot(mtcars$hp, mtcars$mpg, xlab=\"Hp\", ylab=\"Miles Per Gallon\", main=\"hp\")\nplot(mtcars$disp, mtcars$mpg, xlab=\"Disp\", ylab=\"Miles Per Gallon\", main=\"disp\")\n\npar(mfrow=c(1,1))\npar(mar=c(4,4,4,1))\nplot.new()  # draw and empty plot.\n\n##\npar(fig=c(0, 0.8, 0.0, 0.8), new=TRUE, bg=\"wheat1\")\nplot(mtcars$wt, mtcars$mpg, xlab=\"Car Weight\", ylab=\"Miles Per Gallon\")\n\n##\npar(fig=c(0,0.8,0.55,1), new=TRUE)\n\n##\nboxplot(mtcars$wt, horizontal=TRUE, axes=FALSE)\n\n##\npar(fig=c(0.65,1,0,0.8),new=TRUE)\nboxplot(mtcars$mpg, axes=FALSE)\nmtext(\"Enhanced Scatterplot\", side=3, outer=TRUE, line=-3)\n\n#6.4.\tCreating Basic Graph Types#####\n\nhist(mtcars$mpg)\n\n## \nbrks <- seq(0, max(mtcars$mpg + 10), 5)\nbrks\nhist(mtcars$mpg, breaks=brks)\n\n## \nhist(mtcars$mpg, breaks=3)\n\n##\nplot(density(mtcars$mpg))\n\n##\nmpg_new <- cut(mtcars$mpg, breaks = c(0, 10, 15, 20, 25, 30, 35))\ntbl <- table(mpg_new)\ntbl\n\n## \nbarplot(tbl, main = \"MPG\")\n\n## \ndotchart(mtcars$mpg,labels=row.names(mtcars), cex=.7, main=\"MPG\", xlab=\"MPG\")\n\n##\nmpg <- mtcars$mpg[order(mtcars$mpg)]\ncar_names <- row.names(mtcars)[order(mtcars$mpg)]\ncyl <- mtcars$cyl[order(mtcars$mpg)]\n\ndotchart(mpg,labels=car_names, cex=.7, main=\"MPG\", xlab=\"MPG\", groups = cyl, color = cyl-2)\n\n## \nboxplot(mtcars$wt)\n\n## \nboxplot(mpg ~ cyl, data=mtcars)\n\n######\n#7.\tSTATISTICAL INFERENCE#####\n#-----------------------------\n\n#7.1.\tUnivariate Analysis#####\n\n#Note\n\n#Why univariate,the purpose and when is it done.\n#What is done as a part of univariate analysis.\n#Outliers\n#How to do univariate analysis on.\n\n# 1 - Continuous variables (or numeric).\n# 2 - Categorical variables.\n## - Binary\n## - Multiclass \n\n#For example if it is a 'Continuous variables (or numeric)' variable two types of statistics are studied\n#Witch measures the central tendenci.\n\n#\"Mean\"\n# - Which is the average of data point.\n\n#Median\n# - Which is the data point that allows the midway afterthe datapoint are sorted.\n\n#Mode (most sutabul for Categorical variables)\n# - Which is the most frequently accoring value in the data.\n\n\n# Measures of Dispersion:\n\n# - Standard Deviation\n## - Witch Quantfies the amount of varaction in the data\n# - Variance\n## - Squer of the Standard Deviation\n# - Range\n## - Min and Max Values\n# - Quantile\n## - Percentail Points\n# - Coefficient of Variation\n## - SD/Mean\n# - Inter Quartile Range.\n## - Defference B/T 75th and 25th Quantails.\n\nhead(mtcars)\nsummary(mtcars$mpg)\n\n# Measures of Dispersion:\n\n# - Standard Deviation\nsd(mtcars$mpg) \n\n# - Variance\nvar(mtcars$mpg)\n\n# - Range\nrange(mtcars$mpg)\n\n# - Quantile\nquantile(mtcars$mpg, c(.1, .25, 0.5, 0.75, 0.9))\n\n# - Coefficient of Variation\nsd(mtcars$mpg)/mean(mtcars$mpg)\n\n# - Inter Quartile Range.\nIQR(mtcars$mpg)\n\nhist(mtcars$mpg, col = 'orange')\n\n#Categorical variables\ntable(mtcars$cyl)\nhist(mtcars$cyl, col = 'orange')\n\n# Challenge\n\n\"Compute all the univariate statistics we studied in this video for the \n'dist' variable in 'cars' dataset\"\n\nsd(cars$dist)\nvar(cars$dist)\nrange(cars$dist)\nquantile(cars$dist, c(.1, .25, 0.5, 0.75, 0.9))\nsd(cars$dist)/mean(cars$dist)\nIQR(cars$dist)\nhist(cars$dist, col = 'red')\n\n\n#7.2.\tNormal Distribution, Central Limit Theorem, and Confidence Intervals#####\n\n# Parametric and non-Parametric\n# Normal distribution\n# Central limit theorem\n# Confidence intervals\n\nhist(mtcars$mpg)\nhist(mtcars$mpg, freq = F)\n\n\n# Upper end point\n10 + (1.96 * 2 / sqrt(50))\n\n# Lower end point\n10 - (1.96 * 2 / sqrt(50))\n\n#Challenge\n\n140 + ((1.96 * 10) / sqrt(30))\n140 - ((1.96 * 10) / sqrt(30))\n\n\n#7.3.\tCorrelation and Covariance#####\n\ncor(mtcars$mpg, mtcars$wt)\ncov(mtcars$mpg, mtcars$wt)\n\nplot(mtcars$mpg, mtcars$wt)\n\n#Challenge\n\n\"-Compute the correlation b/w 'mpg' and 'hp' variables in 'mtcars' datasets\n-Determaine the nature of relactionship b/w the two variables\"\n\ncor(mtcars$mpg, mtcars$hp)\n\n\n#7.4.\tChi-sq Statistic#####\n## - BiVarient Analysis on Catogarical Variables\n\nhead(mtcars)\nchisq.test(mtcars$cyl , mtcars$carb)\nsummary(table(mtcars$cyl , mtcars$carb))\n\n# Challenge\n\"Find out if the 'cyl' and 'carb' variables from the mtcars dataset are \nstatistically dependent using the chi-squared test\"\n\nchisq.test(mtcars$cyl , mtcars$carb)\n\n#7.5.\tANOVA#####\n\nhead(InsectSprays)\nboxplot(count ~ spray, data = InsectSprays)\n\n#General formula : aov(cont ~ cat1 + cat2, data = mydata)\naov.mod <- aov(count ~ spray, data=InsectSprays)\nsummary(aov.mod)\n\n# Challenge\n\n\"Find out if 'mpg' and 'cyl' variables from 'mtcars' dataset are dependent on each other\nWhat is the F-Ratio\"\n\naov.mod <- aov(mpg ~ cyl, data=mtcars)\nsummary(aov.mod)\n\n\n#7.6.\tStatistical Tests#####\n\nset.seed(100)\nx <- rnorm(20, mean = 10, sd = 2)\nx\n\nt.test(x, mu = 10)\n##\n\nset.seed(100)\nx <- rnorm(20, mean = 10, sd = 2)\ny <- rnorm(20, mean = 11, sd = 2)\n\nt.test(x, y)\n\nx <- c(0.80, 0.83, 1.04, 1.45, 1.38)\ny <- c(1.15, 0.88, 0.90, 0.74, 1.21)\n\n##\nmean(x)\nmean(y)\n\n##\nwilcox.test(x, y, alternative = \"g\")\n\n######\n#8.\tR VERY OWN PROJECT#####\n#--------------------------\n\n#8.1.\tProject 1 â Data Munging and Summarizing#####\n\ninstall.packages('hflights')\ndata('hflights', package = 'hflights')\n\n'or'\n\nhflights <- read.csv(\"https://raw.githubusercontent.com/selva86/datasets/master/hflights.csv\")\nhead(hflights)\n\n#Challenges\n\n\"1. Which 'UniqueCarrier' caused the most number of departure delays?\nIn number and in percentage\"\n\n## 1.\nhead(hflights$DepDelay)\nhflights$DepDelay <- head(ifelse(hflights$DepDelay > 0, 1, 0))\nhflights$DepDelay\n## 2.\ncarrier_dealy_count <- table(hflights$UniqueCarrier, hflights$DepDelay)\ncarrier_dealy_count\nclass(carrier_dealy_count)\n## 3.\ncarrier_dealy_count <- as.data.frame.matrix(carrier_dealy_count)\nclass(carrier_dealy_count)\ncarrier_dealy_count\n## 4.\n# This gives us the number of flights that we delayed and those that were on-time. Now lets find out the percentage delayed.\n\n# If you have an invalid column name like we have here, you can access it by enclosing the column name within backticks.\ncarrier_dealy_count$perc_dealy <- carrier_dealy_count$`1`/\n  as.numeric(carrier_dealy_count$`0` + carrier_dealy_count$`1`)\ncarrier_dealy_count\n## 5.\n# In absolute terms, the carrier that had most late departures is\nrow.names(carrier_dealy_count)[which.max(carrier_dealy_count$`1`)]\nxE\n# While, the carrier that had most late departures in percentage is\nrow.names(carrier_dealy_count)[which.max(carrier_dealy_count$perc_dealy)]\nas\n\n\"2. On what date was the highest traffic recorded in terms of number of departures?\"\n\n## 1. \ninstall.packages('lubridate')\nlibrary(lubridate)\n## 2.\nhflights$date <- paste(hflights$Year,\n                       hflights$Month, \n                       hflights$DayofMonth, sep = '-')\n\nhead(hflights$date)\nclass(hflights$date)\nhflights$date <- ymd(hflights$date, tz = 'UTC')\nclass(hflights$date)\n## 3.\ndepartures <- table(hflights$date)\ndepartures\n## 4.\ndepartures[departures == max(departures)]\n\n\n\"3. If the 'TailNum' variable represents a unique vehicle, \nwhich carrier owns the largest fleet?\"\n\n## 1.\ncarrier_tailnum <- as.data.frame.matrix(table(hflights$UniqueCarrier,\n                                              hflights$TailNum))\ncarrier_tailnum\n# We want to count the unique tail nums, so if the value is greater than 0, I will make it 1 so we count that tailnum only once.\n## 2.\nfleet_count <- as.data.frame(ifelse(carrier_tailnum > 0, 1, 0))\nfleet_count\n## 3.\n# now, rowsums should give the number of tail nums for the uniquecarriers.\nrowSums(fleet_count)\nWN\n# The carrier WN with a fleet size of 567 seems to have the largest fleet size operating out of IAH and HOU airports.\n\n\n\"4. Derive a dataset that is unique at 'FlightNum' variable and has the following derived variables:\n- Average Elapsed Time\n- Average Air Time\n- Mean Arrival Delay\n- Mean Departure delay\n- Mean distance of flight\n- Mean Taxi In time\n- Mean Taxi Out time\n- Total Number of flights\n- Number of times of cancelled\n- Proportion of cancellation\"\n\n## 1.\nhead(hflights)\nfn1 <- aggregate(cbind(ActualElapsedTime, AirTime, ArrDelay, DepDelay, Distance\n                       , TaxiIn, TaxiOut) ~ FlightNum, data = hflights, \n                 FUN = function(x)round(mean(x, na.rm = T),2))\nhead(fn1)\n\n## 2.\n# For computing number of flights, we can choose any arbitrary variable, because, we just need to count the Flightnums.\nfn2 <- aggregate(Year ~ FlightNum, data = hflights, FUN = length)\ncolnames(fn2) <- c(\"FlightNum\", \"NumFlights\")\nhead(fn2)\n\n## 3.\n# For cancelled flights, you can choose only the those rows where Cancelled variable equals 1.\n# Method 1\nfn3 <- aggregate(Cancelled ~ FlightNum, data = hflights[hflights$Cancelled == 1, ]\n                 , FUN = length)\nhead(fn3)\n\n## 4.\n# Finally lets Merge\nfn1 <- merge(fn1, fn2, by = 'FlightNum', all.x = T)\nfn1 <- merge(fn1, fn3, by = 'FlightNum', all.x = T)\n# and replace the missing values with zero.\nfn1$Cancelled[is.na(fn1$Cancelled)] <- 0\n\nhead(fn1)\n\n## 5.\n# Percentage Cancelled\nfn1$Perc_Cancelled <- round(fn1$Cancelled / fn1$NumFlights, 4) * 100\nhead(fn1)\n\nfn1[order(-fn1$perc_cancelled), ]\n\n\"5. Create a cross tabulation for 'Average Departure Delay Time' for each 'Month of the Year'\nagainst each UniqueCarrier.\"\n\nlibrary(reshape2)\n\ncarrier_month_delays <- dcast(UniqueCarrier ~ Month, data=hflights, \n                              value.var=\"DepDelay\", \n                              fun.aggregate=function(x)round(mean(x, na.rm=T), 2))\nhead(carrier_month_delays)\n\n# Lets find out which carriers departed on time and which had long delays?\ncarrier_meandelay <- rowMeans(carrier_month_delays[, -1], na.rm=T)\n\n# As you can notice, certain carriers have long delays while some take off on time. Lets get their names.\ncarrier_month_delays$UniqueCarrier[which(carrier_meandelay < 2)]\ncarrier_month_delays$UniqueCarrier[which(carrier_meandelay > 10)]\n\n# Lets compute the average monthly departure delay time\ncolMeans(carrier_month_delays[, -1], na.rm=T)\n\n\n#8.2.\tProject 2 â Visualization with Base Graphics#####\n\n#Challenges\n\n# 1. Make a histogram of the number of departures for every month of 2011, which month saw the highest traffic?\n\ndata(hflights, package = 'hflights')\n\nhist(hflights$Month)\n\nbarplot(table(hflights$Month), main = 'Monthly Departures', \n        ylab = 'Number of Departures', xlab = 'Month')\n\n# 2. Make line chart of number of departures for every month of 2011, with a separate line for each origin.\n\npar(mar = c(4, 4, 3, 1))\norigin_departures <- table(hflights$Origin, hflights$Month)\norigin_departures\nclass(origin_departures)\n\n# now convert it to a data.frame\norigin_departures <- as.data.frame.matrix(origin_departures)\norigin_departures\nclass(origin_departures)\n\n# First lets make the line chart for \"HOU\"\nplot(as.numeric(origin_departures[1, ]), type=\"b\", \n     ylim = c(0, 20000), \n     main=\"Total Number of Departures\", \n     ylab=\"Number of Departures\", xlab=\"Month\", \n     col=\"blue\", lwd=2)\n\n# Lets add more information by adding the actual values above the points as text.\ntext(x=1:12, y=as.numeric(origin_departures[1, ]+1000), \n     labels=as.numeric(origin_departures[1, ]), cex=0.5)\n\n# Now lets draw the other \"Origin\". This time, we dont want to use the plot function, because that would erase this and make a new plot. So, instead, lets use the 'lines' function.\nlines(as.numeric(origin_departures[2, ]), type=\"b\", \n      ylim = c(0, 20000), main=\"Total Number of Departures\", \n      ylab=\"Number of Departures\", \n      xlab=\"Month\", col=\"orange\", lwd=2)\n\n# And draw the text as done before.\ntext(x=1:12, y=as.numeric(origin_departures[2, ]+1000), \n     labels=as.numeric(origin_departures[2, ]), cex=0.5)\n\n# 3. Make box plots of delay times for each \"UniqueCarrier\".\n\nhf1 <- hflights[hflights$DepDelay > 0, ]\nboxout <- boxplot(DepDelay ~ UniqueCarrier, data=hf1, ylim=c(0,100), main=\"Departure Delays for Unique Carriers\")\n\n# These events are captured within the `out` variable in boxout\nboxout$out\n\n# How many such events are present in each uniquecarrier?\nlong_delays <- boxout$n\nlong_delays\n# So, the unique carriers CO, OO, WN and XE have the most number of long delays. But this may not be the only metric we should look at to judge a carriers' performance. Because, some carriers handle a lot more traffic than others. So we should actually look at the proportion of long delays.\n\ntotal_departures <- as.numeric(table(hflights$UniqueCarrier))\nround(long_delays/total_departures, 2)\n\n# The proportion of long delays is too high for: WN, UA, OO and CO.\n\n#8.3.\tProject 3 â Statistical Inference#####\n\n#Get the data \n\n# If you havent installed the car package, then install it\ninstall.packages(\"car\")\n\ndata(Prestige, package = \"car\")\nhead(Prestige)\n\n# Exercises\n\n# - Determine if numeric variables are statistically significant on another numeric variable.\n\n# - Determine if categorical variables are statisticlly significant on another numeric variable.\n\n#Challenges\n\n# 1. Do Education, Income, and Women variables have statistically significant relationship with the 'prestige' variable.\n\ncor.test(Prestige$education, Prestige$prestige)\ncor.test(Prestige$income, Prestige$prestige)\ncor.test(Prestige$women, Prestige$prestige)\n\n##\n# 2. Does occupational 'type' has a statistically significant relationship with 'prestige'?\n\nboxplot(prestige ~ type, data=Prestige)\n# from the boxplot, we can infer that the 'prof' type has a higher prestige score. So lets confirm that using anova.\naovmod <- aov(prestige ~ type, data=Prestige)\nsummary(aovmod)\n# With a high F-ratio and p-value less than the significance level of 0.05, we can reject the null hypothesis, and conclude that the relationship between type and prestige score is statistically significant.\n\n######\n#9.\tDPLYR AND PIPES#####\n#--------------------------------\n\n#9.1.\tPipes with Magrittr#####\n\nknitr::opts_chunk$set(echo = TRUE)\n\n##\ndf1 <- mtcars[mtcars$hp > 150, ] # filter\ndf2 <- aggregate( . ~ cyl, data=df1, FUN=mean) # aggregate\ndf3 <- round(df2, 2) # round\ndf3$kpl <- df3$mpg*0.42 # Creating new variable\n\n##\ndf_1 <- round(aggregate( . ~ cyl, data=mtcars[mtcars$hp > 150, ], FUN=mean), 2)\ndf_1$kpl <- df_1$mpg*0.42\n\n##\n\ninstall.packages('magrittr')\nlibrary(magrittr)\noutput <- mtcars %>% subset(hp > 150) %>%\n  aggregate(. ~ cyl, data = ., FUN = mean) %>% round(2) %>% \n  transform(kpl = mpg %>% multiply_by(0.42))\n\noutput\n##\n2.2452323 %>% round(3)\n\n##\n\nrnorm(10) %T>% plot(main=\"rnorm\") %>% sum\n\n##\nmtcars$mpg %<>% sqrt\n\n##\nmtcars %$% cor(mpg, cyl)\n\n##\n#Challenge\n# - Use the pipe operaction to -\n# Form mtcars , compute the correlaction between mpg and wt when cyl =6.\n\nmtcars %>% subset(cyl==6) %$% cor(mpg, wt)\n\n\n#9.2.\tThe 7 Data Manipulation Verbs#####\n\n# How to create and work with tibble data object\n# The 7 of the 9 key verbs of data manipulation\n# - Filter and slice\n# - Select and rename\n# - Mutate and transmute\n# - Arrange\n\nlibrary(hflights)\nlibrary(dplyr)\nhf_tbl <- as.tbl(hflights)\nhf_tbl\n\n## \ndplyr::glimpse(hf_tbl)\n\n## \nfilter(hf_tbl, Month==1, Year==2011)\n\n## \nfilter(hf_tbl, (Month==1 & Year==2011))\n\n## \nslice(hf_tbl, 2:6)\n# This is effectively same as. \nhf_tbl[2:6, ]\n\n## \nhf_tbl %>% filter(Month==1) %>% slice(1:5)\n\n# This is same as using the square bracket as a function by enclosing it within a pair of quotes.\nhf_tbl %>% filter(Month==1) %>% `[`(1:5, )\n\n## \ndplyr::select(hf_tbl, Year, DayOfWeek)\ndplyr::select(hf_tbl, Year:DayOfWeek) # if you want to select all variables between them.\nselect(hf_tbl, 1, 2)\nselect(hf_tbl, c(1, 2))\n\n## \nselect(hf_tbl, contains(\"Time\"))\nselect(hf_tbl, starts_with(\"Day\"))\nselect(hf_tbl, ends_with(\"Time\"))\n\n## \nhf_tbl_1 <- rename(hf_tbl, mth=Month)\nhf_tbl_1\n\n## \n#Create new variable and trans fom from existing once.\nht_tbl_2 <- mutate(hf_tbl, yearmonth=paste0(Year, Month) %>% as.numeric)\nglimpse(ht_tbl_2)\n\n## \nht_tbl_3 <- transmute(hf_tbl, yearmonth=paste0(Year, Month) %>% as.numeric)\nht_tbl_3\n\n## \narrange(hf_tbl, Year, Month)\n\n## \narrange(hf_tbl, Year, desc(Month))\n\n# Challenge\n\n#Using the 'mtcars' dataset, Create a new 'tbl' (read tibble) called mtcars_tbl\n#that has -\n\n# 1. A new column called 'make' that contains the names of cars.\n# 2. A new column for mileage in terms of kilometers per liter called 'kpl'.\n# 3. A 'kpl_int' column which is 'kpl' rounded to zero digits.\n# 4. sorted by kpl\n# 5. Contains cars that only 'vs' engines.\n\n# Note:1 mpg = 0.425 kpl\n\n## \nlibrary(dplyr)\ndata(mtcars)\nmtcars_tbl <- mtcars %>% \n  mutate(make=rownames(mtcars)) %>% \n  as.tbl() %>% \n  mutate(kpl=0.425*mpg, kpl_int=round(kpl)) %>% \n  arrange(kpl) %>% filter(vs==1)\nmtcars_tbl\n\n\n#9.3.\tAggregation and Special Functions#####\n\n# How to do any kind of data aggregation using:\n# - group_by\n# - summarize function\n# - Special functions that aid data aggregation\n# -  - n()\n# -  - nth()\n# -  - n_distinct()\n# -  - first()\n# -  - last()\n\nlibrary(dplyr)\nlibrary(hflights)\nhf_tbl <- as.tbl(hflights)\nhf_tbl_gp <- group_by(hf_tbl, UniqueCarrier)\nhf_tbl_gp  # the grouped tbl will appear the same. But it will have a new grouped_df class.\nclass(hf_tbl_gp)  # now its a grouped_df\n\n##\nsummarise(hf_tbl_gp, mean_dist=mean(Distance))\n\n##\nunique_carrier_agg <- hflights %>% group_by(UniqueCarrier) %>% summarize(delay=mean(DepDelay, na.rm=T),\n                                                                         num_obs=n(),  # n() gets the number of observations in the grouped variable.\n                                                                         num_distinct_obs=n_distinct(DepDelay), # n_distinct gets the number of distinct observations in the specified variable.\n                                                                         first_obs=first(DepDelay), # first for first observation\n                                                                         second_obs=nth(DepDelay, 2),  # nth for nth observation from start \n                                                                         last_obs=last(DepDelay))  # and last for the last observation.\n\n##\nhf_tbl_gp <- ungroup(hf_tbl_gp)\nhf_tbl_gp  # As expected, the groups information does not show now.\n\n# Challenge\n\n\"Create aggregated table similar to 'unique_carrier_agg' but \naggregated by 'Origin' variable\"\n\n\norigin_agg <- hflights %>% group_by(Origin) %>% summarize(delay=mean(DepDelay, na.rm=T),\n                                                          num_obs=n(),  \n                                                          num_distinct_obs=n_distinct(DepDelay),\n                                                          first_obs=first(DepDelay), \n                                                          second_obs=nth(DepDelay, 2), \n                                                          last_obs=last(DepDelay))\norigin_agg\n\n#9.4.\tTwo Table Verbs#####\n\nleft_join()\nright_join()\ninner_join()\nfull_join()\n\nlibrary(dplyr)\nmtcars$carname <- mtcars %>% rownames\nmtcars_tbl <- mtcars %>% as.tbl\nmtcars_tbl_1 <- mtcars_tbl %>% select(carname, mpg:drat)\nmtcars_tbl_2 <- mtcars_tbl %>% select(carname, wt:am) %>% sample_frac(.4)\nmtcars_tbl_3 <- rename(mtcars_tbl_2, car=carname)  \n\n## \nmtcars_tbl_1 %>% left_join(mtcars_tbl_2)\n\n## \nmtcars_tbl_1 %>% left_join(mtcars_tbl_3, c(\"carname\"=\"car\"))\n\n## \nmtcars_tbl_1 %>% right_join(mtcars_tbl_2)\nmtcars_tbl_1 %>% inner_join(mtcars_tbl_2)\nmtcars_tbl_1 %>% full_join(mtcars_tbl_2)\n\n\n#9.5.\tWorking With Databases#####\n\ninstall.packages('dplyr')\nlibrary(dplyr)\nflights <- as.tbl(hflights)\ninstall.packages('RSQLite')\nlibrary(RSQLite)\ndata(hflights, package = 'hflights')\n\nmy_db <- src_sqlite(\"my_db.sqlite3\", create = T)\nmy_db\n\n## \ncopy_to(my_db, flights, temporary = FALSE, indexes = list(c(\"Year\", \"Month\", \"DayofMonth\"), \"UniqueCarrier\", \"TailNum\"))\nmy_db\n\n## \nflights_out <- tbl(my_db, \"flights\")\nflights_out\n\n## \nflights_out <- tbl(my_db, sql(\"SELECT * FROM flights\"))\nflights_out\nclass(flights_out) \n\n## \nflights_out %>% group_by(UniqueCarrier) %>% summarise(speed=sum(Distance)/sum(AirTime))\n\nmyData <- flights_out %>% filter(UniqueCarrier==\"AA\") %>% mutate(TaxiTime=TaxiIn + TaxiOut) %>% select(Year, Month, DayofMonth, DayOfWeek, DepTime, UniqueCarrier, TaxiTime)\nmyData\n\n## \nout <- collect(myData)\nout\n\n## \nmyData$query\n\n#Challenge\n\n# Create a SQLite database called mtcarsdb.sqllite in your local hard disk.\n\nmtcars_db <- src_sqlite(\"mtcars_db.sqlite3\", create = T)\n\n# Copy the mtcars dataset to mtcarsdb.sqlite.\n\ncopy_to(mtcars_db, mtcars, temporary = FALSE, \n        indexes = list(c('mpg','cyl','disp', 'hp', 'drat', 'wt', 'qsec', 'vs', 'am', 'gear', 'carb')))\n\n# Retrieve the mtcars table from mtcarsdb.sqlite as a tbl_sqlite.\n\nmtcars_out <- tbl(mtcars_db, \"mtcars\")\nmtcars_out\n\n## \nmtcars_out <- tbl(mtcars_db, sql(\"SELECT * FROM mtcars\"))\nmtcars_out\nclass(flights_out) \n\n# filter the data so it contains only cars with 6 cylinders.\n\nmtcarsData <- mtcars_out %>% filter(cyl==\"6\")\nmtcarsData\n\n\n# Then finally, collect the full table to your local cache as a tbl_df.\n\ntbl_df <- collect(mtcarsData)\ntbl_df\n\n######\n#10.\tDATA.TABLE#####\n#--------------------\n\n# - Fast import using fread(), filter, and select data.\n# - Create, updating, and deleting columns.\n# - How to aggregate Ddata and .N and .I functions.\n# - Chaining, functions with data.table and .SD.\n# - Setting keys, joins, and fast for-loops with set().\n\n#10.1.\tUnderstanding Basics, Filter, and Select#####\n\ninstall.packages('data.table')\nlibrary(data.table)\ninstall.packages('dtplyr')\nlibrary(dtplyr)\nmt<- fread(\"https://raw.githubusercontent.com/selva86/datasets/master/mtcars.csv\")\nmt\nclass(mt)\n\nmtcars$carname <- rownames(mtcars)\ndf <- mtcars \nmtcars_dt <- data.table(df)\nclass(mtcars_dt)\nmtcars_dt\n\n##\nsetDT(df)\nclass(df)\ndf\n\n##\nsetDF(df)\nclass(df)\ndf\n\n##\nmtcars[mtcars$cyl == 6 & mtcars$gear == 4, ]\n\n##\nmtcars_dt[cyl==6 & gear==4, ]\n\n##\nmtcars[, 1]\n\n##\nmtcars_dt[, 1]\n\n##\nmtcars_dt[, 1, with=F]\n\n##\nmtcars_dt[, mpg]\n\n##\nmyvar <- \"mpg\"\nmtcars_dt[, myvar, with=F]\n\n## ------------------------------------------------------------------------\nmtcars_dt[, list(mpg, cyl, gear)]\n\n## ------------------------------------------------------------------------\nmtcars_dt[, .(mpg, cyl, gear)]\n\n\n# Challenge\n\n# Convert the in-built 'airquality' dataset to a data.table.\n\nclass(airquality)\nairquality_dt <- airquality\nsetDT(airquality_dt)\nclass(airquality_dt)\n\naq_dt <- data.table(airquality)\n\n#Then, select 'Solar.R', 'Wind' and 'Temp' for those rows where 'Ozone' is NOT missing.\n\naq_dt[!is.na(Ozone), .(Solar.R, Wind, Temp)]\n\n#10.2.\tUnderstanding Syntax, Creating and Updating Columns#####\n\n#-----------------------------------------------#\n#            |          DT[i, j, by]            #\n#-----------------------------------------------#\n# Data.Table |    i   |   j     |     by        #\n#   SQL      |  WHERE |  sELECT | GROUP BY      #\n#-----------------------------------------------#\n\n\nSQL # Version\n'---'\n'select AVG(mpg) as Mileage, AVG(wt) as Weight WHERE disp > 100 GROUP BY gear\nfrom db.mtcars_tbl;'\n\nData.Table # Version\n'--------'\nmtcars_dt[disp > 100, .(Mileage = mean(mpg), Weight = mean(wt)), by=.(gear)]\n\nlibrary(data.table)\nmtcars$carname <- rownames(mtcars)\nmtcars_dt <- data.table(mtcars)\n\n# Creating new column\nmtcars_dt$cyl_gear <- mtcars_dt$cyl + mtcars_dt$gear\nmtcars_dt\n\n## \nmtcars_dt[, cyl_gear2 := cyl + gear]\nmtcars_dt\n\n## \nmtcars_dt[,  `:=`(cyl_gear3 = cyl * gear,\n                  cyl_gear4 = cyl - gear)]\nmtcars_dt\n\n## \nmtcars_dt[,  .(cyl_gear3 = cyl * gear,\n               cyl_gear4 = cyl - gear)]\n\n## \nmyvar <- c('var1')\n\n## \nmtcars_dt[, myvar:=1]\nmtcars_dt\n\n## \nmtcars_dt[, c(myvar):=1]\nmtcars_dt\n\n## \nmtcars_dt[, (myvar):=2]\nmtcars_dt\n\n## \nmtcars_dt[, c(\"cyl_gear2\", \"cyl_gear\", \"cyl_gear3\", \"cyl_gear4\", \"myvar\", \"var1\") := NULL]\nmtcars_dt\n\n## \n\n#Challenge\n\n\"Create a scaled version of the mpg variable called 'scales_mpg', such that each value\nof 'mpg' is divided by the mean of mpg and is rounded to 2 digits.\"\n\nmtcars_dt[, scaled_mpg := round(mpg/mean(mpg), 2)]\nmtcars_dt\n\n#10.3.\tAggregating Data, .N, and .I#####\n\nlibrary(data.table)\nmtcars$carname <- rownames(mtcars)\nmtcars_dt <- data.table(mtcars)\n\nmtcars_dt[, .(mean_mileage=mean(mpg)), by=cyl]\n\n##\nmtcars_dt[, .(mean_mileage=mean(mpg)), by=.(cyl, gear)]\n\n##\nmtcars_dt[, .(first_mileage=mpg[1]), by=cyl]\n\n##\nmtcars_dt[, .(second_mileage=mpg[2]), by=cyl]\n\n##\nmtcars_dt[, .(last_mileage=mpg[length(mpg)]), by=cyl]\n\n## \nmtcars_dt[, .(last_mileage=mpg[.N]), by=cyl]\n\n## \nmtcars_dt[, .N, by=cyl]\n\n## \nmtcars_dt[, .I]\n\n## \nmtcars_dt[cyl==6, .I]\n\n## \nmtcars_dt[, .I[cyl==6]]\n\n## \nmtcars_dt[, which(cyl==6)]\n\n\n#Challenge\n\n\"Compute the number of cars and the mean mileage for each gear type from mtcars dataset\"\n\nmtcars_dt[, .(.N, mileage=mean(mpg) %>% round(2)), by=gear]\n\n#10.4.\tChaining, Functions, and .SD#####\n\nlibrary(data.table)\nmtcars$carname <- rownames(mtcars)\nmtcars_dt <- data.table(mtcars)\ndt1 <- mtcars_dt[, .(mean_mpg=mean(mpg),\n                     mean_disp=mean(disp),\n                     mean_wt=mean(wt),\n                     mean_qsec=mean(qsec)), by=cyl]\noutput <- dt1[order(cyl), ]\noutput\n\n## \noutput <- mtcars_dt[, .(mean_mpg=mean(mpg),\n                        mean_disp=mean(disp),\n                        mean_wt=mean(wt),\n                        mean_qsec=mean(qsec)), by=cyl][order(cyl), ]\n\n## \nmtcars_dt[, .SD, by=cyl]\n\n## \noutput <- mtcars_dt[, lapply(.SD[, 1:10, with=F], mean), by=cyl]\noutput\n\n## \noutput <- mtcars_dt[, lapply(.SD, mean), by=cyl, .SDcols=c(\"mpg\", \"disp\", \"hp\", \"drat\", \"wt\", \"qsec\")]\noutput\n\n\n#Challenge\n\n\"Convert the below matrix (mymat) to a data.table. Compute the sum of squares of all the rows using apply()\ninside the data.table and assign it to a new column 'ss'\"\n\nset.seed(100)\nrand <- sample(1:10, 100000, replace=T)\nmymat <- matrix(rand, ncol=5)\n\n# or just run this\nsource(\"https://goo.gl/jMbtpU\")\n\nlibrary(data.table)\ndt <- data.table(mymat)\nss <- function(x){(x['V1']^2) + (x['V2']^2) + (x['V3']^2) + (x['V4']^2) + (x['V5']^2)}\ndt[, ss:=apply(.SD, 1, ss)]\n\n## \ndt$ss <- apply(dt, 1, ss)\n\n## \ndt$ss <- (dt$V1^2) + (dt$V2^2) + (dt$V3^2) + (dt$V4^2) + (dt$V5^2)\n\n#10.5.\tFast Loops with set(), Keys, and Joins#####\n\nlibrary(data.table)\nmtcars$carname <- rownames(mtcars)\n\n##\nmtcars_dt <- data.table(mtcars, key=\"carname\")\n\n##\nmtcars_dt <- data.table(mtcars)\nsetkey(mtcars_dt, carname)\n\n##\nmtcars_dt\n\n##\nkey(mtcars_dt)\n\n##\nmtcars_dt[\"Merc 230\"]\nmtcars_dt[\"Valiant\"]\n\n##\ndt1 <- mtcars_dt[1:10,.(carname, mpg, cyl)]\ndt2 <- mtcars_dt[6:15, .(carname, gear)]\ndt1\ndt2\n\n##\ndt1[dt2]\n\n##\ndt2[dt1]\n\n##\ndt2[dt1, nomatch=0L]\n\n##\ndt1[dt2[.(union(dt2$carname, dt1$carname))]]\n\n##\nmerge(dt1, dt2, all = T)  # full join\nmerge(dt1, dt2, all = F)  # inner join\nmerge(dt1, dt2, all.x = T)  # left join\nmerge(dt1, dt2, all.y = T)  # right join\n\n##\nsetkey(mtcars_dt, cyl, gear)\nkey(mtcars_dt)\n\n##\nmtcars_dt[c(8, 3)] # this is incorrect as of version 1.9.6. So always enclose numeric keys in a dot.\nmtcars_dt[.(8, 3)]\nmtcars_dt[J(8, 3)]\n\n## \nsetkey(mtcars_dt, NULL)\nmtcars_dt\n\n## \noutput <- mtcars_dt[, .(mean_mpg=mean(mpg),\n                        mean_disp=mean(disp),\n                        mean_wt=mean(wt),\n                        mean_qsec=mean(qsec)), by=cyl][order(cyl), ]\noutput\n\noutput <- mtcars_dt[, .(mean_mpg=mean(mpg),\n                        mean_disp=mean(disp),\n                        mean_wt=mean(wt),\n                        mean_qsec=mean(qsec)), keyby=cyl]\noutput\nkey(output)\n\n## \nM <- matrix(1, nrow=100000, ncol=2)\ndt <- as.data.table(M)\ndt\n\n## \nsystem.time({\n  for(i in 1:nrow(dt)){\n    dt[i, V1:=0]\n  }  \n})\n\n##\ndt <- as.data.table(M)\nsystem.time({\n  for(i in 1:nrow(dt)){\n    set(dt, i, 1L, 0)\n  }  \n})\n#   user  system elapsed \n#  0.192   0.000   0.192 \n\n\n#Challenge\n\n\"Replace the diagonal in data.frame 'df' to the respective row number using the set function.\"\n\n##\nM <- matrix(1, nrow=10000, ncol=10000)\ndf <- as.data.frame(M)\ndf[1:5, 1:5]\n\n## \nfor(i in 1:nrow(df)){\n  set(df, i, i, i)\n}\ndf[1:5, 1:5]\n\n\n########################################################\n\n#2. Getting Started with R for Data Science#####\n###########################################################################\n#What You Will Learn                                                      #\n#                                                                         #\n#Write R code that can be executed outside RStudio                        #\n#Get data from numerous sources such as files, databases, and even Twitter#\n#Clean data before the analysis phase begins                              #\n#Load libraries into RStudio for use within the analysis phase            #\n#Perform data cleaning on a dataset                                       #\n#Create a codebook so that the data can be presented in a summary         #\n#Understand how to use visualization to understand data and tell a story  #\n###########################################################################\n\n#1.\tINTRODUCING R#####\n#1.1.\tThe Course Overview#####\n#1.2.\tWhat is R?#####\n\n# Supports a large number of statistical packages\n# - Linear and non-linear modelling\n# - classical statistical test\n# - Time series analysis\n# - Classification\n# - Clustering\n\n#1.3.\tThe Structure of the Language#####\n\nmsg <- 'Hello world'\n\ntypeof(msg)\n\nmsg <- 2\n\ntypeof(msg)\n\n#1.4.\tData Structures within R#####\n\n# Vector\n# - One dimension array\n# Matrix\n# - Two dimension array\n# Array\n# - Multi dimension array\n# DataFrame\n# - Similar to matrix but can contain different data types\n# List\n# - Ordered collection of objects\n\n#Concat command\na <- c(\"Hello\",\"World\")\na\nprint a[1]\n\n#Creating a sequence\na <- seq(length = 5, from =0, by =1)\na\na <- seq(length = 10, from =2, by =2)\na\n\n#Creating a matrix\na <- matrix(1,10)\na\na <- matrix(1,10 , nrow = 5, ncol =3)\na\n\n#Creating a data frame\ndf <- data.frame(x = 1:3, y = c(\"a\",\"b\",\"c\"))\ndf\n\n#adding a list to a data frame column\ndf <- data.frame(x = 1:3)\ndf$y <- list(1:2, 1:3, 1:4)\n\n#assigning a list of values to a list\nlist <- list(1:2, 1:3, 1:4)\nlist[[1]]\nlist[[1]][1]\n\n# Coercion oftrn happens automatically but explicit coercion can be peroformed\nas.character()\nas.double()\nas.integer()\nas.logical()\nas.data.frame()\n\n#Diagnostic Functions\n\nStr() # - displays internal structure of object\nName() # - names the elements in the object\nClass() # - retrives the internal class of the object\nMode() # - get or sets the type of storage mode\nLength() # - retrieves the length of the object\nDim() # - retrives the dimensions of an object\n\n#1.5.\tWriting a Simple Program in R#####\n\n#####\n#2.\tGETTING DATA INTO R#####\n#2.1.\tThe Structure of a DataFrame#####\n#2.2.\tCreating a DataFrame from a CSV File#####\n\nsales <- read.csv('sales.CSV',header = T,sep = ',')\n\n#2.3.\tCreating a DataFrame from a Zip File#####\n\nlst <- unzip('sales.zip', files = NULL, exdir = 'unzip', unzip = 'internal')\nlst\n\nfor(i in 1:length(lst)) {print(lst[i])}\n\nwrite.csv(Mydata, file = gzfile('mtcars.csv.gz'))\n\n#2.4.\tCreating a DataFrame from a Database#####\n\n# R Database Packages\n\nRoracle\nRMySql\nRODBC\nRJDBC\nRSQLServer\nDBI\n\n# Connecting to the Database\n\ninstall.packages('RODBC')\nlibrary(RODBC)\n\n#Ceate a connection to the database\n\nchannel <- odbcConnect('mysql', uid = 'vinayeim', pwd = 'Welcome@123',\n                       believeNRows = F)\n\n#DSN - A registered data source name\n\n#UID, PWD - UID and password for authentication\n\n# Check that connection is working(Optional)\n\nodbcGetInfo(channel)\n\n#list of tables in database\n\nsqlTables(channel)\n\n# details of database table\n\nsqlColumns(channel, 'sakila.actor')\n\n# Creating the DataFrame\n\n# - Create a dataframe.Either from a query\n\ndataFrame <- sqlQuery(channel, \"select * from sakila.actor\")\nrm(dataFrame)\n# or get the whole table\n\ndataFrame <- sqlFetch(channel, \"sakila.actor\")\ndataFrame\n\n# Process the DataFrame\n\n# - do stuff with row\n\nfor(i in 1:nrow(dataFrame))\n{\n  row <- dataFrame[i,]\n}\n\n# - Writing Back to the Database\n\nstr <- \"insert into charity_accounts(charity_number, charity_accounts)\nvalues('%s','%s')\"\nrow <- '1000000'\nval <- 'R charity account demo'\n\nsql <- sprintf(str, row, val)\nsql\nret <- sqlQuery(channel, sql)\n\n# - save the DataFrame to database\n\n# - Table created if not in existence\n\nsqlSave(channel, DataFrame)\n\n# Cleaning up\n\n# - delete all the rows of the table\n\nsqlClear (channel, sqtable, errors = T)\n\n# - removes the table sqtable\n\nsqlDrop(channel, sqtable, errors = T)\n\n# - Gets the error message\n\nodbcGetErrMsg(channel)\n\n# - clears error messages\n\nodbcClearError(channel)\n\n# - Dont't forget to close the ODBC connection\n\nodbcClose(channel)\n\n#####\n#3.\tCLEANING AND BLENDING DATA#####\n#3.1.\tThe Tools Available for Cleaning Data#####\n\n# We will learn\n\n# - Normalising strings and changing case\n\n# - Changing case\n\n# - Handling outliers\n\n# - Formatting of dates\n\n# Defining Dirty Data\n\n# - Datacan often be messy and inaccurate\n# - Data can contain\n\n# - Incomplete data\n# - Missing data\n# - Duplicated data\n# - Incorrect data associated with a field\n\nlibrary(stringr)\n\nstr_trim(\"   Hello World   \")\n\nstr_pad(12345, width=6, side=\"left\", pad=0)\n\ntoupper(\"hello world\")\n\ntolower(\"HELLO WORLD\")\n\n\nx<-c(1:10, 20,30)\n\nx[!x %in% boxplot.stats(x)$out]\n\n\ntoday<-as.Date(Sys.Date())\nxmas<-as.Date(\"2016-12-25\")\ndaysleft<-xmas-today\ndaysleft\n\n# Formatting dates\n\n# -  %d days as a month(0-31)\n# -  %a abbreviated day of the week\n# -  %A full day of the week\n# -  %m month as a number(1-12)\n# -  %b abbreviated name of month\n# -  %B full name of month\n# -  %y two digit year\n# -  %Y four digit year\n\nformat(xmas, \"%d/%m/%y\")\n\n\n#3.2.\tDealing with Null Values#####\n\n# Missing data\n\n# - Missing values within R are donated by NA.\n# - The R code is.na(var) tests if the R variable 'var' is NA.\n# - na.fail: will stop if a missing value is encountered.\n# - na.omit: will ignore completely any row with missing value\n# - na.exclude: will ignore rows with missing values but\n# - na.pass: take no action and include the blank values\n\nX<-c(1,2,3,NA,5)\ntry(na.omit(X))\ntry(na.fail(X))\n\n\n\nX<-c(1,2,3,NA,5)\nis.na(X)\n\n\nX<-c(1,2,3,NA,5)\nanyNA(X)\n\n\nX<-c(1,2,3,NA,5)\nna.omit(X)\nna.exclude(X)\n\n\nX<-c(1,2,3,NA,5)\n\n# The fail function will fail when a na value is encountered.\nna.fail(X)\n\n# Will ignore na values in a structure.\nna.pass(X)\n\n\nX<-c(1,2,3,NA,5)\nmean(X) # will return NA\n\n# Setting remove NA values to TRUE\nmean(X, na.rm=TRUE)\n5.5\n\n#3.3.\tStandardizing Date Formats#####\n\n# get todays date\nToday <- Sys.Date()\nToday\n\n# change the format\nformat(Today, format=\"%d/%B/%Y\") \n\n\nx <- c(\"1jan1960\", \"2jan1960\", \"31mar1960\", \"30jul1960\") \nx\nas.Date(x, \"%d%b%Y\") \n\nSys.time() \n\n\nmyTime<-Sys.time()\nformat(myTime, format=\"%H.%M.%S\") \n\n#3.4.\tBlending Multiple DataFrames#####\n\n#Create two data frames\n\nx <- data.frame(letter = c( 'a', 'b' ), word = c( 'Apple', 'Boat' )) \ny <- data.frame(letter=c(\"d\", \"e\"), word=c(\"Dog\", \"elephant\"))\n\n# Create a new dataframe to merge the results\nZ<-data.frame(x,y)\n\n\n#rbind can be used to concatenate two data frames.\nz<-rbind(y,x) \n\n\n\n\np<-data.frame(letter=c('a','b','c'), type=c('Granny Smith', 'Canoe', 'Tabby' ))\n\nZ<-merge(x,p) # merge(x,p, letter)\n\n\n\n# Merging on data with differing labels\n\na<-data.frame(id=c('a','b','c'), Colour=c('Green', 'White', 'Tabby' ))\n\nb<-merge(x=z, y=a, by.x='letter', by.y='id')\nversion\n\n\n#####\n#4.\tCOOKING THE CODEBOOK#####\n#4.1.\tWhat Is a Codebook and Why Create One?#####\n\n# What is a Codebook?\n\n# - An important document for research or data analysis\n# - Provides data on variables coding and database generation\n# - Provides details of the data and whrer it was sourced\n# - Provides a reference for the research or analysis many years later.\n\n#4.2.\tCreating the Codebook Using Standard R API Functionality#####\n\n# load the memisc library\ninstall.packages(\"memisc\")\nlibrary(memisc)\n\n#use a dataset with in R\ndata()\n\n# cast the data to R data set\nx<-data.frame(Orange)\nclass(Orange)\ntypeof(x)\n\n# Call the codebook function\ncodebook(x)\n\nsummary(x)\n\n#4.3.\tManually Creating a Custom Codebook#####\n\ndata()\n\nx<-data.set(Orange)\nx<-data.frame(Orange)\nsapply(Orange, min)\nsapply(Orange, max)\nsapply(Orange, range)\n\nsummary(x)\n\n#####\n#5.\tDATA MINING AND ANALYSIS#####\n#5.1.\tIntroduction to Data Mining and Analysis#####\n\n# What is a Data Mining?\n# - The process of analysing data and summarizing it into useful information.\n# - Data mining is the process to discover interesting knowledge from large amount of data.\n\n# Data Mining steps as follows...\n\n# - Selection\n# - Pre Processing\n# - Transformation\n# - Data Mining\n# - Evaluation\n# - Knowledge\n\n\n# Selection\n# - Select the data from source\n# - Blend data with other data sources\n# - Loading data into the data frame\n\n# Pre Processing\n# - cleaning data\n# - Handle missing values\n# - Formatting of data values\n\n# Transformation\n# - Smoothing\n# - Aggregation\n# - Normalization\n\n# Data Mining (Discovery)\n# - Clustering\n# - Classifying\n# - Regression analysis\n\n# Evaluation\n# - Visualization\n\n# Knowledge\n\n#5.2.\tThe Tools and Techniques for Creating the Story#####\n\ninstall.packages(\"tsoutliers\")\ninstall.packages(\"plyr\")\nlibrary(tsoutliers)\n\n\ndata(\"hicp\")\ny<-hicp[[\"011600\"]]\n\ny\n\nout<-tsoutliers::tso(y, types=c(\"AO\", \"LS\", \"TC\"), maxit.iloop=10)\nplot(out)\n\n#5.3.\tRegression Analysis with R#####\n\n# What is Linear Regression?\n# - A simple linear regression model that describes the relationship b/w two variables\n# - It is used when we want to predict the value of a variable on the value of two or more other variables\n\n# Linear Regression\n\n# - the funcion lm() creates a Linear Regression model\n\nx<-data.set(Orange)\nx\n\nlmx<-lm(Orange$age ~ Orange$circumference) \n\nlmx\n\npredict(lmx)\n\nsummary(lmx)\n\n\n#5.4.\tClustering Data with R#####\n\n# What is Clustering?\n# - The task of grouping a set of objects in such a way that objects in the same group\n#(Called a cluster) are more similat (in some sense or another) to each other than to\n#those in other groups(clusters)\"\n# - Used to identify traits or common deminator in a dataset\n\ndata()\n\nkmeans(Orange$circumference, 2)\nkmeans(Orange$age, 2)\n\nkmeans(Orange$circumference, 3)\nkmeans(Orange$age, 3)\n\nd <- dist(Orange, method = 'euclidean')\nfit <- hclust(d, method = 'centroid')\nplot(fit) # disply dendogram\n\ninstall.packages('mclust')\nlibrary(mclust)\n\nfit <- Mclust(Orange)\nplot(fit)\n\n#5.5.\tClassifying Data with R#####\n\n# What is Classifying?\n\n# - Classification has a close tie in with clustering\n# - Classification is predictive\n# - Often used in direct marketing, medical diagnosis, and fraud detection.\n\n# Classifying Algorithms\n\n# - Support Vector Machines\n# - penalizedSVM\n# - k-Nearest Neighbours\n# - Outliers\n# - Decision Trees\n# - Naive Bayes\n# - Adaboost\n# - JRip\n\ninstall.packages(\"e1071\")\n\nlibrary(e1071)\nlibrary(class)\n\nclassifier<-naiveBayes(iris[,1:4], iris[,5])\nclassifier\npredict(classifier,iris[, 1:4] )\n\n\ninstall.packages(\"rpart\")\nlibrary(rpart)\n\nsvmpred<-svm(iris[, 1:4], iris[, 5])\nsvmpred\npredict(svmpred,iris[, 1:4] )\n\n\ninstall.packages(\"kknn\")\nlibrary(kknn)\niris.kknn <- kknn(Species~., iris, iris, distance = 1, kernel = \"triangular\")\niris.kknn\npredict(iris.kknn)\n\n\n\n# Ward Hierarchical Clustering\n# Calculate the distance metric\n# Method used here is euclidean but could easily be \"maximum\", \"manhattan\", \"canberra\", \"binary\" or \"minkowski\"\nd <- dist(Orange, method = \"euclidean\") \n\n#Use the hierarchical clustering analysis\n#\"ward.D\", \"ward.D2\", \"single\", \"complete\", \"average\" (= UPGMA), \"mcquitty\" (= WPGMA), \"median\" (= WPGMC) or \"centroid\" (= UPGMC).\nfit <- hclust(d, method=\"centroid\")\n\n#Plot the data to a graph\nplot(fit) # display dendogram\n\n\n\ninstall.packages(\"mclust\")\nlibrary(mclust)\nfit <- Mclust(Orange)\nplot(fit) # plot results \nsummary(fit) # display the best model\n\n#####\n#6.\tA PICTURE PAINTS A THOUSAND WORDS#####\n#6.1.\tData Visualization Tools#####\n\n#dygraph\n\ninstall.packages(\"dygraphs\")\n\nlibrary(dygraphs)\n\ndygraph(AirPassengers, xlab=\"year\", ylab=\"passengers\")\n\n\n# Displaying map data\n\ninstall.packages(\"maps\")\nlibrary(maps)\n\nmap('world')\n\n#6.2.\tCreating Static Visualization Plots#####\n\ninstall.packages(\"ggplot2\")\n\nlibrary(ggplot2)\n#Point\nqplot(Orange$circumference, Orange$age, color=\"cyl\", data=Orange, \n      xlab=\"age\", ylab=\"circumference\")\n\n#Box plot\nqplot(Orange$circumference, Orange$age, color=\"cyl\", data=Orange, \n      xlab=\"age\", ylab=\"circumference\", geom=\"boxplot\")\n\n#Line\nqplot(Orange$circumference, Orange$age, color=\"cyl\", data=Orange, \n      xlab=\"age\", ylab=\"circumference\", geom=\"line\")\n\n#dygraph\n\ninstall.packages(\"dygraphs\")\n\nlibrary(dygraphs)\n\ndygraph(AirPassengers, xlab=\"year\", ylab=\"passengers\")\n\n\n# Displaying map data\n\ninstall.packages(\"maps\")\nlibrary(maps)\n\nmap('world')\n\n# get the map of the UK\nmap('world', c('UK', 'Ireland', 'Isle of Man','Isle of Wight'), xlim=c(-11,3), ylim=c(49,60.9))\n\n# overlay cities onto UK map\nmap.cities(x=world.cities) \n\n# add a point to the map\npoints(.1278, 51.5074, col=\"red\", pch=21)\n\n#6.3.\tCreating Interactive Plots#####\n\n# Streaming Data\n# Streaming data packages\n# - streamR\n# - stream\n\n# Loops\n# - forloop\n# - Whileloop\n\nn=1000\ndf=data.frame(time=1:n,y=runif(n))\nwindow=100\nfor(i in 1:(n-window))\n{\n  flush.console()\n  plot(df$time,df$y,type='l',xlim=c(i,i+window))\n  Sys.sleep(.09)\n}\n\ninstall.packages('streamR')\nlibrary(streamR)\n\nlibrary(RCurl)\n\nlibrary(bitops)\n\nlibrary(rjson)\n\nload(\"my_oauth\")\n\nfilterStream(\"tweets.json\", track = c(\"Farage\", \"Europe\"), timeout = 10, \n             oauth = my_oauth)\n\n#6.4.\tPublishing the Graphics#####\n\ninstall.packages(\"plotly\")\nlibrary(plotly)\nlibrary(ggplot2)\n\nplot_ly(x=Orange$age, y=Orange$circumference, type=\"scatter\")\n\n\n# to publish to a plotly account\npo<-plot_ly(x=Orange$age, y=Orange$circumference, type=\"scatter\")\nplotly_POST(po, filename=\"c:/Users/rskeggs/Desktop/tmp/\")\n\n#6.5.\tWhatâs Next?\n\ninstall.packages('twitteR')\n\n# Twitter\n# - Data can be extracted from Twitter feeds\n# - Location specific Tweets can be filtered out\n# - Does not act as a substitute for a firehose.\n\n# Simple Example\n\nconsumerKey <- '<supplied by twitter>'\nconsumerSecret <- '<supplied by twitter>'\naccessToken <- '<supplied by twitter>'\naccessSecret <- '<supplied by twitter>'\n\nsetup_twitter_oauth(consumerKey, consumerSecret, \n                    accessToken, accessSecret)\n\nelection <- searchTwitter(hashtag, n = 1500)\n\n'https://app.twitter.com/ap/new'\n\n########################################################\n\n#03.Learning Data Mining with R#####\n###########################################################################\n#What You Will Learn                                                      #\n#                                                                         #\n#Get to know the basic concepts of R: the data frame and data manipulation#\n#Discover the powerful tools at hand for data preparation and data cleansing#\n#Visually find patterns in data                                           #\n#Work with complex data sets and understand how to process data sets      #\n#Get to know how object-oriented programming is done in R                 #\n#Explore graphs and the statistical measure in graphs                     #\n#Gain insights into the different association types                       #\n#Decide what algorithms actually should be used and what the desired and possible outcomes of the analysis should be#\n#Grasp the discipline of classification, the mathematical foundation that will help you understand the bayes theorem and the naÃ¯ve bayes classifier#\n#Delve into various algorithms for classification such as KNN and see how they are applied in R#\n#Evaluate k-Means, Connectivity, Distribution, and Density based clustering#\n############################################################################\n\n\n#1. GETTING STARTED â A MOTIVATING EXAMPLE#####\n#1.1.   The Course Overview#####\n#1.2.   Getting Started with R#####\n\nlibrary(ggplot2)\nattach(mtcars)\n\nqplot(wt,mpg, data = mtcars)\n\ntransmission = factor(mtcars$am,levels = c(0,1), labels = c(\"Automatic\",\"Manual\"))\nmod=lm(wt~mpg,data=mtcars)\nqplot(wt,mpg,\n      data= mod,\n      color=transmission,\n      shape=transmission,\n      #geom=\"smooth\",\n      geom=c(\"point\",\"smooth\"),\n      #method=\"lm\", formula=y~x,\n      xlab = \"Weight\",\n      ylab= \"Miles per Gallon\",\n      main = \"Regression Example\")\n\n\n#1.3.   Data Preparation and Data Cleansing#####\n\n# Data Cleansing\n# - Data Frames and Indexing covered later\n# - NA - 'missing value' => is.NA\n# - NULL - 'empty set' => is.null\n# - Inf - 'valid numeric type'\n# - NAN - 'result of undefined operation' => is.nan\n\n\n#1.4.   The Basic Concepts of R#####\n\n# Reading a CSV\n# Important parameters to the 'read.csv' function -\n# - head(T/F)\n# - Sep\n# - Quote\n# - dec\n\ndf <- read.table('https://raw.githubusercontent.com/romeokienzler/packt/master/R/abc.csv')\ndf\n\n# Read from a Database\n\nlibrary(RODBC)\ndbConn <- odbcConnection('dbName')\ndf <- sqlFetch(dbConn, 'ABC')\nclose(myconn)\n\n#1.5.   Data Frames and Data Manipulation#####\n\na <- c(1,2,3,4)\nb <- c('up', 'down', 'left', 'right')\nc <- c('TRUE', 'TRUE', 'TRUE', 'FALSE')\nd <- c(1,2)\ne <- c(1,2,3)\n\nmydata <- data.frame(a, b, c, d)\nmydata <- data.frame(a, b, c, e)\ncolnames(mydata) <- c('ID', 'Direction', 'crash', 'Recycle')\nmydata\ncolnames(mydata)\nrownames(mydata)\nrownames(mydata) <- c('a', 'b', 'c', 'd')\nmydata\nrownames(mydata) <- c('a', 'b')\nmydata\n\n# Data Normalizing\n\n# Formula - Zi = xi-min(x)/max(x)-min(x)\n\n# Data Normalizing\n\n# Formula - Zi = xi-min(x)/max(x)-min(x)\n\nnormalize <- function(x)\n{\n  (x-min(x)/max(x)-min(x))\n}\n\niris[1:4]\napply(iris[1:4], 2, normalize)\n\n#####\n#2. CLUSTERING â A DATING APP FOR YOUR DATA POINTS#####\n#2.1.   Data Points and Distances in a Multidimensional Vector Space#####\n#2.2.   An Algorithmic Approach to Find Hidden Patterns in Data#####\n#2.3.   A Real-world Life Science Example#####\n\nlibrary(ggplot2)\nsource(\"http://www.bioconductor.org/biocLite.R\")\nbiocLite(\"GEOquery\")\nlibrary(Biobase)\nlibrary(GEOquery)\n\ngetwd()\nGDS5093_data <- file.path(getwd(), '03-Learning Data Mining with R/Datafiles/')\nGDS5088_data <- file.path(getwd(), '03-Learning Data Mining with R/Datafiles/')\n\nGDS5093 <- getGEO('GDS5093', destdir= GDS5093_data)\nGDS5088 <- getGEO('GDS5088', destdir= GDS5088_data)\nhead('GDS5088')\nhead('GDS5093')\n\ndfVirus = Table(GDS5093)\ndfMother = Table(GDS5088)\ndim(dfVirus)\ndim(dfMother)\ncolnames(dfMother)\ndfVirus[,2]\ndfMother[,2]\ncommonGenes = intersect(dfVirus[,2],dfMother[,2])\nlength(commonGenes)\nlength(dfVirus[,2])\nlength(dfMother[,2])\ndfVirusFilterMask = (dfVirus$IDENTIFIER %in% commonGenes) & !duplicated(dfVirus$IDENTIFIER)\ndfVirusFiltered = dfVirus[dfVirusFilterMask,]\ndfMotherFilterMask = dfMother$IDENTIFIER %in% commonGenes & !duplicated(dfMother$IDENTIFIER)\ndfMotherFiltered = dfMother[dfMotherFilterMask,]\ndim(dfVirusFiltered)\ndim(dfMotherFiltered)\n#qplot(dfVirusFiltered$IDENTIFIER,dfVirusFiltered$GSM1253056, geom = \"point\")\n\n\nmeanVirus=colMeans(apply(dfVirusFiltered[,3:dim(dfVirusFiltered)[[2]]],1, as.numeric)) \nmeanMother=colMeans(apply(dfMotherFiltered[,3:dim(dfMotherFiltered)[[2]]],1, as.numeric)) \ndfMeanVirus = data.frame(1:length(meanVirus),replicate(length(meanVirus),0),meanVirus)\ndfMeanMother = data.frame(1:length(meanMother),replicate(length(meanMother),1),meanMother)\n#gene sorting? #kommentare\ncolnames(dfMeanVirus)\ncolnames(dfMeanVirus)[1] = \"geneid\"\ncolnames(dfMeanVirus)[2] = \"sample\"\ncolnames(dfMeanVirus)[3] = \"mean\"\ncolnames(dfMeanMother)[1] = \"geneid\"\ncolnames(dfMeanMother)[2] = \"sample\"\ncolnames(dfMeanMother)[3] = \"mean\"\n\n\nresult = rbind(dfMeanVirus,dfMeanMother)\n\nqplot(result$geneid,result$mean, color=factor(result$sample))\nsubresult = result[result$geneid<100,]\nqplot(subresult$geneid,subresult$mean, color=factor(subresult$sample))\n\n######\n#3. R DEEP DIVE, WHY IS R REALLY COOL?#####\n#3.1.   Example â Using a Single Line of Code in R#####\n\nView(mtcars)\n\nmt1 = mtcars[mtcars$mpg>20,c('gear','mpg')]\nView(mt1)\n\nmt2 = aggregate(. ~ gear,mtcars[mtcars$mpg>20,c('gear','mpg')],mean)\nView(mt2)\n\nmt3 = subset(aggregate(. ~ gear,mtcars[mtcars$mpg>20,c('gear','mpg')],mean), mpg>25)\nView(mt3)\n\n#3.2.   R Data Types#####\n#3.3.   R Functions and Indexing#####\n#3.4.   S3 Versus S4 â Object-oriented Programming in R#####\n######\n#4. ASSOCIATION RULE MINING#####\n#4.1.   Market Basket Analysis#####\n\nrm(list = ls())\n\ninstall.packages('arules')\nlibrary(arules)\ntrans = read.transactions(\n  file=\n    url(\n      \"https://raw.githubusercontent.com/romeokienzler/developerWorks/master/market_baskets.csv\"), \n  sep = \",\")\ninspect(trans)\ntrans.matrix <- as(trans,\"matrix\") * 1\narticles <- colnames(trans.matrix) \nbuy.frequency <- 30\nminimum.support <- buy.frequency/length(trans)\nminimum.support\nconfidence <- 0.33\nitemFrequencyPlot(trans, support = minimum.support)\nitemFrequencyPlot(trans, topN = 10)\n\nrules <- apriori(trans.matrix, parameter = list(supp = minimum.support, conf = confidence, target = \"rules\"))\nrules\nsummary(rules)\ninspect(rules)\ninspect(sort(rules, by = \"support\")[1:10])\ninspect(sort(rules, by = \"confidence\")[1:10])\ninspect(sort(rules, by = \"lift\")[1:10])\nrules\n\ninstall.packages('digest')\ninstall.packages('scales')\ninstall.packages('arulesViz')\nlibrary(digest)\nlibrary(scales)\nlibrary(arulesViz)\nrequire(\"arulesViz\")\n\nplot(sort(rules, by = \"confidence\")[1:10], method=\"graph\", control=list(type=\"items\"))\n\n\neclat.itemsets <- eclat(trans.matrix, parameter = list(supp = minimum.support, maxlen = 7))\ninspect(eclat.itemsets)\nplot(eclat.itemsets)\n\n#4.2.   Introduction to Graphs#####\n#4.3.   Different Association Types#####\n\n# Different association types\n\n# - Boolean association\n# - Single dimensional association\n# - Multi dimensional association\n# - Correlation association\n# - Quantitative association\n\n# Boolean association\n\n# - Simplest example of an association\n# - Rules in the form 'if X then Y, if NOT X then NOT Y'\n# - The typical rule in market basket analysis\n# - if item X is present in the market basket then most likely item Y is also\n# - if item X is not present then most likely Y is also not present.\n\n\n# Single dimensional association\n\n# - Left hand side(LHS) and right hand side(RHS) of the rule are scalar\n# - The typical rule in market basket analysis\n# - if item X is present in the market basket then most likely item Y is also\n# - if item X is not present then most likely Y is also not present(Wher X and Y are a scalar item id for example).\n\n# Multi dimensional association\n\n# - Left hand side(LHS) and right hand side(RHS) of the rule can be vectors insted of scalar\n# - The typical rule in market basket analysis\n# - if item X is present in the market basket and the buyer is over 18 yers old and is a new customer then most likely item Y is bought\n# - The same in labeled vector notation\n\"(item = X, isOver18 = T, isNewCustomer = T) => (item = Y)\"\n# - Or the same vector in strict mathematical notation may look like\n\"(123,1,1) = 421 (123 and 412 indicate item IDs and 1 indicates True)\"\n\n# Correlation association\n\n# - Correlation based association are expressed as Correlation matrix\n# - Matrix expresses the probability of seeing two items together\n\"\nA   B   C\nA 1   0.3 0.7\nB 0.3 1   0.3\nC 0.7 0.3 1\n\n\"\n# Quantitative association\n\n# - so far only worked with categorical/ordinal data\n# - Rule Mining for quantitative data creates quantitative rules\n# - Example: If a buyer is 63 years old and a loyal customer since 2 or more\n    #years he most likely will buy 2 or more tooth brushes per year.\n\n#4.4.   The Apriori Algorithm#####\n#4.5.   The Eclat Algorithm#####\n#4.6.   The FP-Growth Algorithm#####\n######\n#5. CLASSIFICATION#####\n#5.1.   Mathematical Foundations#####\n#5.2.   The Naive Bayes Classifier#####\n#5.3.   Spam Classification with NaÃ¯ve Bayes#####\n\ninstall.packages('tm')\nlibrary(tm)\nlibrary(e1071)\n\n#The data can be obtained here: https://inclass.kaggle.com/c/adcg-ss14-challenge-02-spam-mails-detection/data\nsetwd(\"/Users/romeokienzler/Documents/romeo/Dropbox/arbeit/r/rkurs/lecture3/spamclassification/TR/\")\nlabels = read.csv('spam-mail.tr.label')\nlabels = labels[,2]\n#get a list of all email from the directory\nfile_list = list.files()\n\n#remove the first entry (doesn't contain an email)\nfile_list = file_list[-1]\n\n#create an empty data frame\ndf = data.frame(emailtext=character(),stringsAsFactors=FALSE) \n\n#add each email as row to the data frame\nfor (fileName in file_list){\n  text=readChar(fileName, file.info(fileName)$size)\n  df=rbind(df,data.frame(text,stringsAsFactors = FALSE))\n}\n\n#we have now 2500 emails in the data frame\ndim(df)\n\n\ndtm=DocumentTermMatrix(Corpus(VectorSource(df$text)))\ndim(dtm)\ninspect(dtm[2380:2385, 100000:100005])\n\n#load labels from separate file\nlabels\n\nfive_times_words <- findFreqTerms(sms_dtm_train, 5)\nlength(five_times_words)\n\nsms_train <- DocumentTermMatrix(spam_corpus_train, control=list(dictionary = five_times_words))\n\nsms_test <- DocumentTermMatrix(spam_corpus_test, control=list(dictionary = five_times_words))\n\n\nconvert_count <- function(x) {\n  y <- ifelse(x > 0, 1,0)\n  y <- factor(y, levels=c(0,1), labels=c(\"No\", \"Yes\"))\n  y\n}\n\ndtm_convert <- apply(dtm, 2, convert_count)\nclassifier <- naiveBayes(dtm_convert[1:1250,], factor(labels[1:1250]))\nclass(classifier)\npred <- predict(classifier, newdata=dtm_convert[1251:2500,])\ntable(pred, labels[1251:2500])\ntruthVector = pred == labels[1251:2500]\ngood = length(truthVector[truthVector==TRUE])\nbad = length(truthVector[truthVector==FALSE])\ngood/(good+bad)\n\n#5.4.   Support Vector Machines#####\n#5.5.   K-nearest Neighbors#####\n\nlibrary(caTools)\nPredictedClasses = knn(train = dfTrain, test = dfTest, cl =lables,k = 3)\n\n######\n#6. CLUSTERING#####\n#6.1.   Hierarchical Clustering#####\n#6.2.   Distribution-based Clustering#####\n\ninstall.packages('dbscan')\ninstall.packages('scatterplot3d')\n\nlibrary(dbscan)\nlibrary(scatterplot3d)\ndata(iris)\nView(iris)\nirisorg = iris\nhead(irisorg)\niris <- as.matrix(iris[,1:4])\n\nunique(irisorg$Species)\nscatterplot3d(iris[,1],iris[,4],iris[,3], highlight.3d=TRUE, col.axis=\"blue\", col.grid=\"lightblue\", main=\"scatterplot3d - 1\", pch=20)\nres = dbscan(iris, eps = .8, minPts = 4)\nlength(unique(res$cluster))\nmap = list(\"setosa\"=1, \"versicolor\"=2, \"virginica\"=3)\nspecieskey = unlist(lapply(irisorg$Species,  function(species) map[[species]]))\n\n\ntruthVectorValidate = res$cluster == specieskey\ngood = length(truthVectorValidate[truthVectorValidate==TRUE])\nbad = length(truthVectorValidate[truthVectorValidate==FALSE])\ngood/(good+bad)\n\n#6.3.   Density-based Clustering#####\n#6.4.   Using DBSCAN to Cluster Flowers Based on Spatial Properties#####\n######\n#7. COGNITIVE COMPUTING AND ARTIFICIAL INTELLIGENCE IN DATA MINING#####\n#7.1.   Introduction to Neural Networks and Deep Learning#####\n#7.2.   Using the H2O Deep Learning Framework#####\n\ninstall.packages('h2o')\n\nlibrary(h2o)\nlocalH2O =h2o.init(nthreads = -1)\n#Data can be downloaded here: https://www.kaggle.com/c/digit-recognizer/data\nMNIST_DIGITStrain = read.csv( 'train.csv' )\ndim(MNIST_DIGITStrain)\npar( mfrow = c(10,10), mai = c(0,0,0,0))\nfor(i in 1:100){\n  y = as.matrix(MNIST_DIGITStrain[i, 2:785])\n  dim(y) = c(28, 28)\n  image( y[,nrow(y):1], axes = FALSE, col = gray(255:0 / 255))\n  text( 0.2, 0, MNIST_DIGITStrain[i,1], cex = 3, col = 2, pos = c(3,4))\n}\n\n\nmfile = './train.csv'\n\nMDIG = h2o.importFile(path = mfile,sep=',')\n\n# Show the data objects on the H2O platform\nh2o.ls()\n\n#---\nNN_model = h2o.deeplearning(\n  x = 2:785,\n  training_frame = MDIG,\n  hidden = c(400, 200, 2, 200, 400 ),\n  epochs = 600,\n  activation = 'Tanh',\n  autoencoder = TRUE\n)\n\ntrain_supervised_features2 = h2o.deepfeatures(NN_model, MDIG, layer=3)\n\nplotdata2 = as.data.frame(train_supervised_features2)\nplotdata2$label = as.character(as.vector(MDIG[,1]))\n\nlibrary(ggplot2)\nqplot(DF.L3.C1, DF.L3.C2, data = plotdata2, color = label, main = 'Neural network: 400 - 200 - 2 - 200 - 400')\n\n#---\n\n\ntsData<-h2o.importFile(path = \"...train.csv\")\nres.dl <- h2o.deeplearning(x = 2:785, y = 1, training_frame = MDIG, activation = \"Tanh\",hidden=rep(160,5),epochs = 20)\npred.dl<-h2o.predict(object=res.dl,newdata=tsData[,-1])\npred.dl.df<-as.data.frame(pred.dl)\ntsData.df<-as.data.frame(tsData)\n\ntruthVector = tsData.df$label == as.integer(round(pred.dl.df[,1]))\ngood = length(truthVector[truthVector==TRUE])\nbad = length(truthVector[truthVector==FALSE])\ngood/(good+bad)\n\n#7.3.   Real-time Cloud Based IoT Sensor Data Analysis#####\n\nlibrary(scatterplot3d)\nlibrary(ggplot2)\nlibrary(stats)\nlibrary(h2o)\nlocalH2O = h2o.init(nthreads = -1)\nsimuURL=\"https://pmqsimulator-romeokienzler-1155.mybluemix.net/data\"\ndf = read.csv(simuURL,sep = \";\",header = FALSE)\ndim(df)\ncolnames(df) = c(\"t\",\"x\",\"y\",\"z\")\nscatterplot3d(df$x,df$y,df$z, type = \"l\")\n\nggplot() +\n  geom_line(data = df, aes(x = t, y = x),colour = \"red\") +\n  geom_line(data = df, aes(x = t, y = y), colour = \"blue\") +\n  geom_line(data = df, aes(x = t, y = z), colour = \"green\")\n\n\nxfft = Im(fft(df$x))\nyfft = Im(fft(df$y))\nzfft = Im(fft(df$z))\nggplot() +\n  geom_line(data = df, aes(x = t, y = xfft),colour = \"red\")  +\n  geom_line(data = df, aes(x = t, y = yfft), colour = \"blue\") +\n  geom_line(data = df, aes(x = t, y = zfft), colour = \"green\")\n\ndf = read.csv(simuURL,sep = \";\",header = FALSE)\ndim(df)\ncolnames(df) = c(\"t\",\"x\",\"y\",\"z\")\n\nxfft = Im(fft(df$x))\nyfft = Im(fft(df$y))\nzfft = Im(fft(df$z))\nggplot() +\n  geom_line(data = df, aes(x = t, y = xfft),colour = \"red\")  +\n  geom_line(data = df, aes(x = t, y = yfft), colour = \"blue\") +\n  geom_line(data = df, aes(x = t, y = zfft), colour = \"green\")\n\ntsDataPrev=NULL\nwhile (TRUE) {\n  print(\"reading data...\")\n  df = read.csv(simuURL,sep = \";\",header = FALSE)\n  print(\"done\")\n  colnames(df) = c(\"t\",\"x\",\"y\",\"z\")\n  xfft = Im(fft(df$x))\n  yfft = Im(fft(df$y))\n  zfft = Im(fft(df$z))\n  tsData = as.h2o(t(data.frame(xfft,yfft,zfft)))\n  NN_model = h2o.deeplearning(\n    x = 1:3,\n    training_frame = tsData,\n    hidden = c(3000, 2, 3000),\n    epochs = 300,\n    activation = 'Tanh',\n    autoencoder = TRUE\n  )\n  \n  anomalyrates = as.data.frame(h2o.anomaly(NN_model,tsData))\n  print(sum(anomalyrates))\n  flush.console()\n  if (!is.null(tsDataPrev)) {\n    anomalyrates = as.data.frame(h2o.anomaly(NN_model,tsDataPrev))\n    print(sum(anomalyrates))\n    flush.console()   \n  }\n  tsDataPrev=tsData;\n}\n\n#############################################################################\n\n#4 Learning R for Data Visualization#####\n#############################################################################\n#What You Will Learn                                                        #\n#                                                                           #\n#See how to plot a distribution with histograms and box-plot                #\n#Deepen your knowledge by adding bar-charts, scatterplots, and time series plots using ggplot2#\n#Enhance the user experience using dynamic visualisation                    #\n#Save your work for publication, in tiff, at a good resolution              #\n#Test your coding limits by creating stunning interactive plots for the web #\n#Create a fully-featured website using Shiny with real-time features such as adding and controlling functionalities#\n#############################################################################\n\n#1. INTRODUCING SCIENTIFIC PLOTTING IN R#####\n\n#1.1.   The Course Overview#####\n\n'http://www.fabioveronesi.net/'\n'http://r-video-tutorial.blogspot.ch/'\n\n#1.2.   Preview of R Plotting Functionalities#####\n\n# Import ggplot2\nlibrary(ggplot2)\n\n#Set the working directory where the dataset is stored\nsetwd(\"~/Data Science with R/04-Learning R for Data Visualization/Code\")\n\n#Loading CSV Files\nData <- read.table(file=\"EPA_Data.csv\", \n                   sep=\",\", \n                   header=TRUE, \n                   colClasses=c(\"Date\",\"factor\",rep(\"numeric\",5)), \n                   na.string=\"NA\")\n\nhead(Data)\nhelp(read.table)\n\n#Examining the data\nstr(Data)\n\n# Simple histogram in R\n\ndev.off()\n\nhist(Data$NO2)\n\n# Adding additional Elements\nhist(Data$NO2, \n     breaks = 30,\n     col = 'grey',\n     xlab = 'NO2',\n     main = 'Histogram')\n\n# Plot a simple Histogram of NO2\n\nggplot(data = Data, aes(x = NO2)) + \n  geom_histogram() +\n  ggtitle('Histogram of NO2\\nCalifornia')\n\n\n#1.3.   Introducing the Dataset#####\n#1.4.   Loading Tables and CSV Files#####\n\n#Set the working directory where the dataset is stored\nsetwd(\"E:/OneDrive/R Video Course - Packt/Data\")\n\n\n#Loading CSV Files\nData <- read.table(file=\"EPA_Data.csv\", \n                   sep=\",\", \n                   header=TRUE, \n                   colClasses=c(\"Date\",\"factor\",rep(\"numeric\",5)), \n                   na.string=\"NA\")\n\nhelp(read.table)\n\n#Examining the data\nstr(Data)\n\n#1.5.   Loading Excel Files#####\n\n#Loading the required libraries\ninstall.packages(c(\"xlsx\",\"rJava\"))\n\n#Java needs to be installed on the system in rder to load the package xlsx\n#http://java.com/en/download/manual.jsp\n#Careful to install the correct version of Java for 64bit machines\n\nlibrary(xlsx)\n\n#Set the working directory where the dataset is stored\nsetwd(\"~/Data Science with R/04-Learning R for Data Visualization/Code\")\n\n#Loading Excel Files\nData.xlsx <- read.xlsx(file=\"EPA_Data.xlsx\",sheetName=\"Sheet1\")\n\n#Examining the data\nstr(Data.xlsx)\n\n#1.6.   Exporting Data#####\n\n#Loading the required libraries\nlibrary(xlsx)\n\n#Set the working directory where the dataset is stored\nsetwd(\"~/Data Science with R/04-Learning R for Data Visualization/Code\")\n\n#Loading CSV Files\nData <- read.table(file=\"EPA_Data.csv\", sep=\",\", header=TRUE, colClasses=c(\"Date\",\"factor\",rep(\"numeric\",5)), na.string=\"NA\")\n\n#Subsetting the data\nCalifornia <- Data[Data$State==\"California\",]\n\n#For more information on subsetting data in R\n#please refer to:\n#http://www.ats.ucla.edu/stat/r/modules/subsetting.htm\n#http://www.statmethods.net/management/subset.html\n\n#Exporting the results\nwrite.table(California, file=\"Data/California.csv\", sep=\",\", row.names=F)\nwrite.table(California, file=\"Data/California.txt\", sep=\" \", row.names=F)\nwrite.xlsx(California, file=\"Data/California.xlsx\", sheetName=\"Air Pollution Data\", row.names=F)\n\n#Separating EXCEL Sheets for each pollutant\nwrite.xlsx(California[,c(\"Date\",\"Temperature\",\"CO\")], file=\"California.xlsx\", sheetName=\"CO\", row.names=F)\nwrite.xlsx(California[,c(\"Date\",\"Temperature\",\"NO2\")], file=\"California.xlsx\", sheetName=\"NO2\", row.names=F, append=T)\nwrite.xlsx(California[,c(\"Date\",\"Temperature\",\"SO2\")], file=\"California.xlsx\", sheetName=\"SO2\", row.names=F, append=T)\n\n######\n#2. SCIENTIFIC PLOTTING IN GGPLOT2#####\n#2.1.   Creating Histograms#####\n\n#Import ggplot2\nlibrary(ggplot2)\n\n#Set the working directory where the dataset is stored\nsetwd(\"~/Data Science with R/04-Learning R for Data Visualization/Code\")\n\n#Load the dataset in R\nData <- read.table(file=\"EPA_Data.csv\", \n                   sep=\",\", \n                   header=TRUE, \n                   colClasses=c(\"Date\",\"factor\",rep(\"numeric\",5)), \n                   na.string=\"NA\")\n\n\n#Explore the dataset to identify the variables\nstr(Data)\n\n\n#Plot a simple Histogram of NO2\nggplot(data=Data, aes(x=NO2)) + \n  geom_histogram() \n\n\n#Use Faceting to plot multiple histograms\nggplot(data=Data, aes(x=NO2)) + \n  geom_histogram(binwidth = 1) +\n  facet_wrap(~State)\n\n#2.2.   The Importance of Box Plots#####\n\n#Import ggplot2\nlibrary(ggplot2)\n\n#Set the working directory where the dataset is stored\nsetwd(\"~/Data Science with R/04-Learning R for Data Visualization/Code\")\n\n#Load the dataset in R\nData <- read.table(file=\"EPA_Data.csv\", \n                   sep=\",\", \n                   header=TRUE, \n                   colClasses=c(\"Date\",\"factor\",rep(\"numeric\",5)), \n                   na.string=\"NA\")\n\n\n\n#Create simple Box-Plots\nggplot(data=Data, aes(x=State, y=NO2)) + \n  geom_boxplot()\n\n\n#Ordered\nordered <- c(\"Maine\",\"Iowa\",\"Texas\",\"California\",\"Ohio\",\"New York\")\n\nggplot(data=Data, aes(x=State, y=NO2)) + \n  geom_boxplot() +\n  scale_x_discrete(limits=ordered)\n\n#2.3.   Plotting Bar Charts#####\n\n#Import ggplot2\nlibrary(ggplot2)\n\n#Set the working directory where the dataset is stored\nsetwd(\"~/Data Science with R/04-Learning R for Data Visualization/Code\")\n\n#Load the dataset in R\nData <- read.table(file=\"EPA_Data.csv\", \n                   sep=\",\", \n                   header=TRUE, \n                   colClasses=c(\"Date\",\"factor\",rep(\"numeric\",5)), \n                   na.string=\"NA\")\n\n\n#Let us create a subset of our data,\n#including just observations on October the 1st\nPollution_1stOct2013 <- Data[Data$Date==\"2013-10-01\",]\n\n\n#Explore the dataset to identify the variables\nstr(Pollution_1stOct2013)\n\n\n#Plot a simple Bar-Chart\nggplot(data=Pollution_1stOct2013, aes(x=State, y=SO2)) +\n  geom_bar(stat=\"identity\") \n\n\n#Order the columns\nordered <- Pollution_1stOct2013[order(Pollution_1stOct2013$SO2),]\n\nggplot(data=Pollution_1stOct2013, aes(x=State, y=SO2)) +\n  geom_bar(stat=\"identity\") +\n  scale_x_discrete(limits=ordered$State)\n\n#2.4.   Plotting Multiple Variables â Scatterplots#####\n\n#Import ggplot2\nlibrary(ggplot2)\n\n#Set the working directory where the dataset is stored\nsetwd(\"~/Data Science with R/04-Learning R for Data Visualization/Code\")\n\n#Load the dataset in R\nData <- read.table(file=\"EPA_Data.csv\", \n                   sep=\",\", \n                   header=TRUE, \n                   colClasses=c(\"Date\",\"factor\",rep(\"numeric\",5)), \n                   na.string=\"NA\")\n\n#Extract only the measurements in California\nCalifornia <- Data[Data$State==\"California\",]\n\n\n#Create a simple scatterplot\nggplot(data=California, aes(x=NO2, y=CO)) +\n  geom_point() \n\n\n#Increase the sizes of the dots\nggplot(data=California, aes(x=NO2, y=CO)) +\n  geom_point(size=3) \n\n\n#Increase the sizes of the dots\nggplot(data=California, aes(x=NO2, y=SO2, color=Temperature, size=CO)) +\n  geom_point()\n\n#2.5.   Dealing with Time â Time-series Plots#####\n\n#Import ggplot2\nlibrary(ggplot2)\n\n#Set the working directory where the dataset is stored\nsetwd(\"~/Data Science with R/04-Learning R for Data Visualization/Code\")\n\n#Load the dataset in R\nData <- read.table(file=\"EPA_Data.csv\", \n                   sep=\",\", \n                   header=TRUE, \n                   colClasses=c(\"Date\",\"factor\",rep(\"numeric\",5)), \n                   na.string=\"NA\")\n\n#Extract only the measurements in California\nCalifornia <- Data[Data$State==\"California\",]\n\n\n#Create a simple Time-Series plot\nggplot(data=California, aes(x=Date, y=NO2)) +\n  geom_line() \n\n\n#Add data as colors and change the line size\nggplot(data=California, aes(x=Date, y=NO2, color=CO)) +\n  geom_line(size=1)\n\n#2.6.   Handling Uncertainty#####\n\n#Import ggplot2\nlibrary(ggplot2)\n\n#Set the working directory where the dataset is stored\nsetwd(\"~/Data Science with R/04-Learning R for Data Visualization/Code\")\n\n#Load the dataset in R\nWeekly_Values <- read.table(\"Weekly_Values.csv\",sep=\",\",header=T)\nordered <- Weekly_Values[order(Weekly_Values$SO2.mean),]\n\nstr(Weekly_Values)\n\n\n#Plot Error Bars\nggplot(data=Weekly_Values, aes(x=State, y=SO2.mean)) +\n  geom_bar(stat=\"identity\", fill=\"grey\") +\n  scale_x_discrete(limits=ordered$State) +\n  geom_errorbar(data=Weekly_Values, aes(ymin=SO2.mean-(2*SO2.se), \n                                        ymax=SO2.mean+(2*SO2.se)), width=0.1)\n\n\n#Error bars in scatterplots\nggplot(data=Weekly_Values, aes(x=NO2.mean, y=SO2.mean)) +\n  geom_point(stat=\"identity\", fill=\"grey\") +\n  geom_errorbar(data=Weekly_Values, aes(ymin=SO2.mean-SO2.se, \n                                        ymax=SO2.mean+SO2.se), width=0.01)+\n  geom_errorbarh(data=Weekly_Values, aes(xmin=NO2.mean-NO2.se, \n                                         xmax=NO2.mean+NO2.se), height=0.01)\n\n######\n#3. CUSTOMIZING PLOTS#####\n#3.1.   Changing Theme#####\n\n#Import ggplot2\nlibrary(ggplot2)\n\n#Set the working directory where the dataset is stored\nsetwd(\"E:/OneDrive/R Video Course - Packt/Data\")\n\n#Load the dataset in R\nData <- read.table(file=\"EPA_Data.csv\", \n                   sep=\",\", \n                   header=TRUE, \n                   colClasses=c(\"Date\",\"factor\",rep(\"numeric\",5)), \n                   na.string=\"NA\")\n\n\n#Default Theme\nggplot(data=Data, aes(NO2)) + \n  geom_histogram()\n\n\n#Theme Minimal\n#No Background, no Axes lines, light gridilines\nggplot(data=Data, aes(NO2)) + \n  geom_histogram() +\n  theme_minimal()\n\n\n#Theme Light\n#No Background, light box around the plot, light gridilines\nggplot(data=Data, aes(NO2)) + \n  geom_histogram() +\n  theme_light()\n\n\n#Theme Classic\n#No Background, tick axes lines, no gridilines\nggplot(data=Data, aes(NO2)) + \n  geom_histogram() +\n  theme_classic()\n\n#3.2.   Changing Colors#####\n\n#Import ggplot2\nlibrary(ggplot2)\n\n#Set the working directory where the dataset is stored\nsetwd(\"E:/OneDrive/R Video Course - Packt/Data\")\n\n#Load the dataset in R\nData <- read.table(file=\"EPA_Data.csv\", \n                   sep=\",\", \n                   header=TRUE, \n                   colClasses=c(\"Date\",\"factor\",rep(\"numeric\",5)), \n                   na.string=\"NA\")\n\n\n#Here we are goign to use the subsets we created before\nCalifornia <- Data[Data$State==\"California\",]\n\n\n\n#Default: from dark blue to light blue\nggplot(data=California, aes(x=NO2, y=CO, color=Temperature)) +\n  geom_point() \n\n\n#Change the two colors\nggplot(data=California, aes(x=NO2, y=CO, color=Temperature)) +\n  geom_point() +\n  scale_color_gradient(low=\"orange\", high=\"red\")\n\n\n#More info about all the colors available in R here:\n#http://research.stowers-institute.org/efg/R/Color/Chart/\n\n\n#Change between n colors\nggplot(data=California, aes(x=NO2, y=CO, color=Temperature)) +\n  geom_point() +\n  scale_color_gradientn(colours=c(\"blue\",\"light blue\",\"green\",\"orange\",\"red\"))\n\n#3.3.   Modifying Axis and Labels#####\n\n#Import ggplot2\nlibrary(ggplot2)\n\n#Set the working directory where the dataset is stored\nsetwd(\"E:/OneDrive/R Video Course - Packt/Data\")\n\n#Load the dataset in R\nData <- read.table(file=\"EPA_Data.csv\", \n                   sep=\",\", \n                   header=TRUE, \n                   colClasses=c(\"Date\",\"factor\",rep(\"numeric\",5)), \n                   na.string=\"NA\")\n\n\n#Here we are goign to use the subsets we created before\nCalifornia <- Data[Data$State==\"California\",]\n\n\n\n\n#Add a Title\nggplot(data=California, aes(x=NO2, y=CO, color=Temperature)) +\n  geom_point() +\n  scale_color_gradientn(colours=c(\"blue\",\"light blue\",\"green\",\"orange\",\"red\")) +\n  theme_classic() +\n  labs(title = \"Scatterplot\")\n\n\n\n#Change the Title of the Legend\nggplot(data=California, aes(x=NO2, y=CO, color=Temperature)) +\n  geom_point() +\n  scale_color_gradientn(colours=c(\"blue\",\"light blue\",\"green\",\"orange\",\"red\")) +\n  theme_classic() +\n  labs(title = \"Scatterplot\", \n       colour = \"Temp. (?C)\")\n\n\n\n#Change the X axis label\nggplot(data=California, aes(x=NO2, y=CO, color=Temperature)) +\n  geom_point() +\n  scale_color_gradientn(colours=c(\"blue\",\"light blue\",\"green\",\"orange\",\"red\")) +\n  theme_classic() +\n  labs(title = \"Scatterplot\", \n       colour = \"Temp. (?C)\") +\n  xlab(\"Nitrogen Dioxide (ppm)\") +\n  ylab(\"Carbon Monoxide (ppm)\")\n\n#3.4.   Adding Supplementary Elements#####\n\n#Import ggplot2\nlibrary(ggplot2)\n\n#Set the working directory where the dataset is stored\nsetwd(\"E:/OneDrive/R Video Course - Packt/Data\")\n\n#Load the dataset in R\nData <- read.table(file=\"EPA_Data.csv\", \n                   sep=\",\", \n                   header=TRUE, \n                   colClasses=c(\"Date\",\"factor\",rep(\"numeric\",5)), \n                   na.string=\"NA\")\n\n\n#Here we are goign to use the subsets we created before\nCalifornia <- Data[Data$State==\"California\",]\n\nPollution_1stOct2013 <- Data[Data$Date==\"2013-10-01\",]\n\n\n#Add a Linear Trend Line\nggplot(data=California, aes(x=NO2, y=SO2)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE)\n\n\n\n#Add a quadratic Trend Line\nggplot(data=California, aes(x=NO2, y=SO2)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula=y ~ poly(x, 2), se = TRUE)\n\n\n\n#Add Vertical Lines\nggplot(data=California, aes(NO2)) + \n  geom_histogram() +\n  geom_vline(xintercept = mean(California$NO2),\n             color=\"red\",\n             linetype = \"longdash\")\n#More info on the line types available can be found here:\n#http://sape.inf.usi.ch/quick-reference/ggplot2/linetype\n\n\n\n#Add Horizontal Lines\nordered <- Pollution_1stOct2013[order(Pollution_1stOct2013$SO2),]\n\nggplot(data=Pollution_1stOct2013, aes(x=State, y=SO2)) +\n  geom_bar(stat=\"identity\") +\n  scale_x_discrete(limits=ordered$State) +\n  geom_hline(yintercept = 0.5,\n             color=\"red\",\n             linetype = \"longdash\")\n\n#3.5.   Adding Text Inside and Outside of the Plot#####\n\n#Import ggplot2\nlibrary(ggplot2)\n\n#Set the working directory where the dataset is stored\nsetwd(\"E:/OneDrive/R Video Course - Packt/Data\")\n\n#Load the dataset in R\nData <- read.table(file=\"EPA_Data.csv\", \n                   sep=\",\", \n                   header=TRUE, \n                   colClasses=c(\"Date\",\"factor\",rep(\"numeric\",5)), \n                   na.string=\"NA\")\n\n\n#Here we are goign to use the subsets we created before\nCalifornia <- Data[Data$State==\"California\",]\n\nPollution_1stOct2013 <- Data[Data$Date==\"2013-10-01\",]\n\n\n#Add Fixed Text Label\nggplot(data=California, aes(NO2)) + \n  geom_histogram() +\n  geom_vline(xintercept = mean(California$NO2),\n             color=\"red\",\n             linetype = \"longdash\") +\n  geom_text(aes(label=\"Mean\", x=11, y=25, angle=90), color=\"red\")\n\n\n\n#Add Dynamic Text Label\nordered <- Pollution_1stOct2013[order(Pollution_1stOct2013$SO2),]\n\nggplot(data=Pollution_1stOct2013, aes(x=State, y=SO2)) +\n  geom_bar(stat=\"identity\", fill=\"light gray\") +\n  scale_x_discrete(limits=ordered$State) +\n  theme_classic() +\n  geom_text(aes(label=paste(Pollution_1stOct2013$SO2), \n                x=State, \n                y=SO2, \n                angle=0), color=\"red\")\n\n\n#Change the Axis Labels\nggplot(data=Pollution_1stOct2013, aes(x=State, y=SO2)) +\n  geom_bar(stat=\"identity\") +\n  scale_x_discrete(limits=ordered$State, \"State\", \n                   labels = c(\"ME\",\"TX\",\"CA\",\"IA\",\"NY\",\"OH\"))\n\n\n\n#Change the Axis Labels\nggplot(data=California, aes(x=Temperature, y=Pressure)) +\n  geom_point() +\n  xlab(\"\") +\n  scale_x_continuous(breaks=seq(0,30,5), \n                     label=paste(seq(0,30,5),\"?C\")) +\n  ylab(\"\") +\n  scale_y_continuous(breaks=seq(950,1000,10), \n                     label=paste(seq(950,1000,10),\"hPA\"))\n\n#3.6.   Multi-plots#####\n\n#Import ggplot2\nlibrary(ggplot2)\n\n#Set the working directory where the dataset is stored\nsetwd(\"E:/OneDrive/R Video Course - Packt/Data\")\n\n#Load the dataset in R\nData <- read.table(file=\"EPA_Data.csv\", \n                   sep=\",\", \n                   header=TRUE, \n                   colClasses=c(\"Date\",\"factor\",rep(\"numeric\",5)), \n                   na.string=\"NA\")\n\n\n#Here we are goign to use the subsets we created before\nCalifornia <- Data[Data$State==\"California\",]\n\n\n\n#Multiple plots in one graph\nggplot(data=Data, aes(SO2)) + \n  geom_histogram(binwidth = 0.1) + \n  facet_wrap(~State)\n\n\n\n#Different plots side-by-side\ninstall.packages(\"gridExtra\")\nlibrary(gridExtra)\n\n\n#Vertical Stacking\nplot1 <- ggplot(data=California, aes(NO2)) + \n  geom_histogram(binwidth = 0.5) +\n  ylab(\"\") +\n  scale_y_continuous(breaks=NULL)\n\nplot2 <- ggplot(data=California, aes(x=1, y=NO2)) +\n  geom_boxplot(width=0.5) +\n  coord_flip() +\n  xlab(\"\") +\n  scale_x_continuous(breaks=NULL)\n\ngrid.arrange(plot1, plot2, nrow=2)\n\n\n\n\n#Horizontal Stacking\nplot1 <- ggplot(data=California, aes(NO2)) + \n  geom_histogram(binwidth = 0.5)\n\nplot2 <- ggplot(data=California, aes(x=Date, y=NO2)) +\n  geom_line()\n\ngrid.arrange(plot1, plot2, ncol=2)\n\n\n######\n#4. EXPORTING PLOTS#####\n#4.1.   Exporting Plots as Images#####\n\n#Import ggplot2\nlibrary(ggplot2)\n\n#Set the working directory where the dataset is stored\nsetwd(\"E:/OneDrive/R Video Course - Packt/Data\")\n\n#Load the dataset in R\nData <- read.table(file=\"EPA_Data.csv\", \n                   sep=\",\", \n                   header=TRUE, \n                   colClasses=c(\"Date\",\"factor\",rep(\"numeric\",5)), \n                   na.string=\"NA\")\n\n\n#Here we are goign to use the subsets we created before\nCalifornia <- Data[Data$State==\"California\",]\n\n\n#Create a plot as an object\nImage_Plot <- ggplot(data=California, aes(x=NO2, y=SO2, color=Temperature)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  theme_classic() +\n  labs(title = \"Scatterplot\", \n       colour = \"Temp. (Â°C)\") +\n  xlab(\"Nitrogen Dioxide (ppm)\") +\n  ylab(\"Carbon Monoxide (ppm)\")\n\n\n#Check the plot\nprint(Image_Plot)\n\n\n#Save as jpeg image\nggsave(filename=\"NamePlot.jpeg\", plot=Image_Plot)\n\n\n\n#Save as tiff image\nggsave(filename=\"NamePlot.tiff\", plot=Image_Plot)\n\n\n\n#Change the size\nggsave(filename=\"NamePlot.tiff\", plot=Image_Plot, width=14, height=8, units=\"cm\")\n\n\n\n#Understand the differences between Bitmap and Vector images\n#http://www.bbc.co.uk/schools/gcsebitesize/dida/graphics/bitmapvectorrev3.shtml\n#http://etc.usf.edu/techease/win/images/what-is-the-difference-between-bitmap-and-vector-images/\n\n\n#Save as PDF\nggsave(filename=\"NamePlot.pdf\", plot=Image_Plot)\n\n\n#Save as SVG\nggsave(filename=\"NamePlot.svg\", plot=Image_Plot)\n\n\n#4.2.   Adjusting the Page Size#####\n\n#Import ggplot2\nlibrary(ggplot2)\n\n#Set the working directory where the dataset is stored\nsetwd(\"E:/OneDrive/R Video Course - Packt/Data\")\n\n#Load the dataset in R\nData <- read.table(file=\"EPA_Data.csv\", \n                   sep=\",\", \n                   header=TRUE, \n                   colClasses=c(\"Date\",\"factor\",rep(\"numeric\",5)), \n                   na.string=\"NA\")\n\n\n#Here we are goign to use the subsets we created before\nCalifornia <- Data[Data$State==\"California\",]\n\n\n#Create a plot as an object\nImage_Plot <- ggplot(data=California, aes(x=NO2, y=SO2, color=Temperature)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  theme_classic() +\n  labs(title = \"Scatterplot\", \n       colour = \"Temp. (Â°C)\") +\n  xlab(\"Nitrogen Dioxide (ppm)\") +\n  ylab(\"Carbon Monoxide (ppm)\")\n\n\n#Check the plot\nprint(Image_Plot)\n\n\n\n#Save in A4 - Portrait\nggsave(filename=\"NamePlot.pdf\", plot=Image_Plot, paper=\"a4\")\n\n\n\n#Save in A4 - Landscape\nggsave(filename=\"NamePlot.pdf\", plot=Image_Plot, paper=\"a4r\")\n\n\n\n#Other possible formats:\n#\"letter\"\n#\"legal\"\n#\"executive\"\n\n\n#Customize the page size\n#B5\nggsave(filename=\"NamePlot.pdf\", plot=Image_Plot, width=250, height=176, units=\"mm\")\n\n#Page sizes can be found here:\n#http://www.papersizes.org/a-paper-sizes.htm\n#http://www.papersizes.org/b-paper-sizes.htm\n\n\n#Change Resolution\n#ggplot2 save at 300dpi by default\nggsave(filename=\"NamePlot.tiff\", plot=Image_Plot)\n\n\n\n#Change the resolution\nggsave(filename=\"NamePlot.tiff\", plot=Image_Plot, dpi=600)\n\n\n\n######\n#5. INTERACTIVE PLOTS IN RCHARTS#####\n#5.1.   Getting Started with Interactive Plotting#####\n\n#Install the pachake devtools and load it\ninstall.packages(\"devtools\")\nrequire(devtools)\n\n#Install from GitHub\ninstall_github('rCharts', 'ramnathv')\n\n#Load rCharts\nlibrary(rCharts)\n\n\n#Run a demo to make sure everything is correctly installed.\n#Follow the instruction in the console below.\ndemo(polycharts)\n\n\n\n#rCharts Homepage\n#http://rcharts.io/\n\n#rCharts Documentation:\n#https://media.readthedocs.org/pdf/rcharts/latest/rcharts.pdf\n\n\n#5.2.   Creating Interactive Histograms and Box Plots#####\n\n#Load rCharts\nlibrary(rCharts)\n\n#Set the working directory where the dataset is stored\nsetwd(\"E:/OneDrive/R Video Course - Packt/Data\")\n\n#Load the dataset in R\nData <- read.table(file=\"EPA_Data.csv\", \n                   sep=\",\", \n                   header=TRUE, \n                   colClasses=c(\"Date\",\"factor\",rep(\"numeric\",5)), \n                   na.string=\"NA\")\n\n\n#Here we are goign to use the subsets we created before\nCalifornia <- Data[Data$State==\"California\",]\n\n\n\n#First we need to collect bins and frequencies in a data.frame\nhistogram <- hist(California$NO2, \n                  breaks=seq(0,max(California$NO2)+1,1), \n                  plot=F)\n\ndat <- data.frame(Frequency=histogram$counts, \n                  NO2=histogram$mids)\n\n\n\n#Learning the syntax of rCharts\nhist.int <- nPlot(x = \"NO2\", y = \"Frequency\", data = dat, \n                  type = \"discreteBarChart\")\nhist.int\n\n\n\n#Adding elements\nhist.int <- nPlot(x = \"NO2\", y = \"Frequency\", \n                  data = dat, type = \"discreteBarChart\")\nhist.int$chart(color=rep(\"blue\",nrow(dat)))\nhist.int$xAxis(axisLabel=\"NO2\")\nhist.int\n\n\n#We can add some javascript code for more flexibility\nhist.int <- nPlot(x = \"NO2\", y = \"Frequency\", \n                  data = dat, type = \"discreteBarChart\")\nhist.int$chart(color=rep(\"blue\",nrow(dat)))\nhist.int$xAxis(axisLabel=\"NO2\")\nhist.int$chart(tooltipContent = \"#! function(key, x, y){ \n               return 'Frequency: ' + y + '<br> NO2 (ppm): ' + x \n               } !#\")\nhist.int\n\n\n#Thanks to Ramnath Vaidyanathan (https://github.com/ramnathv) for some of the code presented here\n\n\n\n\n#compute boxplot statistics and cast it as a dataframe with no headers\nNO2.stats <- setNames(\n  as.data.frame(boxplot(NO2 ~ State, data = Data, plot = F)$stats),\n  nm = NULL\n)\n\n\n\n#Box-Plots\n#Create a new Highchart\nh1 <- Highcharts$new()\nh1$set(series = list(list(name = \"NO2 (ppm)\", data = NO2.stats)))\n\n\n#Add axis details\nh1$xAxis(\n  categories = levels(Data$State),\n  title = list(text = 'State')\n)\n\nh1$yAxis(\n  title = list(text = 'NO2 (ppm)')  \n)\n\n#Define the type of plot\nh1$chart(type = 'boxplot')\nh1\n\n\n\nSO2.stats <- setNames(\n  as.data.frame(boxplot(SO2 ~ State, data = Data, plot = F)$stats),\n  nm = NULL\n)\n\nh1 <- Highcharts$new()\nh1$set(series = list(list(name = c(\"NO2 (ppm)\",\"SO2 (ppm)\"), \n                          data = c(NO2.stats,SO2.stats))))\n\n\n#Add axis details\nh1$xAxis(\n  categories = levels(Data$State),\n  title = list(text = 'State')\n)\n\nh1$yAxis(\n  title = list(text = c(\"NO2 (ppm)\",\"SO2 (ppm)\")) \n)\n\n\n#Define the type of plot\nh1$chart(type = 'boxplot')\nh1\n\n\n#SOURCE: https://github.com/ramnathv/rCharts/issues/294\n\n#5.3.   Plotting Interactive Bar Charts#####\n\n#Load rCharts\nlibrary(rCharts)\nlibrary(xlsx)\n\n#Set the working directory where the dataset is stored\nsetwd(\"E:/OneDrive/R Video Course - Packt/Data\")\n\nseasons <- read.xlsx(\"Season_Reading.xlsx\",sheetName=\"Summary\")\n\nstr(seasons)\n\n\n\n#Basic Plot\nbar.int <- nPlot(y=\"SO2.mean\", x=\"Season\", group=\"State\", data = seasons, \n                 type = \"multiBarChart\")\nbar.int\n\n\n#Add Elements\nbar.int <- nPlot(y=\"SO2.mean\", x=\"Season\", group=\"State\", data = seasons, \n                 type = \"multiBarChart\")\nbar.int$yAxis(axisLabel=\"SO2 (ppm)\")\nbar.int$xAxis(axisLabel=\"Seasons\")\nbar.int$chart(margin=list(left=100))\nbar.int\n\n\n#Include a Template\nbar.int <- nPlot(y=\"SO2.mean\", x=\"Season\", group=\"State\", data = seasons, \n                 type = \"multiBarChart\")\nbar.int$templates$script <- \"http://timelyportfolio.github.io/rCharts_nvd3_templates/chartWithTitle_styled.html\"\nbar.int$set(title = \"Seasonality of SO2\")\nbar.int$yAxis(axisLabel=\"SO2 (ppm)\")\nbar.int$xAxis(axisLabel=\"Seasons\")\nbar.int$chart(margin=list(left=100))\nbar.int\n\n#SOURCE: https://github.com/timelyportfolio/rcharts_nvd3_templates\n\n#5.4.   Creating Interactive Scatterplots#####\n\n#Load rCharts\nlibrary(rCharts)\n\n#Set the working directory where the dataset is stored\nsetwd(\"E:/OneDrive/R Video Course - Packt/Data\")\n\n#Load the dataset in R\nData <- read.table(file=\"EPA_Data.csv\", \n                   sep=\",\", \n                   header=TRUE, \n                   colClasses=c(\"Date\",\"factor\",rep(\"numeric\",5)), \n                   na.string=\"NA\")\n\n#Extract only the measurements in California\nCalifornia <- Data[Data$State==\"California\",]\n\n\n#Let's start with a basic multiple scatterplot\nscat.int <- nPlot(x=\"NO2\", y=\"SO2\", group=\"State\", \n                  data=Data, type=\"scatterChart\") \nscat.int\n\n\n#We can change the size of the points \n#by adding another simple javascript function\nscat.int <- nPlot(x=\"NO2\", y=\"SO2\", group=\"State\", \n                  data=Data, type=\"scatterChart\") \nscat.int$xAxis(axisLabel=\"NO2 (ppm)\")\nscat.int$yAxis(axisLabel=\"SO2 (ppm)\")\nscat.int$chart(size = '#! function(d){return d.CO} !#')\nscat.int$chart(tooltipContent = \"#! function(key, x, y, e){ \n               return 'NO2 (ppm): ' + x + \n               '<br> SO2 (ppm): ' + y +\n               '<br> CO (ppm): ' + e.point.CO\n               } !#\")\nscat.int\n\n\n#We can even add controls to choose different variables\nscat.int <- nPlot(x=\"NO2\", y=\"SO2\", group=\"State\", \n                  data=Data, type=\"scatterChart\") \nscat.int$addControls(\"x\", value = \"NO2\", values = names(Data)[3:7])\nscat.int$addControls(\"y\", value = \"SO2\", values = names(Data)[3:7])\nscat.int\n\n#5.5.   Developing Interactive Time-series Plots and Saving#####\n\n#Load rCharts\nlibrary(rCharts)\n\n#Set the working directory where the dataset is stored\nsetwd(\"E:/OneDrive/R Video Course - Packt/Data\")\n\n#Load the dataset in R\nData <- read.table(file=\"EPA_Data.csv\", \n                   sep=\",\", \n                   header=TRUE, \n                   colClasses=c(\"Date\",\"factor\",rep(\"numeric\",5)), \n                   na.string=\"NA\")\n\n\n#Here we are goign to use the subsets we created before\nCalifornia <- Data[Data$State==\"California\",]\n\n\n#Set the data in the correct format\ndat <- transform(California, date = as.character(Date))\n\n\n#Simple Time-Series Plot\nts.int <- mPlot(x = \"date\", y = \"NO2\", data = dat, \n                type = \"Line\")\nts.int\n\n\n#Increasing its visual appeal\nts.int <- mPlot(x = \"date\", y = \"NO2\", data = dat, \n                type = \"Line\")\nts.int$set(pointSize = 0, lineWidth = 1, \n           xLabels=\"day\",xLabelAngle=45)\nts.int\n\n\n\n#Save the Plot in a HTML file\n\n#First install package \"base64enc\"\ninstall.packages(\"base64enc\")\nlibrary(base64enc)\n\n\n#Now we can save the plot\nts.int$save(\"Time_Series.html\", standalone = TRUE)\n\n#6. CREATING A WEBSITE WITH SHINY#####\n#6.1.   Getting Started with Shiny#####\n#6.2.   Creating a Simple Website#####\n\n#install.packages(\"shiny\")\nlibrary(shiny)\nlibrary(ggplot2)\n\n#Load the dataset in R\nsetwd(\"E:/OneDrive/R Video Course - Packt/Data\")\n\nData <- read.table(file=\"EPA_Data.csv\", \n                   sep=\",\", \n                   header=TRUE, \n                   colClasses=c(\"Date\",\"factor\",rep(\"numeric\",5)), \n                   na.string=\"NA\")\n\n\n\n#The basic elements of a Shiny Website\n\n#User Interface\nui <- shinyUI(fluidPage(\n  \n  #Title\n  titlePanel(\"Title text\"),\n  \n  #Sidebar\n  sidebarLayout(\n    sidebarPanel(\n      #Add Elements here\n      \n    ),\n    \n    mainPanel(\n      #Add Elements here\n      \n      \n    )\n  )\n))\n\n\n\n#Server side script\nserver <- shinyServer(function(input, output) {\n  #Add Elements here\n  \n  \n})\n\n\n\nrunApp(list(ui=ui, server=server))\n\n\n\n\n#Let's create a simple website for our data\n#User Interface\nui <- shinyUI(fluidPage(\n  \n  #Title\n  titlePanel(\"Learning R for Data Visualization\"),\n  \n  #Sidebar\n  sidebarLayout(\n    sidebarPanel(\n      \n      #Add Elements here\n      helpText(\"How to create a simple website in Shiny\"),\n      \n      selectInput(inputId=\"TypePlot\", label=\"Type of Plot\", \n                  choices=c(None=0,\n                            Histogram=\"hist\",\n                            Scatterplot=\"points\"))\n      \n    ),\n    \n    mainPanel(\n      #Add Elements here\n      plotOutput('plot')\n      \n    )\n  )\n))\n\n\n\n\n#Server side script\nserver <- shinyServer(function(input, output) {\n  #Add Elements here\n  output$plot <- renderPlot({\n    \n    if(input$TypePlot!=0){\n      \n      if(input$TypePlot==\"hist\"){\n        ggplot(data=Data, aes(x=NO2)) + \n          geom_histogram() \n        \n      } else {\n        ggplot(data=Data, aes(x=NO2, y=CO)) +\n          geom_point()\n      }\n    }\n    \n  })\n  \n})\n\n\nrunApp(list(ui=ui, server=server))\n\n#6.3.   File Input#####\n\nlibrary(shiny)\nlibrary(ggplot2)\n\n\n#Simple File Import \n\n#User Interface\nui <- shinyUI(fluidPage(\n  \n  #Title\n  titlePanel(\"Learning R for Data Visualization\"),\n  \n  #Sidebar\n  sidebarLayout(\n    sidebarPanel(\n      \n      #Add Elements here\n      helpText(\"How to create a simple website in Shiny\"),\n      \n      fileInput(inputId=\"Data\", label=\"Select a CSV file:\", \n                multiple=F),\n      \n      selectInput(inputId=\"TypePlot\", label=\"Type of Plot\", \n                  choices=c(None=0,\n                            Histogram=\"hist\",\n                            Scatterplot=\"points\"))\n      \n    ),\n    \n    mainPanel(\n      #Add Elements here\n      plotOutput('plot')\n      \n    )\n  )\n))\n\n\n\n\n#Server side script\nserver <- shinyServer(function(input, output) {\n  #Add Elements here\n  output$plot <- renderPlot({\n    \n    #LOADING DATA\n    FILE <- input$Data\n    \n    if (is.null(FILE))\n      return(NULL)\n    \n    Data <- read.table(FILE$datapath, header = T, \n                       sep = \",\")\n    \n    \n    if(input$TypePlot!=0){\n      \n      if(input$TypePlot==\"hist\"){\n        ggplot(data=Data, aes(x=NO2)) + \n          geom_histogram() \n        \n      } else {\n        ggplot(data=Data, aes(x=NO2, y=CO)) +\n          geom_point()\n      }\n    }\n    \n  })\n  \n})\n\n\nrunApp(list(ui=ui, server=server))\n\n\n\n\n#Add a way to select the file separator\n\n#User Interface\nui <- shinyUI(fluidPage(\n  \n  #Title\n  titlePanel(\"Learning R for Data Visualization\"),\n  \n  #Sidebar\n  sidebarLayout(\n    sidebarPanel(\n      \n      #Add Elements here\n      helpText(\"How to create a simple website in Shiny\"),\n      \n      selectInput(\"separator\", \"Data Separator:\", \n                  c(Comma=\",\",BlankSpace=\" \",Semicolon=\";\")),\n      \n      fileInput(inputId=\"Data\", label=\"Select a file:\", multiple=F),\n      \n      selectInput(inputId=\"TypePlot\", label=\"Type of Plot\", \n                  choices=c(None=0,\n                            Histogram=\"hist\",\n                            Scatterplot=\"points\"))\n      \n    ),\n    \n    mainPanel(\n      #Add Elements here\n      plotOutput('plot')\n      \n    )\n  )\n))\n\n\n\n\n#Server side script\nserver <- shinyServer(function(input, output) {\n  #Add Elements here\n  output$plot <- renderPlot({\n    \n    #LOADING DATA\n    FILE <- input$Data\n    \n    if (is.null(FILE))\n      return(NULL)\n    \n    Data <- read.table(FILE$datapath, header = T, \n                       sep = input$separator)\n    \n    \n    if(input$TypePlot!=0){\n      \n      if(input$TypePlot==\"hist\"){\n        ggplot(data=Data, aes(x=NO2)) + \n          geom_histogram() \n        \n      } else {\n        ggplot(data=Data, aes(x=NO2, y=CO)) +\n          geom_point()\n      }\n    }\n    \n  })\n  \n})\n\n\nrunApp(list(ui=ui, server=server))\n\n#6.4.   Conditional Panels â UI#####\n\nlibrary(shiny)\nlibrary(ggplot2)\n\n\n\n#User Interface\nui <- shinyUI(fluidPage(\n  \n  #Title\n  titlePanel(\"Learning R for Data Visualization\"),\n  #Sidebar\n  sidebarLayout(\n    sidebarPanel(\n      #Add Elements here\n      helpText(\"How to create a simple website in Shiny\"),\n      selectInput(\"separator\", \"Data Separator:\", \n                  c(Comma=\",\",BlankSpace=\" \",Semicolon=\";\")),\n      fileInput(inputId=\"Data\", label=\"Select a file:\", multiple=F),\n      selectInput(inputId=\"TypePlot\", label=\"Type of Plot\", \n                  choices=c(None=0,\n                            Histogram=\"hist\",\n                            Scatterplot=\"points\")),\n      #HISTOGRAM\n      conditionalPanel(\n        condition = \"input.TypePlot == 'hist'\",\n        uiOutput(\"HISTnames.selector\"),\n        uiOutput(\"HISTmulti.selector\"),\n        actionButton(\"hist.button\", \"Plot!\")\n      ),\n      \n      #SCATTERPLOT\n      conditionalPanel(\n        condition = \"input.TypePlot == 'points'\",\n        uiOutput(\"SCPx.selector\"),\n        uiOutput(\"SCPy.selector\"),\n        uiOutput(\"SCPcol.selector\"),\n        uiOutput(\"SCPsize.selector\"),\n        actionButton(\"sct.button\", \"Plot!\")\n      )\n      \n    ),\n    \n    mainPanel(\n      #Add Elements here\n      \n      #HISTOGRAM\n      conditionalPanel(\n        condition = \"input.TypePlot == 'hist'\",\n        plotOutput('histogram')\n      ),\n      \n      #SCATTERPLOT\n      conditionalPanel(\n        condition = \"input.TypePlot == 'points'\",\n        plotOutput('scatterplot')\n      )\n      \n    )\n  )\n))\n\n\n\n\n#Server side script\nserver <- shinyServer(function(input, output) {\n  #Add Elements here\n  \n  #HISTOGRAM\n  output$histogram <- renderPlot({\n    \n    #LOADING DATA\n    FILE <- input$Data\n    \n    if (is.null(FILE))\n      return(NULL)\n    \n    Data <- read.table(FILE$datapath, header = T, sep = input$separator)\n    \n    classes <- sapply(Data, class)\n    \n    #PANEL FOR UI\n    output$HISTnames.selector <- renderUI({\n      selectInput(inputId=\"hist.x\", label=\"Select the variable to plot:\", \n                  choices=names(Data)[classes==\"numeric\"])\n    })\n    \n    output$HISTmulti.selector <- renderUI({\n      selectInput(inputId=\"multi\", label=\"Select the facets variable:\", \n                  choices=c(\"None\",names(Data)[classes==\"factor\"]))\n    })\n    \n    #CREATE PLOT\n    hist.plot <- eventReactive(input$hist.button, {\n      \n      if(input$multi!=\"None\"){\n        data.histogram <- data.frame(var=Data[,input$hist.x], \n                                     multi=Data[,input$multi])\n        \n        plot <- ggplot(data=data.histogram,aes(x=var)) + \n          geom_histogram() +\n          xlab(paste(input$hist.x)) + \n          ylab(\"Frequency\") +\n          ggtitle(paste(\"Histogram of\",input$hist.x)) +\n          facet_wrap(~multi) +\n          theme_minimal()\n        \n        print(plot)\n        \n      } else {\n        data.histogram <- data.frame(var=Data[,input$hist.x])\n        \n        plot <- ggplot(data=data.histogram,aes(x=var)) + \n          geom_histogram() +\n          xlab(paste(input$hist.x)) + \n          ylab(\"Frequency\") +\n          ggtitle(paste(\"Histogram of\",input$hist.x)) +\n          theme_minimal()\n        \n        \n        print(plot)\n      }\n      \n      \n    })\n    \n    \n    #PLOT!\n    hist.plot()\n    \n  })\n  \n  \n  #SCATTERPLOT\n  output$scatterplot <- renderPlot({\n    \n    #LOADING DATA\n    FILE <- input$Data\n    \n    if (is.null(FILE))\n      return(NULL)\n    \n    dat.load <- read.table(FILE$datapath, header = T, sep = input$separator)\n    classes <- sapply(dat.load, class)\n    \n    \n    #PANELS FOR UI\n    output$SCPx.selector <- renderUI({\n      selectInput(inputId=\"x.sct\", label=\"Select the variable for X:\", \n                  choices=c(None=0,names(dat.load)[classes==\"numeric\"]))\n    })\n    \n    output$SCPy.selector <- renderUI({\n      selectInput(inputId=\"y.sct\", label=\"Select the variable for Y:\", \n                  choices=c(None=0,names(dat.load)[classes==\"numeric\"]))\n    })\n    \n    output$SCPcol.selector <- renderUI({\n      selectInput(inputId=\"col.sct\", label=\"Select the variable for color:\", \n                  choices=c(None=0,names(dat.load)[classes==\"numeric\"]))\n    })\n    \n    output$SCPsize.selector <- renderUI({\n      selectInput(inputId=\"size.sct\", label=\"Select the variable for size:\", \n                  choices=c(None=0,names(dat.load)[classes==\"numeric\"]))\n    })\n    \n    \n    #CREATE PLOT\n    scatterplot.plot <- eventReactive(input$sct.button, {\n      \n      if(input$col.sct!=0&input$size.sct==0){\n        data.scatterplot <- data.frame(x=dat.load[,input$x.sct], \n                                       y=dat.load[,input$y.sct], \n                                       color=dat.load[,input$col.sct])\n        \n        plot <- ggplot(data=data.scatterplot, aes(x=x, y=y)) +\n          geom_point(mapping=aes(color=color)) +\n          ggtitle(paste0(\"Scatterplot \",input$x.sct, \" vs. \", input$y.sct)) +\n          labs(color=paste(input$col.sct)) +\n          xlab(paste(input$x.sct)) + \n          ylab(paste(input$y.sct)) +\n          theme_minimal()\n        \n        print(plot)\n        \n      } else if(input$col.sct!=0&input$size.sct!=0){\n        data.scatterplot <- data.frame(x=dat.load[,input$x.sct], \n                                       y=dat.load[,input$y.sct], \n                                       color=dat.load[,input$col.sct], \n                                       size=dat.load[,input$size.sct])\n        \n        plot <- ggplot(data=data.scatterplot, aes(x=x, y=y)) +\n          geom_point(mapping=aes(color=color, size=size)) +\n          ggtitle(paste0(\"Scatterplot \",input$x.sct, \" vs. \", input$y.sct)) +\n          labs(color=paste(input$col.sct), size=paste(input$size.sct)) +\n          xlab(paste(input$x.sct)) + \n          ylab(paste(input$y.sct)) +\n          theme_minimal()\n        \n        print(plot)\n        \n      } else {\n        data.scatterplot <- data.frame(x=dat.load[,input$x.sct], y=dat.load[,input$y.sct])\n        \n        plot <- ggplot(data=data.scatterplot, aes(x=x, y=y)) +\n          geom_point() +\n          ggtitle(paste0(\"Scatterplot \",input$x.sct, \" vs. \", input$y.sct)) +\n          xlab(paste(input$x.sct)) + \n          ylab(paste(input$y.sct)) +\n          theme_minimal()\n        \n        print(plot)\n      }\n    })\n    \n    #PLOT!\n    scatterplot.plot()\n  })\n  \n  \n})\n\n\nrunApp(list(ui=ui, server=server))\n\n#6.5.   Conditional Panels â Servers#####\n\nlibrary(shiny)\nlibrary(ggplot2)\n\n\n\n#User Interface\nui <- shinyUI(fluidPage(\n  \n  titlePanel(\"Learning R for Data Visualization\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      helpText(\"Here you can select a file (csv or txt) and plot your data!\"),\n      selectInput(\"separator\", \"Data Separator:\", c(Comma=\",\",BlankSpace=\" \",Semicolon=\";\")),\n      \n      fileInput(inputId=\"Data\", label=\"Select a file:\", multiple=F),\n      \n      selectInput(\"TypePlot\", \"Type of Plot\", c(\"None\",BarChart=\"bar\",Histogram=\"hist\",BoxPlot=\"box\",Scatterplot=\"points\",TimeSeries=\"ts\")),\n      \n      #HISTOGRAM\n      conditionalPanel(\n        condition = \"input.TypePlot == 'hist'\",\n        uiOutput(\"HISTnames.selector\"),\n        uiOutput(\"HISTmulti.selector\"),\n        actionButton(\"hist.button\", \"Plot!\")\n      ),\n      \n      \n      #BAR CHART\n      conditionalPanel(\n        condition = \"input.TypePlot == 'bar'\",\n        uiOutput(\"BARx.selector\"),\n        uiOutput(\"BARy.selector\"),\n        uiOutput(\"BARcol.selector\"),\n        actionButton(\"barchart.button\", \"Plot!\")\n      ),\n      \n      \n      #BOX PLOT\n      conditionalPanel(\n        condition = \"input.TypePlot == 'box'\",\n        uiOutput(\"BOXx.selector\"),\n        uiOutput(\"BOXy.selector\"),\n        actionButton(\"box.button\", \"Plot!\")\n      ),\n      \n      \n      #SCATTERPLOT\n      conditionalPanel(\n        condition = \"input.TypePlot == 'points'\",\n        uiOutput(\"SCPx.selector\"),\n        uiOutput(\"SCPy.selector\"),\n        uiOutput(\"SCPcol.selector\"),\n        uiOutput(\"SCPsize.selector\"),\n        actionButton(\"sct.button\", \"Plot!\")\n      ),\n      \n      \n      #TIME SERIES\n      conditionalPanel(\n        condition = \"input.TypePlot == 'ts'\",\n        uiOutput(\"TSx.selector\"),\n        uiOutput(\"TSy.selector\"),\n        actionButton(\"ts.button\", \"Plot!\")\n      )\n      \n      \n    ),\n    \n    \n    #MAIN\n    mainPanel(\n      \n      #HISTOGRAM\n      conditionalPanel(\n        condition = \"input.TypePlot == 'hist'\",\n        plotOutput('histogram')\n      ),\n      \n      #BAR CHART\n      conditionalPanel(\n        condition = \"input.TypePlot == 'bar'\",\n        plotOutput('barchart')\n      ),\n      \n      #BOX PLOT\n      conditionalPanel(\n        condition = \"input.TypePlot == 'box'\",\n        plotOutput('boxplot')\n      ),\n      \n      \n      #SCATTERPLOT\n      conditionalPanel(\n        condition = \"input.TypePlot == 'points'\",\n        plotOutput('scatterplot')\n      ),\n      \n      \n      #TIME-SERIES\n      conditionalPanel(\n        condition = \"input.TypePlot == 'ts'\",\n        plotOutput('time_series')\n      )\n      \n    )\n    \n  )))\n\n\n\n\n#Server side script\nserver <- shinyServer(function(input, output) {\n  \n  #HISTOGRAM\n  output$histogram <- renderPlot({\n    \n    #LOADING DATA\n    FILE <- input$Data\n    \n    if (is.null(FILE))\n      return(NULL)\n    \n    dat.load <- read.table(FILE$datapath, header = T, sep = input$separator)\n    classes <- sapply(dat.load, class)\n    \n    #PANEL FOR UI\n    output$HISTnames.selector <- renderUI({\n      selectInput(inputId=\"hist.x\", label=\"Select the variable to plot:\", \n                  choices=names(dat.load)[classes==\"numeric\"])\n    })\n    \n    output$HISTmulti.selector <- renderUI({\n      selectInput(inputId=\"multi\", label=\"Select the facets variable:\", \n                  choices=c(\"None\",names(dat.load)[classes==\"factor\"]))\n    })\n    \n    #CREATE PLOT\n    hist.plot <- eventReactive(input$hist.button, {\n      \n      if(input$multi!=\"None\"){\n        data.histogram <- data.frame(var=dat.load[,input$hist.x], \n                                     multi=dat.load[,input$multi])\n        \n        plot <- ggplot(data=data.histogram,aes(x=var)) + \n          geom_histogram() +\n          xlab(paste(input$hist.x)) + \n          ylab(\"Frequency\") +\n          ggtitle(paste(\"Histogram of \",input$hist.x)) +\n          facet_wrap(~multi) +\n          theme_minimal()\n        \n        print(plot)\n      } else {\n        data.histogram <- data.frame(var=dat.load[,input$hist.x])\n        \n        plot <- ggplot(data=data.histogram,aes(x=var)) + \n          geom_histogram() +\n          xlab(paste(input$hist.x)) + \n          ylab(\"Frequency\") +\n          ggtitle(paste(\"Histogram of \",input$hist.x)) +\n          theme_minimal()\n        \n        print(plot)\n      }\n      \n      \n    })\n    \n    #PLOT!\n    hist.plot()\n    \n  })\n  \n  \n  #BAR CHART\n  output$barchart <- renderPlot({\n    \n    #LOADING DATA\n    FILE <- input$Data\n    \n    if (is.null(FILE))\n      return(NULL)\n    \n    dat.load <- read.table(FILE$datapath, header = T, sep = input$separator)\n    classes <- sapply(dat.load, class)\n    \n    \n    #PANELS FOR UI\n    output$BARx.selector <- renderUI({\n      selectInput(inputId=\"x.bar\", label=\"Select the variable for X:\", \n                  choices=c(None=0,names(dat.load)[classes==\"factor\"]))\n    })\n    \n    output$BARy.selector <- renderUI({\n      selectInput(inputId=\"y.bar\", label=\"Select the variable for Y:\", \n                  choices=c(None=0,names(dat.load)[classes==\"numeric\"]))\n    })\n    \n    output$BARcol.selector <- renderUI({\n      selectInput(inputId=\"col.bar\", label=\"Select the variable for color:\", \n                  choices=c(None=0,names(dat.load)[classes==\"numeric\"]))\n    })\n    \n    \n    #CREATE PLOT\n    barchart.plot <- eventReactive(input$barchart.button, {\n      \n      #Here we add a condition to allow users not to fill the bar-chart\n      if(input$col.bar!=0){\n        data.barchart <- data.frame(x=dat.load[,input$x.bar], \n                                    y=dat.load[,input$y.bar], \n                                    color=dat.load[,input$col.bar])\n        \n        ggplot(data=data.barchart,aes(x=x, y=y, fill=color)) + \n          geom_bar(stat=\"identity\") +\n          xlab(paste(input$x.bar)) + \n          ylab(paste(input$y.bar)) +\n          theme_minimal()\n      } else {\n        data.barchart <- data.frame(x=dat.load[,input$x.bar], \n                                    y=dat.load[,input$y.bar])\n        \n        ggplot(data=data.barchart,aes(x=x, y=y)) + \n          geom_bar(stat=\"identity\") +\n          xlab(paste(input$x.bar)) + \n          ylab(paste(input$y.bar)) +\n          theme_minimal()\n      }\n    })\n    \n    #PLOT!\n    barchart.plot()\n  })\n  \n  \n  #BOX PLOT\n  output$boxplot <- renderPlot({\n    \n    #LOADING DATA\n    FILE <- input$Data\n    \n    if (is.null(FILE))\n      return(NULL)\n    \n    dat.load <- read.table(FILE$datapath, header = T, sep = input$separator)\n    classes <- sapply(dat.load, class)\n    \n    \n    #PANELS FOR UI\n    output$BOXx.selector <- renderUI({\n      selectInput(inputId=\"x.box\", label=\"Select the variable for X:\", \n                  choices=names(dat.load)[classes==\"factor\"])\n    })\n    \n    output$BOXy.selector <- renderUI({\n      selectInput(inputId=\"y.box\", label=\"Select the variable for Y:\", \n                  choices=names(dat.load)[classes==\"numeric\"])\n    })\n    \n    \n    \n    #CREATE PLOT\n    boxplot.plot <- eventReactive(input$box.button, {\n      \n      data.boxplot <- data.frame(x=dat.load[,input$x.box], \n                                 y=dat.load[,input$y.box])\n      \n      plot <- ggplot(data=data.boxplot,aes(x=x, y=y)) + \n        geom_boxplot() +\n        xlab(paste(input$x.box)) + \n        ylab(paste(input$y.box)) +\n        theme_minimal()\n      \n      print(plot)\n    })\n    \n    #PLOT!\n    boxplot.plot()\n    \n  })\n  \n  \n  \n  #SCATTERPLOT\n  output$scatterplot <- renderPlot({\n    \n    #LOADING DATA\n    FILE <- input$Data\n    \n    if (is.null(FILE))\n      return(NULL)\n    \n    dat.load <- read.table(FILE$datapath, header = T, sep = input$separator)\n    classes <- sapply(dat.load, class)\n    \n    \n    #PANELS FOR UI\n    output$SCPx.selector <- renderUI({\n      selectInput(inputId=\"x.sct\", label=\"Select the variable for X:\", \n                  choices=c(None=0,names(dat.load)[classes==\"numeric\"]))\n    })\n    \n    output$SCPy.selector <- renderUI({\n      selectInput(inputId=\"y.sct\", label=\"Select the variable for Y:\", \n                  choices=c(None=0,names(dat.load)[classes==\"numeric\"]))\n    })\n    \n    output$SCPcol.selector <- renderUI({\n      selectInput(inputId=\"col.sct\", label=\"Select the variable for color:\", \n                  choices=c(None=0,names(dat.load)[classes==\"numeric\"]))\n    })\n    \n    output$SCPsize.selector <- renderUI({\n      selectInput(inputId=\"size.sct\", label=\"Select the variable for size:\", \n                  choices=c(None=0,names(dat.load)[classes==\"numeric\"]))\n    })\n    \n    \n    #CREATE PLOT\n    scatterplot.plot <- eventReactive(input$sct.button, {\n      \n      if(input$col.sct!=0&input$size.sct==0){\n        data.scatterplot <- data.frame(x=dat.load[,input$x.sct], \n                                       y=dat.load[,input$y.sct], \n                                       color=dat.load[,input$col.sct])\n        \n        plot <- ggplot(data=data.scatterplot, aes(x=x, y=y)) +\n          geom_point(mapping=aes(color=color)) +\n          ggtitle(paste0(\"Scatterplot \",input$x.sct, \" vs. \", input$y.sct)) +\n          labs(color=paste(input$col.sct)) +\n          xlab(paste(input$x.sct)) + \n          ylab(paste(input$y.sct)) +\n          theme_minimal()\n        \n        print(plot)\n        \n      } else if(input$col.sct!=0&input$size.sct!=0){\n        data.scatterplot <- data.frame(x=dat.load[,input$x.sct], \n                                       y=dat.load[,input$y.sct], \n                                       color=dat.load[,input$col.sct], \n                                       size=dat.load[,input$size.sct])\n        \n        plot <- ggplot(data=data.scatterplot, aes(x=x, y=y)) +\n          geom_point(mapping=aes(color=color, size=size)) +\n          ggtitle(paste0(\"Scatterplot \",input$x.sct, \" vs. \", input$y.sct)) +\n          labs(color=paste(input$col.sct), size=paste(input$size.sct)) +\n          xlab(paste(input$x.sct)) + \n          ylab(paste(input$y.sct)) +\n          theme_minimal()\n        \n        print(plot)\n      } else {\n        data.scatterplot <- data.frame(x=dat.load[,input$x.sct], \n                                       y=dat.load[,input$y.sct])\n        \n        plot <- ggplot(data=data.scatterplot, aes(x=x, y=y)) +\n          geom_point() +\n          ggtitle(paste0(\"Scatterplot \",input$x.sct, \" vs. \", input$y.sct)) +\n          xlab(paste(input$x.sct)) + \n          ylab(paste(input$y.sct)) +\n          theme_minimal()\n        \n        print(plot)\n      }\n    })\n    \n    #PLOT!\n    scatterplot.plot()\n  })\n  \n  \n  \n  #TIME-SERIES\n  output$time_series <- renderPlot({\n    \n    #LOADING DATA\n    FILE <- input$Data\n    \n    if (is.null(FILE))\n      return(NULL)\n    \n    dat.load <- read.table(FILE$datapath, header = T, sep = input$separator)\n    classes <- sapply(dat.load, class)\n    \n    \n    #PANELS FOR UI \n    output$TSx.selector <- renderUI({\n      selectInput(inputId=\"x.ts\", label=\"Select the variable for X:\", \n                  choices=names(dat.load)[classes==\"factor\"])\n    })\n    \n    output$TSy.selector <- renderUI({\n      selectInput(inputId=\"y.ts\", label=\"Select the variable for Y:\", \n                  choices=names(dat.load)[classes==\"numeric\"])\n    })\n    \n    \n    \n    #CREATE PLOT\n    time_series.plot <- eventReactive(input$ts.button, {\n      data.time_series <- data.frame(x=as.Date(dat.load[,input$x.ts]), \n                                     y=dat.load[,input$y.ts])\n      \n      plot <- ggplot(data=data.time_series,aes(x=x, y=y)) + \n        geom_line() +\n        xlab(\"Date\") + \n        ylab(paste(input$y.ts)) +\n        theme_minimal()\n      \n      print(plot)\n    })\n    \n    #PLOT!\n    time_series.plot()\n    \n  })\n  \n})\n\n\nrunApp(list(ui=ui, server=server))\n\n#6.6.   Deploying the Site#####\n\n#install.packages('devtools')\n#Wea ready installed devtools in Section 5 so we do not have to do it again.\n\ndevtools::install_github('rstudio/shinyapps')\n\n\n#This can be copied and pasted directly from the website\nshinyapps::setAccountInfo(name='YOUR_SERVER_NAME',\n                          token='YOUR_TOKEN',\n                          secret='YOUR_SECRET')\n\n\n\nlibrary(shinyapps)\nshinyapps::deployApp('YOUR_PATH')\n\n#It is important that only ui.r and server.r are present in this folder\n\n#############################################################\n\n#  5.R for Datascience Solutions#####\n#################################################################################\n#What You Will Learn                                                                            #\n#                                                                                                   #\n#Get to know the functional characteristics of R language                             #\n#Extract, transform, and load data from heterogeneous sources-                      #\n#Understand how easily R can confront probability and statistics problems         #\n#Get simple R instructions to quickly organize and manipulate large datasets      #\n#Create professional data visualizations and interactive reports                    #\n#Predict user purchase behavior by adopting a classification approach               #\n#Implement data mining techniques to discover items that are frequently purchased together#\n#Group similar text documents by using various clustering methods                   #\n#################################################################################\n\n#1. Functions in R                          00:30:08#####\n#1.1.   R Functions and Arguments           00:06:25#####\n\naddnum<- function(x, y){\n  s <- x+y\n  return(s)\n}\n\naddnum (3,7)\n\naddnum2<- function(x, y){\n  x+y\n}\n\naddnum2(3,7)\naddnum2\nbody(addnum2)\nformals(addnum2)\nargs(addnum2)\nhelp(sum)\n?sum\n\ndefaultarg <- function(x, y = 5){\n  y <- y * 2\n  s <- x+y\n  return(s)\n}\n\ndefaultarg(3)\ndefaultarg(1:3)\ndefaultarg(3,6)\ndefaultarg(y = 6, x = 3)\n\nfuncarg <- function(x, y, type= \"sum\"){\n  if (type == \"sum\"){\n    sum(x,y)\n  }else if (type == \"mean\"){\n    mean(x,y)\n  }else{\n    x * y\n  }\n}\n\nfuncarg(3,5)\nfuncarg(3,5, type = 'mean')\nfuncarg(3,5, type = 'unknown')\n\nunspecarg <- function(x, y, ...){\n  x <- x + 2\n  y <- y * 2\n  sum(x,y, ...)\n}\n\nunspecarg(3,5)\nunspecarg(3,5,7,9,11)\nfuncarg(3,5, t = 'unknown')\n\n\n#1.2.   Understanding Environments          00:02:58#####\n\nenvironment()\n.GlobalEnv\nglobalenv()\n\nidentical(globalenv(), environment())\n\nmyenv <- new.env()\nmyenv\n\nmyenv$x <- 3\n\nls(myenv)\n\nls()\n\nx\naddnum <- function(x, y){\n  x+y\n}\n\nenvironment(addnum)\nenvironment(lm)\n\naddnum2 <- function(x, y){\n  print(environment())\n  x+y\n}\naddnum2(2,3)\n\naddnum3 <- function(x, y){\n  func1 <- function(x){\n    print(environment())\n  }\n  func1(x)\n  print(environment())\n  x + y\n}\n\naddnum3(2,5)\nparentenv <- function(){\n  e <- environment()\n  print(e)\n  print(parent.env(e))\n}\nparentenv()\n\n#1.3.   Working with Lexical Scoping            00:02:49#####\n\nx <- 5\ntmpfunc <- function(){\n  x + 3\n}\ntmpfunc()\nx <- 5\nparentfunc <- function(){\n  x<- 3\n  childfunc <- function(){\n    x\n  }\n  childfunc()\n}\nparentfunc()\nx <- 'string'\nlocalassign<- function(x){\n  x <- 5\n  x\n}\nlocalassign(x)\nx\nx <- 'string'\ngobalassign<- function(x){\n  x <<- 5\n  x\n}\ngobalassign (x)\nx\nsearch()\n\n#1.4.   Understanding Closure               00:02:17#####\n\naddnum <- function(a,b){\n  a + b\n}\naddnum(2,3)\n(function(a,b){\n  a + b\n})(2,3)\nmaxval<- function(a,b){\n  (function(a,b){\n    return(max(a,b))\n  }\n  )(a, b)\n}\nmaxval(c(1,10,5),c(2,11))\nx <- c(1,10,100)\ny <- c(2,4,6)\nz <- c(30,60,90)\na <- list(x,y,z)\nlapply(a, function(e){e[1] * 10})\nx <- c(1,10,100)\nfunc <- list(min1 = function(e){min(e)}, max1 = function(e){max(e)} )\nfunc$min1(x)\nlapply(func, function(f){f(x)})\n\nx <- c(1,10,100)\ny <- c(2,4,6)\nz <- c(30,60,90)\na <- list(x,y,z)\nsapply(a, function(e){e[1] * 10})\n\n#1.5.   Performing Lazy Evaluation          00:01:56#####\n\nlazyfunc <- function(x, y){\n  x\n}\nlazyfunc(3)\nlazyfunc2 <- function(x, y){\n  x + y\n}\nlazyfunc2(3)\nlazyfunc4 <- function(x, y=2){\n  x + y\n}\nlazyfunc4(3)\nfibonacci <- function(n){\n  if (n==0)\n    return(0)\n  if (n==1)\n    return(1)\n  return(fibonacci(n-1) + fibonacci(n-2))\n}\nfibonacci(10)\n\nlazyfunc3 <- function(x, y){\n  force(y)\n  x\n}\nlazyfunc3(3)\ninput_function <- function(x, func){\n  func(x)\n}\ninput_function(1:10, sum)\n\n#1.6.   Creating Infix Operators                00:02:51#####\n\n3 + 5\n'+'(3,5)\n3:5 * 2 - 1\n'-'('*'(3:5, 2), 1)\nx <- c(1,2,3,3, 2)\ny <- c(2,5)\n'%match%' <- function(a,b){\n  intersect(a, b)\n}\nx %match% y\n'%diff%' <- function(a,b){\n  setdiff(a, b)\n}\nx %diff% y\nx %match% y %match% z\ns <- list(x,y,z)\nReduce('%match%',s)\n\n'+' <- function(x,y) paste(x,y, sep = \"|\")\nx = '123'\ny = '456'\nx+y\n\n#1.7.   Using the Replacement Function          00:02:17#####\n\nx <- c(1,2,3)\nnames(x) <- c('a','b','c')\nx\nx <- 'names<-'(x,value=c('a','b','c'))\nx\nx<-c(1,2,3)\n\"erase<-\" <- function(x, value){\n  x[!x %in% value]\n}\nerase(x) <- 2\nx\nx <- c(1,2,3)\nx <- 'erase<-'(x,value=c(2))\nx\nx <- c(1,2,3)\nerase(x) = c(1,3)\nx\nx <- c(1,2,3)\nerase(x) = c(1,3)\nx\nx <- c(1,2,3)\ny <- c(2,2,3)\nz <- c(3,3,1)\na = list(x,y,z)\n\"erase<-\" <- function(x, pos, value){\n  x[[pos]] <- x[[pos]][!x[[pos]] %in% value]\n  x\n}\nerase(a, 2) = c(2)\na\nget(\"names<-\")\n\n#1.8.   Handling Errors in a Function           00:04:30#####\n\n'hello world' + 3\naddnum <- function(a,b){\n  if(!is.numeric(a) | !is.numeric(b)){\n    stop(\"Either a or b is not numeric\")\n  }\n  a + b\n}\naddnum(2,3)\naddnum(\"hello world\",3)\naddnum2 <- function(a,b){\n  if(!is.numeric(a) | !is.numeric(b)){\n    warning(\"Either a or b is not numeric\")\n  }\n  a + b\n}\naddnum2(\"hello world\",3)\n\noptions(warn=2)\naddnum2(\"hello world\", 3)\nsuppressWarnings(addnum2(\"hello world\",3))\nerrormsg <- try(addnum(\"hello world\",3))\nerrormsg\nerrormsg <- try(addnum(\"hello world\",3), silent=TRUE)\niter <- c(1,2,3,'O',5)\nres <- rep(NA, length(iter))\nfor (i in 1:length(iter)) {\n  res[i] = as.integer(iter[i])\n}\nres\niter <- c(1,2,3,'O',5)\nres <- rep(NA, length(iter))\nfor (i in 1:length(iter)) {\n  res[i] = try(as.integer(iter[i]), silent=TRUE)\n}\nres\naddnum3 <- function(a,b){\n  stopifnot(is.numeric(a), !is.numeric(b))\n  a + b\n}\naddnum3(\"hello\", \"world\")\ndividenum <- function(a,b){\n  result <- tryCatch({\n    print(a/b)\n  }, error = function(e) {\n    if(!is.numeric(a) | !is.numeric(b)){\n      print(\"Either a or b is not numeric\")\n    }\n  }, finally = {\n    rm(a)\n    rm(b)\n    print(\"clean variable\")\n  }\n  )\n}\ndividenum(2,4)\ndividenum(\"hello\", \"world\")\ndividenum(1)\n\ndividenum <- function(a,b){\n  result <- tryCatch({\n    a/b\n  }, error = function(e) {\n    conditionMessage(e)\n  }\n  )\n  result\n}\ndividenum(3,5)\ndividenum(3,\"hello\")\n\n#1.9.   The Debugging Function                  00:04:05#####\n\ndebugfunc <- function(x, y){\n  x <- y + 2\n  x\n}\ndebug(2)\ndebugfunc(2)\ndebug(debugfunc)\ndebugfunc(2)\n\nundebug(debugfunc)\ndebugfunc2(2)\ntrace(debugfunc2, quote(if(missing(y)){browser()}), at=4)\ndebugfunc2(3)\ndebugfunc3 <- function(x, y){\n  x <- 3\n  sum(x)\n  x <- y + 2\n  sum(x,y)\n  x\n}\ntrace(sum)\ndebugfunc3(2,3)\nlm(y~x)\ntraceback()\n\n#2. Data Extracting, Transforming, and Loading      00:17:03#####\n#2.1.   Downloading Open Data               00:02:14#####\n\ndownload.file('http://real-chart.finance.yahoo.com/table.csv?s=%5EGSPC&d=6&e=3&f=2015&g=d&a=0&b=3&c=1950&ignore=.csv', 'snp500.csv')\ngetwd()\nlist.files('./')\n\ninstall.packages(\"RCurl\")\nlibrary(RCurl)\nrows <- getURL(\"https://nycopendata.socrata.com/api/views/jd4g-ks2z/rows.csv?accessType=DOWNLOAD\")\n\n\n#2.2.   Reading and Writing CSV Files           00:01:13#####\n\ngetwd()\nlist.files('./')\nstock_data <- read.table('snp500.csv', sep=',' , header=TRUE)\nsubset_data <- stock_data[1:6, c(\"Date\", \"Open\", \"High\", \"Low\", \"Close\")]\nhead(stock_data)\nstock_data2 <- read.csv('snp500.csv', header=TRUE)\nhead(stock_data2)\nwifi_hotspot <- read.csv(text = rows)\n\n\n#2.3.   Scanning Text Files                     00:02:21#####\n\nstock_data3 = scan('snp500.csv',sep=',', what=list(Date = '', Open = 0, High = 0, Low = 0,Close = 0, Volume = 0, Adj_Close = 0),  skip=1, fill=T)\nmode(stock_data3)\n\ndownload.file(\"https://github.com/ywchiu/rcookbook/raw/master/chapter2/weather.op\", \"weather.op\")\nweather <- read.fwf(\"weather.op\", widths = c(6,6,10,11,9,8), col.names = c(\"STN\",\"WBAN\",\"YEARMODA\",\"TEMP\",\"MAX\",\"MIN\"), skip=1)\nhead(weather)\nnames(weather)\n\n\n#2.4.   Working with Excel Files                00:01:55#####\n\ninstall.packages(\"xlsx\")\nlibrary(xlsx)\ndownload.file(\"http://api.worldbank.org/v2/en/topic/3?downloadformat=excel\", \"worldbank.xls\", mode=\"wb\")\noptions(java.parameters = \"-Xmx2000m\")\nwb = read.xlsx2(\"worldbank.xls\", sheetIndex = 1, startRow = 4)\nwb2 = wb[c(\"Country.Name\",\"Country.Code\",\"Indicator.Name\",\"Indicator.Code\", \"X2014\")]\ndim(wb2)\nwrite.xlsx2(wb2, \"2014wbdata.xlsx\", sheetName = \"Sheet1\")\n\n\n#2.5.   Reading Data from Databases             00:04:03#####\n\ninstall.packages(\"RJDBC\")\nlibrary(RJDBC)\ndrv <- JDBC(\"com.mysql.jdbc.Driver\",\n            \"C:\\\\Program Files\\\\MySQL\\\\mysql-connector-java-5.0.8-bin.jar\"\n)\nconn <- dbConnect(drv,\n                  \"jdbc:mysql://localhost:3306/finance\",\n                  \"root\",\n                  \"test\")\ndbListTables(conn)\ntrade_data <- dbGetQuery(conn, \"SELECT * FROM majortrade\")\ndbDisconnect(conn)\n\ninstall.packages(\"RMySQL\")\nlibrary(RMySQL)\nmydb = dbConnect(MySQL(), user='root', password='test', host='localhost')\ndbSendQuery(mydb, \"USE finance\")\nfetch(dbSendQuery(mydb, \"SELECT * FROM majortrade;\"))\n\n\n#2.6.   Scraping Web Data                   00:05:17#####\n\ninstall.packages(\"rvest\")\nlibrary(rvest)\nspx_quote <- html(\"http://www.bloomberg.com/quote/SPX:IND\")\ncell  <- spx_quote %>% html_nodes(\".cell\")\nlabel <- cell %>%\n  html_nodes(\".cell__label\")  %>%\n  html_text() %>%\n  lapply(function(e)gsub(\"\\n|\\\\s+\", \"\", e))\nvalue <- cell %>%\n  html_nodes(\".cell__value\") %>%\n  html_text() %>%\n  lapply(function(e)gsub(\"\\n|\\\\s+\", \"\", e))\nnames(value) <- title\nenergy <- html(\"http://www.bloomberg.com/energy\")\nenergy.table = energy %>% html_node(\".data-table\") %>% html_table()\n\ninstall.packages(\"RSelenium\")\nlibrary(RSelenium)\nremDr <- remoteDriver(remoteServerAddr = \"localhost\"\n                      , port = 4444\n                      , browserName = \"firefox\"\n)\nremDr$getStatus()\nremDr$open()\nremDr$navigate(\"http://www.bloomberg.com/quote/SPX:IND \")\nwebElem <- remDr$findElements('css selector', \".cell\")\nwebData = sapply(webElem, function(x){\n  label = x$findChildElement('css selector', '.cell__label')\n  value = x$findChildElement('css selector', '.cell__value')\n  cbind(c(\"label\" = label$getElementText(), \"value\" = value$getElementText()))\n}\n)\n\n## Accessing Facebook Data\n```\naccess_token = '<access_token>'\nfb_data <- html(sprintf(\"https://graph.facebook.com/me/tagged_places?access_token=%s\",access_token))\ninstall.packages(\"rjson\")\nlibrary(rjson)\nfb_json <-  fromJSON(fb_data %>% html_text())\nfb_place = sapply(fb_json$data, function(e){e$place$name})\nfb_id = sapply(fb_json$data, function(e){e$place$id})\ndata.frame(place = fb_place, id = fb_id)\n\ninstall.packages(\"Rfacebook\")\nlibrary(Rfacebook)\ngetUsers(\"me\", \"<access_token>\")\n```\n\n## Working with Rtwitter \n```\ninstall.packages(\"twitteR\")\nlibrary(twitteR)\n\nconsumer_key <- '<consumer_key>'\nconsumer_secret <- '<consumer_secret>'\naccess_token <- '<access_token>'\naccess_secret <- '<access_secret>'\nsetup_twitter_oauth(consumer_key,\n                    consumer_secret,\n                    access_token,\n                    access_secret)\n\nres <- searchTwitter(\"world cup\", n=100)\n\n######\n######\n#3. Data Pre-Processing and Preparation     00:29:14#####\n#3.1.   Renaming the Data Variable          00:02:27#####\n\ndownload.file(\"https://github.com/ywchiu/rcookbook/raw/master/chapter3/employees.csv\", \" employees.csv\")\n\ndownload.file(\"https://github.com/ywchiu/rcookbook/raw/master/chapter3/salaries.csv\", \"salaries.csv\")\n\nemployees = read.csv('employees.csv', head=FALSE)\nsalaries = read.csv('salaries.csv', head=FALSE)\n\nnames(employees)\nnames(salaries)\n\nnames(employees) = c(\"emp_no\", \"birth_date\", \"first_name\", \"last_name\", \"gender\", \"hire_date\")\nnames(employees)\n\ncolnames (salaries) = c(\"emp_no\", \"salary\",  \"from_date\",   \"to_date\")\ncolnames (salaries)\n\nrownames (salaries) = salaries$emp_no\n\ndimnames(employees) = list(c(1,2,3,4,5,6,7,8,9,10), c(\"emp_no\", \"birth_date\", \"first_name\", \"last_name\", \"gender\", \"hire_date\"))\n\n\n#3.2.   Converting Data Types               00:02:35#####\n\nclass(employees$birth_date)\n\nstr(employees)\n\nemployees$birth_date = as.Date(employees$birth_date)\nemployees$hire_date = as.Date(employees$hire_date)\n\nemployees$first_name = as.character(employees$first_name)\nemployees$last_name = as.character(employees$last_name)\n\nstr(employees)\n\nsalaries$from_date = as.Date(salaries$from_date)\nsalaries$to_date = as.Date(salaries$to_date)\n\nemployees = read.csv('~/Desktop/employees.csv', colClasses = c(NA,\"Date\", \"character\", \"character\", \"factor\", \"Date\"), head=FALSE)\nstr(employees)\n\n\n#3.3.   Working with Date Format                00:02:55#####\n\nemployees$hire_date + 30\n\nemployees$hire_date - employees$birth_date\n\ndifftime(employees$hire_date, employees$birth_date, unit=\"weeks\")\n\ninstall.packages(\"lubridate\")\nlibrary(lubridate)\n\nymd(employees$hire_date)\n\nspan <- interval(ymd(employees$birth_date), ymd(employees$hire_date))\ntime_period <- as.period(span)\n\nyear(time_period)\n\nnow()\n\nspan2 <- interval(now() , ymd(employees$birth_date))\nyear(as.period(span2))\n\nSys.setlocale(category = \"LC_ALL\", locale = \"English_United States.1252\")\n\n\n#3.4.   Adding New Records                  00:02:09#####\n\nemployees <- rbind(employees, c(10011, '1960-01-01', 'Jhon', 'Doe', 'M', '1988-01-01'))\n\nemployees <- rbind(employees, c(10011, '1960-01-01', 'Jhon', 'Doe', 'M', '1988-01-01'))\n\ncbind(employees, position = NA)\n\nspan <- interval(ymd(employees$birth_date), now())\ntime_period <- as.period(span)\nemployees$age <- year(time_period)\n\ntransform(employees, age = year(time_period), position = \"RD\", marrital = NA )\n\nwith(employees, year(birth_date))\nemployees $birth_year <- with(employees, year(birth_date))\n\n\n#3.5.   Filtering Data                      00:03:28#####\n\nhead(employees, 3)\n\ntail(employees, 3)\n\nemployees[1:3,]\n\nemployees[1:3, 2:4]\n\nemployees[c(2,5), c(1,3)]\n\nemployees[1:3, c(\"first_name\",\"last_name\")]\n\nemployees[1:3,-6]\n\nemployees[1:3, !names(employees) %in% c(\"last_name\", \"first_name\")]\n\nemployees[employees$gender == 'M',]\n\nsalaries[salaries$salary >= 60000 & salaries$salary < 70000,]\n\nemployees[substr(employees$first_name,0,2)==\"Ge\",]\n\nemployees[grep('[aeiou]$', employees$first_name),]\n\nsubset(employees, rownames(employees) %in% 1:3, select=c(\"first_name\",\"last_name\"))\n\nsubset(employees, employees$gender == 'M')\n\n\n#3.6.   Dropping Data                       00:01:42#####\n\nemployees <- employees[,-5]\n\nemployees$hire_date <- NULL \n\nemployees <- employees[c(-2,-4,-6),]\n\nwithin(employees, rm(birth_date, hire_date))\n\n\n#3.7.   Merging and Sorting Data                00:03:59#####\n\nemployees_salary  <- merge(employees, salaries, by=\"emp_no\")\nhead(employees_salary,3)\n\nmerge(employees, salaries, by=\"emp_no\", all.x =TRUE)\n\ninstall.packages(\"plyr\")\nlibrary(plyr)\n\njoin(employees, salaries, by=\"emp_no\")\n\njoin_all(list(employees, salaries), \"emp_no\")\n\n## Sorting data \n```\na <- c(5,1,4,3,2,6,3)\nsort(a)\nsort(a, decreasing=TRUE)\n\norder(a)\norder(a, decreasing = TRUE)\n\nsorted_salaries <- salaries[order(salaries$salary, decreasing = TRUE),]\nhead(sorted_salaries)\n\nsorted_salaries2 <-salaries[order(salaries$salary, salaries$from_date, decreasing = TRUE),]\nhead(sorted_salaries2)\n\narranged_salaries <- arrange(salaries, salary, desc(from_date))\nhead(arranged_salaries)\n\n\n#3.8.   Reshaping Data                  00:02:42#####\n\nwide_salaries <- dcast(salaries, emp_no ~ year(ymd(from_date)), value.var=\"salary\")\nwide_salaries[1:3, 1:7]\n\nwide_employees_salary <- dcast(employees_salary, emp_no + paste(first_name, last_name) ~ year(ymd(from_date)), value.var=\"salary\", variable.name=\"condition\")\nwide_employees_salary[1:3,1:7]\n\nlong_salaries <- melt(wide_salaries, id.vars=c(\"emp_no\"))\nhead(long_salaries)\n\nhead(na.omit(long_salaries))\n\nun_salaries <- unstack(long_salaries[,c(3,1)])\nhead(un_salaries, 3)\n\nstack_salaries <- stack(un_salaries )\nhead(stack_salaries)\n\n\n#3.9.   Detecting Missing Data              00:03:14#####\n\nsalaries[salaries$to_date > \"2100-01-01\",]\n\nsalaries[salaries$to_date > \"2100-01-01\",\"to_date\"] = NA\n\nis.na(salaries$to_date)\n\nsum(is.na(salaries$to_date))\n\nsum(is.na(salaries$to_date) == TRIE)/length(salaries$to_date)\n\nwide_salaries <- dcast(salaries, emp_no ~ year(ymd(from_date)), value.var=\"salary\")\nwide_salaries[1:3, 1:7]\n\nsapply(wide_salaries, function(df) {\n  sum(is.na(df)==TRUE)/ length(df);\n}) \n\ninstall.packages(\"Amelia\")\nlibrary(Amelia)\n\nmissmap(wide_salaries, main=\"Missing Map\")\t\n\nAmeliaView()\t\t\n\n\n#3.10.  Imputing Missing Data               00:04:03#####\n\ntest.emp = salaries[salaries$emp_no == 10001,]\n\ntest.emp[8,c(\"salary\")]\ntest.emp[8,c(\"salary\")] = NA\n\nna.omit(test.emp)\n\nmean_salary <- mean(salaries$salary[salaries$emp_no == 10001], na.rm=TRUE)\nmean_salary\n\nsalaries$salary[salaries$emp_no == 10001 & is.na(salaries$salary)] = mean_salary\n\ninstall.packages(\"mice\")\nlibrary(mice)\n\ntest.emp$from_date = year(ymd(test.emp$from_date))\ntest.emp$to_date = year(ymd(test.emp$to_date))\nimp = mice(test.emp, meth=c('norm'), set.seed=7)\ncomplete(imp)\n\nattach(test.emp)\n\nfit = lm(salary ~ from_date)\n\npredict(fit, data.frame(from_date = 1993))\n\n#4. Data Manipulation                       00:30:35#####\n#4.1.   Enhancing a data.frame with a data.table    00:04:49#####\n\ninstall.packages(\"data.table\")\nlibrary(data.table)\npurchase <- read.table(\"purchase_view.tab\", header=TRUE, sep='\\t')\ndim(purchase)\n\norder <- read.table(\"purchase_order.tab\", header=TRUE, sep='\\t')\ndim(order)\npurchase.transform = as.data.table(purchase) \nclass(purchase.transform)\ndt <- data.table(product=c(\"p1\", \"p2\", \"p3\"), price=c(100,200,300), category=\"beverage\" )\ndt\nsetnames(dt, c(\"Product\", \"Price\", \"Drink\"))\nhead(dt)\npurchase.dt <- fread(\"purchase_view.tab\", header=TRUE, sep='\\t')\norder.dt <- fread(\"purchase_order.tab\", header=TRUE, sep='\\t')\nsystem.time(purchase <- read.table(\"purchase_view.tab\", header=TRUE, sep='\\t'))\nsystem.time(purchase.dt <- fread(\"purchase_view.tab\", header=TRUE, sep='\\t'))\ndim(purchase.dt)\nstr(purchase.dt)\n\ninstall.packages(\"readr\")\nlibrary(\"readr\")\n\norder.readr = read_tsv(\"purchase_order.tab\")\nhead(order.readr)\nclass(order.readr)\n\n\n#4.2.   Managing Data with data.table           00:04:14#####\n\nhead(purchase.dt[1:3])\nhead(purchase[1:3])\npurchase.dt[1:3, User]\npurchase[1:3,\"User\"]\nuser.price <- order[1:3, c(\"User\", \"Price\") ]\nhead(user.price)\ndt.user.price <- order.dt[1:3, list(User, Price) ]\ndt.user.price\ndt.user.price2 <- order.dt[1:3, .(User, Price) ]\nhead(dt.user.price2)\ndt.price <- order.dt[Quantity > 3, Price]\nhead(dt.price)\nprice <- order[order$Quantity > 3, \"Price\"]\nhead(price)\ndt.omit.price <- order.dt[, na.omit(Price)]\nhead(dt.omit.price)\nomit.price <- order[!is.na(order$Price), \"Price\"]\nhead(omit.price)\ndt.omit.price2 <- order.dt[na.omit(Price)]\nhead(dt.omit.price2, 3)\ndt.omit.price2[,Avg_Price := Price/Quantity]\nhead(dt.omit.price2[Quantity >=2], 3)\ndt.omit.price2[,avg_price:=NULL]\nhead(dt.omit.price2[Quantity >=2], 3)\ndt.omit.price3 <- copy(dt.omit.price2)\npurchase.dt[.N, User]\npurchase[nrow(purchase), \"User\"]\n\ndt <- data.table(product=c(\"p1\", \"p2\", \"p3\"), price=c(100,200,300), category=\"beverage\" )\ndt2 <- dt\ndt.copy = copy(dt)\ndt\ndt2\ndt.copy\nidentical(dt, dt2)\nidentical(dt, dt.copy)\n\n\n#4.3.   Performing Fast Aggregation with data.table 00:02:09#####\n\norder.dt[,mean(na.omit(Price))]\nmean.price.by.user <- order.dt[,mean(na.omit(Price)), User]\nhead(mean.price.by.user)\nmean.price.by.user2 <- order.dt[,.(Price=mean(na.omit(Price))), User]\nhead(mean.price.by.user2)\nmean.price.by.date <- order.dt[,.(Price=mean(na.omit(Price))), by=as.Date(Time)]\nhead(mean.price.by.date)\nprice_sum_n_users.by.date <- order.dt[,.(Price_Sum = sum(na.omit(Price)), N_Users= length(unique(User))) , by=as.Date(order.dt$Time)]\nhead(price_sum_n_users.by.date)\nprice_sum_n_users.by.date2 <- order.dt[,.(Price_Sum = sum(na.omit(Price)), N_Users=uniqueN(User)) , by=as.Date(order.dt$Time)]\nhead(price_sum_n_users.by.date2)\nprice_avg.by.date <- order.dt[,.(Price_Avg = sum(na.omit(Price)) / uniqueN(User)) , by=.(Date=as.Date(order.dt$Time))]\nhead(price_avg.by.date)\nPrice_Sum.by.Date_N_Product <- order.dt[,.(Price_Sum = na.omit(sum(Price* Quantity))) , by=.(Date = as.Date(Time), Product)]\nsorted.price.asc <- Price_Sum.by.Date_N_Product[order(Price_Sum)]\nhead(sorted.price.asc)\nsorted.price.desc <- Price_Sum.by.Date_N_Product[order(-Price_Sum)]\nhead(sorted.price.desc)\n\norder.dt[,':='(Avg_P_By_U= mean(na.omit(Price)) ), User]\nhead(order.dt,3)\n\n\n#4.4.   Merging large Datasets with a data.table    00:02:41#####\n\nproduct.dt <- order.dt[,.(Buy = length(Action)),by=Product]\nhead(product.dt[order(-Buy)])\nview.dt <- purchase.dt[, .(n_views = length(User)), by=Product]\nhead(view.dt[order(-n_views)])\nmerged.dt <- merge(view.dt, product.dt, by=\"Product\")\nhead(merged.dt)\nsetkey(view.dt,Product)\nsetkey(product.dt,Product)\n\ninner_join.dt <- view.dt[product.dt, nomatch=0]\nhead(inner_join.dt)\nleft_join <- merge(view.dt, product.dt, by=\"Product\", all.x=TRUE)\nleft_join.dt <- product.dt[view.dt]\nhead(left_join.dt)\nright_join <- merge(view.dt, product.dt, by=\"Product\", all.y=TRUE)\nright_join.dt <- view.dt[product.dt]\nhead(right_join.dt)\nfull_outer_join <- merge(view.dt, product.dt, by=\"Product\", all=TRUE)\nhead(full_outer_join)\n\ndt <- data.table(product=c(\"A\", \"B\", \"B\", \"A\", \"A\"), price=c(100,200,300,100,200), quantity=c(2,2,1,3,2) )\ndt\nsetkey(dt, product, price)\ndt\ndt[c(\"A\",\"B\"),mult=\"first\"]\ndt[c(\"A\",\"B\"),mult=\"last\"]\ndt[\"B\",.(sum(price), sum(quantity))]\n\n\n#4.5.   Subsetting and Slicing Data with dplyr      00:02:08#####\n\ninstall.packages(\"dplyr\")\nlibrary(dplyr)\nquantity.over.3 <- filter(order.dt, Quantity >= 3)\nhead(quantity.over.3, 3)\nquantity.price.filter <-  filter(order.dt, Quantity >= 3, Price > 1000)\nhead(quantity.price.filter, 3)\nquantity.price.or.filter <-  filter(order.dt, Quantity >= 3 | Price > 1000)\nhead(quantity.price.or.filter, 3)\nquantity.price.in.filter <-  filter(order.dt, Product %in% c('P0006944501', 'P0006018073'))\nhead(quantity.price.in.filter, 3)\nslice(order.dt,1:3)\nslice(order.dt,(n()-1):n())\n\ninstall.packages(\"RSQLite\")\nlibrary(RSQLite)\norderdt <- fread(\"purchase_order.tab\", header=TRUE, sep='\\t')\nmy_db <- src_sqlite(\"order.sqlite\", create = T)\norder_sqlite <-copy_to(my_db, orderdt, temporary = FALSE)\ntbl(my_db, sql(\"SELECT Product FROM orderdt\"))\n\n\n#4.6.   Sampling Data with dplyr                00:01:25#####\n\n\n\n#4.7.   Selecting Columns with dplyr            00:02:40#####\n\nselect.quantity.price <-  select(order.dt, Quantity, Price)\nhead(select.quantity.price, 3)\nselect.not.price <-  select(order.dt, -Price)\nhead(select.not.price, 3)\nselect.everything <- select(order.dt, everything())\nhead(select.everything,3)\nselect.from.user.to.quantity <-  select(order.dt, User:Quantity)\nhead(select.from.user.to.quantity, 3)\nselect.contains.p <- select(order.dt, contains('P') )\nhead(select.contains.p,3)\nselect.p.price.over.1000 <- select(filter(order.dt, Price >= 1000 ), contains('P') )\nhead(select.p.price.over.1000, 3)\n\nset.seed(123)\ndf <- data.frame(a1=rnorm(3), a2=rnorm(3), b1=1, b2=NA, b3=\"str\")\nselect(df, num_range(\"a\", 1:2))\n\n\n#4.8.   Chaining Operations in dplyr            00:02:09#####\n\nsum(1:10)\n1:10 %>% sum()\nselect.p.price.over.1000 <- select(filter(order.dt, Price >= 1000 ), contains('P') )\nhead(select.p.price.over.1000, 3)\nchain.operations <- select(order.dt, contains('P') ) %>% filter(Price >= 1000 )\nhead(chain.operations,3)\nselect(order.dt, contains('P') ) %>% filter(Price >= 1000 ) %>% select(Price) %>% sum()\n\n?\"%.%\"\n\n\n#4.9.   Arranging Rows with dplyr               00:01:22#####\n\norder.dt %>% arrange(Price) %>% head(3)\norder.dt %>% arrange(desc(Price)) %>% head(3)\norder.dt %>% arrange(Price, desc(Quantity)) %>% head(3)\n\ns <- c(1,3,2,4,6)\ndesc(s)\n\n\n#4.10.  Eliminating Duplicated Rows with dplyr  00:01:39#####\n\norder.dt %>% select(Product) %>% distinct() %>% head(3)\ndistinct.product.user.dt <- order.dt %>% select(Product, User) %>% distinct()\nhead(distinct.product.user.dt, 3)\nnrow(order.dt)\nnrow(distinct.product.user.dt)\n\ndata.frame(a=c(1,2,1,1,2)) %>% select(a) %>% unique()\n\n\n#4.11.  Adding New Columns with dplyr           00:01:14#####\n\norder.dt %>%  select(Quantity, Price) %>% mutate(avg_price= Price/Quantity) %>% head()\ntransmute(order.dt, Avg_Price= Price/Quantity) %>% head()\n\norder.dt %>% select(Quantity, Price) %>% transform(Avg_Price= Price/Quantity) %>% head()\n\n\n#4.12.  Summarizing Data with dplyr         00:01:54#####\n\norder.dt %>% \n  select(User, Price) %>%\n  group_by(User) %>%\n  summarise(sum(Price)) %>%\n  head()\n\norder.dt %>% \n  select(User, Price, Quantity) %>%\n  filter(! is.na(Price)) %>%\n  group_by(User) %>%\n  summarise_each(funs(sum), Price, Quantity) %>%\n  head()\n\norder.dt %>% \n  select(User, Price) %>%\n  filter(! is.na(Price)) %>%\n  group_by(User) %>%\n  summarise_each(funs(max(., na.rm=TRUE), min(., na.rm=TRUE)), Price) %>%\n  head()\n\npurchase.dt %>% \n  select(User, Product) %>%\n  group_by(Product) %>%\n  summarise_each(funs(n())) %>%\n  head()  \n\npurchase.dt %>% \n  select(User, Product) %>%\n  group_by(Product) %>%\n  summarise_each(funs(n_distinct(User))) %>%\n  head()\n\nsample.df <-data.frame(user   =c(\"U1\", \"U1\", \"U1\", \"U3\"),\n                       product=c(\"A\" , \"B\" , \"A\" , \"B\" ),\n                       price  =c(200 ,  100,  300 , 300 ))\n                       \nsample.df %>% \ngroup_by(user,product) %>% \nsummarise(price_sum = sum(price)) %>% \narrange(price_sum)\n                       \nsample.df %>% \n  group_by(user,product) %>% \n  summarise(price_sum = sum(price)) %>% \n  ungroup() %>%\n  arrange(price_sum)\n\n#4.13.  Merging Data with dplyr             00:02:11#####\n\nproduct.dt <- order.dt[,.(Buy = length(Action)),by=Product]\nhead(product.dt[order(-Buy)])\nview.dt <- purchase.dt[, .(n_views = length(User)), by=Product]\nhead(view.dt[order(-n_views)])\nmerged.dt <- inner_join(view.dt, product.dt, by=\"Product\")\nhead(merged.dt)\nleft_join.dt <- left_join(view.dt, product.dt, by=\"Product\")\nhead(left_join.dt)\nright_join.dt <- right_join(view.dt, product.dt, by=\"Product\")\nhead(right_join.dt)\nfull_join.dt <- full_join(view.dt, product.dt, by=\"Product\")\nhead(full_join.dt)\n\nfirst.df  <- data.frame(a=c(\"A\",\"B\"), b=c(10,20))\nsecond.df <- data.frame(a=c(\"B\",\"C\"), c=c(\"P\",\"A\"))\nanti_join(second.df,first.df, by=\"a\")\nanti_join(first.df,second.df, by=\"a\")\n\n#5. Visualizing Data with ggplot2               00:26:42#####\n#5.1.   Creating Basic Plots with ggplot2           00:04:15#####\n\ninstall.packages(\"ggplot2\")\nlibrary(ggplot2)\nsuperstore <-read.csv('superstore_sales.csv', header=TRUE)\nsuperstore$Order.Date <- as.Date(superstore$Order.Date)\nstr(superstore)\nsum_price_by_province <- superstore %>% \n  filter(Order.Date > '2012-01-01') %>% \n  select(Sales, Province, Order.Date) %>% \n  group_by(Year_Month = as.Date(strftime(Order.Date,\"%Y/%m/01\")), Province) %>% \n  summarise(Total_Sales = sum(Sales))\nhead(sum_price_by_province)\nsample_sum <- sum_price_by_province %>% filter(Year_Month > '2012-07-01', Province %in% c('Alberta', 'British Columbia' ) ) \ng <- ggplot(data=sample_sum, mapping=aes(x=Year_Month, y=Total_Sales, colour=Province))\ng\ng <- g + geom_point()\ng\ng <- g + geom_line()\ng \ng <- g + xlab(\"Year Month\") + ylab(\"Sale Amount\") + ggtitle(\"Sale Amount By Region\")\ng\nqplot(Year_Month, Total_Sales, data=sample_sum, colour=Province, geom=c(\"line\", \"point\") ) \n\n#5.2.   Changing Aesthetics Mapping         00:03:09#####\n\ng <- ggplot(data=sample_sum, mapping=aes(x=Year_Month, y=Total_Sales, colour=Province))\ng + geom_point()\ng2 <- ggplot(data=sample_sum) + geom_point(mapping=aes(x=Year_Month, y=Total_Sales, colour=Province))\ng2 \ng + geom_point(aes(size=5))\ng + geom_point(size=5, colour=\"blue\") + geom_line()\ng + geom_point(aes(y=Total_Sales/10000))\ng + geom_point(aes(colour=NULL)) \ng  + geom_point(aes(colour=\"blue\"))\ng  + geom_point(colour=\"blue\")\n\n#5.3.   Introducing Geometric Objects           00:03:13#####\n\ng <- ggplot(data=sample_sum, mapping=aes(x=Year_Month, y=Total_Sales, col=Province ))\ng + geom_point() \ng+ geom_line(linetype=\"dashed\")\ng+geom_bar(stat = \"identity\", aes(fill=Province) , position = \"stack\")\ng+geom_bar(stat = \"identity\", aes(fill=Province), position = \"fill\")\ng+geom_bar(stat = \"identity\", aes(fill=Province), position = \"dodge\")\ng+geom_boxplot(aes(x=Province)) + xlab(\"Province\")\nset.seed(123)\nnorm.sample = data.frame(val=rnorm(1000))\nggplot(norm.sample, aes(val)) + geom_histogram(binwidth = 0.1)\nggplot(norm.sample, aes(val)) + geom_density()\nsample_stat <- sum_price_by_province  %>% select(Province, Total_Sales) %>% group_by(Province) %>% summarise(sales_stat=sum(Total_Sales))\nhead(sample_stat)\ng <- ggplot(sample_stat, aes(x = \"\", y=sales_stat, fill=Province)) + geom_bar(stat = \"identity\")\ng\ng +  coord_polar(\"y\", start=0)\n\n#5.4.   Performing Transformations          00:03:27#####\n\nsample_sum2 <- sum_price_by_province %>% filter(Province %in% c('Alberta', 'British Columbia' ) ) \ng <- ggplot(data=sample_sum2, mapping=aes(x=Year_Month, y=Total_Sales, col=Province ))\ng + geom_point() + geom_smooth()\ng + geom_point() + stat_smooth()\nlibrary(MASS)\ng + geom_point() + geom_smooth(method=rlm)\ng + geom_point() + geom_point(stat = \"summary\", fun.y = \"mean\", colour = \"red\", size = 4)\ng + geom_point() + stat_summary(fun.y = \"mean\", colour = \"red\", size = 4, geom=\"point\")\n\n#5.5.   Adjusting Scales                    00:02:16#####\n\ng <- ggplot(data=sample_sum, mapping=aes(x=Year_Month, y=Province, size=Total_Sales, colour = Province ))\ng + geom_point()\ng + geom_point(aes(size=Total_Sales)) + scale_size_continuous(range=c(1,10))\ng + geom_point(aes(colour=Total_Sales)) + scale_color_gradient()\ng+geom_point(aes(shape=Province) )  + scale_shape_manual(values=c(5,10))\ng2 <- ggplot(data=sample_sum, mapping=aes(x=Year_Month, y=Total_Sales, colour = Province ))\ng2+geom_bar(stat = \"identity\", aes(fill=Province), position = \"dodge\") + scale_fill_brewer(palette=2)\ng2+geom_bar(stat = \"identity\", aes(fill=Province), position = \"dodge\")  + scale_y_continuous(limits = c(1,100000), trans=\"log10\")\n?RColorBrewer\n\n#5.6.   Faceting                            00:02:06#####\n\ng <- ggplot(data=sample_sum, mapping=aes(x=Year_Month, y=Total_Sales, colour = Province ))\ng+geom_point() +  facet_wrap(~Province)\ng+geom_point() +  facet_wrap(~Province, ncol=1)\ng <- ggplot(data=sample_sum, mapping=aes(x=\"Total Sales\", y=Total_Sales, col=Province ))\ng+geom_bar(stat = \"identity\", aes(fill=Province)) + facet_grid(Year_Month ~ Province)\nlibrary(lattice)\nbarchart(Total_Sales ~ Province| Year_Month , data=sample_sum)\n\n#5.7.   Adjusting Themes                    00:01:33#####\n\ng <- ggplot(data=sample_sum, mapping=aes(x=Year_Month, y=Total_Sales, colour = Province ))\ng+geom_point(size=5) + theme_bw()\ng+geom_point(size=5) + theme_dark()\ng +geom_point(size=5) + \n  theme(\n    axis.text = element_text(size = 12),\n    legend.background = element_rect(fill = \"white\"),\n    panel.grid.major = element_line(colour = \"yellow\"),\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"blue\")\n  )\n?theme\n\n#5.8.   Combining Plots                 00:02:04#####\n\nlibrary(grid)\ngrid.newpage()\ng <- ggplot(data=sample_sum, mapping=aes(x=Year_Month, y=Total_Sales, colour = Province ))\nplot1 <- g + geom_point()\nplot2 <- g + geom_line() \npushViewport(viewport(layout = grid.layout(1, 2)))\nprint(plot1, vp =viewport(layout.pos.row = 1, layout.pos.col = 1))\nprint(plot2, vp =viewport(layout.pos.row = 1, layout.pos.col = 2))\ninstall.packages(\"gridExtra\")\nlibrary(\"gridExtra\") \ngrid.arrange(plot1,plot2, ncol=2)\n\n#5.9.   Creating Maps                       00:04:39#####\n\ninstall.packages(\"ggmap\")\ninstall.packages(\"maptools\")\nlibrary(ggmap)\nlibrary(maptools)\nnyc.shp <- readShapeSpatial(\"nycc.shp\")\nclass(nyc.shp)\nggplot() +  geom_polygon(data = nyc.shp, aes(x = long, y = lat, group = group), color = \"yellow\", size = 0.25)\nwifi.hotspot <- readShapeSpatial(\"wifi_hostspot.shp\")\nclass(wifi.hotspot)\nggplot() + geom_point(data=as.data.frame(wifi.hotspot), aes(x,y), color=\"red\")\nggplot() +  geom_polygon(data = nyc.shp, aes(x = long, y = lat, group = group), color = \"yellow\", size = 0.25)  + geom_point(data=as.data.frame(wifi.hotspot), aes(x,y), color=\"red\")\nc(min(wifi.hotspot$long), min(wifi.hotspot$lat),max(wifi.hotspot$long), max(wifi.hotspot$lat)) \nmap <- get_map(location = c(min(wifi.hotspot$long), min(wifi.hotspot$lat),max(wifi.hotspot$long), max(wifi.hotspot$lat)), source=\"osm\")\nggmap(map)\nggmap(map) + geom_point(data=as.data.frame(wifi.hotspot), aes(x=long,y=lat), color=\"red\")\n\n#6. Making Interactive Reports              00:24:13#####\n#6.1.   Creating R Markdown Reports         00:02:47#####\n\n\n\n#6.2.   Learning the Markdown Syntax            00:03:14#####\n\n?Sweave\n\n#6.3.   Embedding R Code Chunks             00:02:18#####\n\n\n\n#6.4.   Creating Interactive Graphics with ggvis    00:02:39#####\n\ninstall.packages(\"ggvis\")\nlibrary(ggvis)\nhouse <- read.csv('RealEstate.csv', header=TRUE)\nstr(house)\nhouse %>% ggvis(~Size, ~Price) %>% layer_points()\nhouse %>% filter(Price >= 500000) %>% ggvis(~Size, ~Price) %>% layer_points()\nhouse %>% ggvis(~Size, ~Price, fill := input_select(choices = c(\"red\", \"blue\", \"green\"), selected = \"blue\", label = \"Color\")) %>% layer_points() \n\n\n#6.5.   Understanding Basic Syntax and Grammar  00:01:57#####\n\nhouse %>% ggvis(~Size, ~Price, fill=~Status, size=10, shape=~Status) %>% layer_points()\ng %>% add_props(fill:=\"red\") %>% layer_points()\nhouse %>% ggvis(~Size, ~Price) %>% layer_points(fill=~Status)  %>% layer_smooths(stroke:= \"red\", strokeWidth:=3, se=TRUE) \nhouse %>% ggvis(~Size, ~Price) %>% layer_points(fill=~Status)  %>%   layer_model_predictions(formula= Price ~ Size, model = \"lm\", se = TRUE)\nhouse %>% ggvis(~Size, ~Price) %>% layer_points(fill=~Status)  %>% group_by(Status) %>% layer_model_predictions(model = \"lm\", stroke=~Status, strokeWidth:=3, se=TRUE)\nhouse %>% ggvis(~Size, fill:=\"blue\") %>% layer_histograms(width=300)\nhouse %>% ggvis(~Status, ~Price, fill=~Status) %>% layer_bars(width=0.5)\nhouse %>% ggvis(~Status, ~Price, fill=~Status)  %>% layer_boxplots() \nhouse %>% ggvis(~Size, ~Price) %>% layer_lines(strokeWidth:=5, stroke:=\"blue\")\n\n\n#6.6.   Controlling Axes and Legends and Using Scales   00:02:55#####\n\nhouse %>% ggvis(~Size, ~Price) %>% layer_points() %>% \n  add_axis(\"x\", title = \"Real Estate Square Feet\", orient=\"top\") %>% \n  add_axis(\"y\", title = \"Real Estate Price\", title_offset = 80)\nhouse %>% ggvis(~Size, ~Price) %>% layer_points() %>% \n  add_axis(\"x\", title = \"Real Estate Square Feet\", \n           subdivide = 4, values = seq(0,8000, by=1000)) %>% \n  add_axis(\"y\", title = \"Real Estate Price\", \n           subdivide = 5, values = seq(0,5500000, by=1000000), title_offset = 80)\nhouse %>% ggvis(~Size, ~Price) %>% layer_points() %>% \n  add_axis(\"x\", title = \"Real Estate Square Feet\", \n           properties = axis_props(\n             axis = list(strokeWidth = 2),\n             grid = list(stroke = \"blue\"),\n             ticks = list(stroke = \"red\", strokeWidth = 2),\n             labels = list(align = \"left\", fontSize = 10)\n           )) %>% \n  add_axis(\"y\", title = \"Real Estate Price\", title_offset = 80,\n           properties = axis_props(\n             axis = list(strokeWidth = 2),\n             grid = list(stroke = \"orange\"),\n           ))\nhouse %>% ggvis(~Status, ~Price, fill=~Status) %>% layer_bars(width=0.5)  %>%  add_legend(\"fill\", title=\"Sale Status\")\nhouse %>% ggvis(~Status, ~Price, fill=~Status) %>% layer_bars(width=0.5)  %>%  hide_legend(\"fill\")\nhouse %>% \n  ggvis(~Status, ~Price, fill=~Status) %>% \n  layer_bars(width=0.5)  %>%  \n  add_legend(\"fill\", title=\"Sale Status\", \n             properties = legend_props(\n               title = list(fontSize = 14),\n               labels = list(fontSize = 12),\n               symbol = list(shape = \"square\", size = 100)\n             ))\n\n##  Using scales\n```\nhouse %>% ggvis(~Status, ~Price, fill=~Status) %>% layer_bars() %>% scale_numeric(\"y\", trans = \"pow\", exponent = 0.2)\nhouse %>% ggvis(~Status, ~Price, fill=~Status) %>% layer_bars() %>% scale_nominal(\"fill\", range = c(\"pink\", \"green\", \"lightblue\"))\n\n\n#6.7.   Adding Interactivity to a ggvis Plot        00:03:40#####\n\n\nhouse %>% ggvis(~Status, ~Price, fill:= (c(\"red\",\"blue\"), label=\"Fill Color\")) %>% layer_bars() \nhouse %>% ggvis(~Size, ~Price, fill:= input_text(\"red\",label=\"Fill Color\"), size:=input_slider(1,10)) %>% layer_points()\nhouse %>% ggvis(~Size, ~Price) %>% \n  layer_points(fill=~Status)  %>%  \n  layer_model_predictions(\n    formula= Price ~ Size, \n    se = TRUE,\n    model = input_radiobuttons(c(\"loess\" = \"loess\", \"Linear\" = \"lm\"),\n                               label = \"Model type\"))\nsale_type <- as.character(unique(house$Status))\nhouse %>%\n  ggvis(~Size, ~Price, fill=~Status) %>%\n  filter(Status %in% eval(input_select(sale_type, multiple=TRUE)) ) %>%\n  layer_points()\nsale_type <- as.character(unique(house$Status))\n\nhouse %>%\n  ggvis(~Size, ~Price, fill=~Location) %>%\n  filter(Status %in% eval(input_checkboxgroup(sale_type)) ) %>%\n  layer_points() %>% hide_legend(\"fill\")\nhouse %>%\n  ggvis(x=input_radiobuttons(c(\"Size\", \"Price\"), map=as.name), \n        y=~Price, \n        fill=~Status) %>%\n  layer_points() %>% hide_legend(\"fill\")\ndisplay_val <- function(x) {\n  if(is.null(x)) return(NULL)\n  paste0(names(x), \": \", format(x), collapse = \"<br />\")\n}\nhouse %>%\n  ggvis(~Size, ~Price, fill=~Status) %>%\n  filter(Status %in% eval(input_checkboxgroup(sale_type)) ) %>%\n  layer_points() %>% \n  add_tooltip(display_val, on=\"hover\")\n\n#6.8.   Creating an R Shiny Document            00:02:15#####\n\nhelp(package=\"shiny\")\n\n#6.9.   Publishing an R Shiny Report            00:02:28#####\n\n#7. Simulation from Probability Distributions       00:21:43#####\n#7.1.   Generating Random Samples           00:02:51#####\n\nsample(10)\nset.seed(123)\nsample(10)\nsample(10,2)\nsample.int(10,size=2)\nsample.int(42,6)\nsample(c(1,0), 10, replace=TRUE)\nsample(c(\"HEAD\",\"TAIL\"), 10, replace=TRUE)\nfair <- sample(c(1:6), 200, replace=TRUE)\ntable(fair)\nloaded <- sample(c(1:6), 200, replace=TRUE, prob=c(0.1,0.1,0.1,0.1,0.1,0.5))\ntable(loaded)\n\nset.seed(123)\nsample(10)\nsample(10)\nset.seed(123)\nsample(10)\nsample(10)\nset.seed(123)\nhead(.Random.seed)\n\n\n#7.2.   Understanding Uniform Distributions     00:01:38#####\n\n\n\n#7.3.   Generating Binomial Random Variates     00:02:30#####\n\n\n\n#7.4.   Generating Poisson Random Variates      00:02:06#####\n\n\n\n#7.5.   Sampling from a Normal Distribution     00:04:07#####\n\n\n\n#7.6.   Sampling from a Chi-Squared Distribution    00:01:59#####\n\n\n\n#7.7.   Understanding Student's t- Distribution 00:02:11#####\n\n\n\n#7.8.   Sampling from a Dataset             00:01:52#####\n\n\n\n#7.9.   Simulating the Stochastic Process       00:02:29#####\n\n\n\n#8. Statistical Inference in R                  00:24:54#####\n#8.1.   Getting Confidence Intervals            00:05:54#####\n\nset.seed(123)\npopulation <- rnorm(1000, mean = 10, sd = 3)\n\ndens <- density(population)\nplot(dens, col=\"red\")\nsamp = sample(population, 100)\nmean(samp)\nsd(samp)\n\n1 - 0.01 / 2\n\nqnorm(0.995)\n\nsde <-(sd(samp)/sqrt(100))\nsde\n\nupper <- mean(samp) + qnorm(0.995) * sde\nupper\n\nlower <- mean(samp) - qnorm(0.995) * sde\nlower\n\npolygon(c(lower, dens$x[dens$x>lower & dens$x < upper], upper), \n        c(0, dens$y[dens$x>=lower & dens$x <= upper], 0), \n        col=\"red\", density = c(10, 50) ,angle = c(-45, 45))\n\n1 - 0.05 / 2\n\nqnorm(0.975)\n\nupper2 <- mean(samp) + qnorm(0.975) * sde\nupper2\n\nlower2 <- mean(samp) - qnorm(0.975) * sde\nlower2\n\npolygon(c(lower2, dens$x[dens$x>lower2 & dens$x < upper2], upper2), \n        c(0, dens$y[dens$x>=lower2 & dens$x <= upper2], 0), \n        col=\"blue\")\n\nset.seed(123)\npopulation2 <- rpois(1000, lambda = 3)\nmean(population2)\nhist(population2)\nsample2 = sample(population2, 100)\t\t\t\t  \nsample_mean <- rep(NA, 1000)\nfor(i in 1:1000){\n  sample_mean[i] <- mean(sample(sample2, replace=TRUE))\n}\nupper3 <-  quantile(sample_mean, 0.975)\nupper3\n\nlower3 <- quantile(sample_mean, 0.025)\nlower3\nboot.ci(z)\t\t\t\t  \n\n\n#8.2.   Performing Z-tests                  00:03:12#####\n\npop_mean <- 300\npop_sd <- 10\ncoke = c\nsde <- pop_sd / sqrt(length(coke))\nz   <- (mean(coke) - pop_mean) / sde\nz\npnorm(z)\n1- pnorm(z)\np   <- (1 - pnorm(abs(z))) * 2\np\nz.test <- function(x, pop_mean, pop_sd, side=\"twoside\"){\n  sde <- pop_sd / sqrt(length(x))\n  z   <- (mean(x) - pop_mean) / sde\n  \n  switch(side, \n         twoside={\n           p   <- (1 - pnorm(abs(z))) * 2\n         },\n         less={\n           p   <- pnorm(z)\n         },\n         {\n           p   <- 1- pnorm(z)\n         }\n  )\n  return(list(z = z , p = p))\n}\nz.test(a, 300,10)\n\nz.test(a, 300,10, \"l\")\n\ninstall.packages(\"BSDA\")\nlibrary(BSDA)\n?z.test \n\n\n#8.3.   Performing Student's t-Tests            00:02:15#####\n\nweight = c(84.12,85.17,62.18,83.97,76.29,76.89,61.37,70.38,90.98,85.71,89.33,74.56,82.01,75.19,80.97,93.82,78.97,73.58,85.86,76.44)\nboxplot(weight)\nabline(h=70,lwd=2, col=\"red\")\nt.test(weight, mu = 70)\n\nweight2 <- c(69.35,63.21,71.57,73.23,65.26,60.32,66.96,59.78,69.71,76.88,81.39,64.9,75.53,65.05,77.21,64.9,71.93,75.04,74.29,77.53)\nboxplot(list(weight, weight2))\nabline(h=mean(weight),lwd=2, col=\"blue\")\nabline(h=mean(weight2),lwd=2, col=\"red\")\n\nt.test(weight, weight2)\n\n?t.test\n\n## Identifying skewness\n```\nset.seed(123)\nx <- rnorm(100,mean=3,sd=5)\nhist(x)\nset.seed(123)\ny = runif(100,0,5)\nhist(y)\nshapiro.test(x)\n\nshapiro.test(y)\n\ninstall.packages(\"tseries\")\nlibrary(tseries)\n\njarque.bera.test(x)\n\njarque.bera.test(y)\n\n\n#8.4.   Conducting Exact Binomial Tests         00:02:09#####\n\nbinom.test(x=92, n=315, p=1/6)\n\n?binom.test\n\n\n#8.5.   Performing Kolmogorov-Smirnov Tests 00:02:16#####\n\nset.seed(123)\nx = rnorm(50)\nks.test(x,\"pnorm\")\n\nset.seed(123)\nx <- runif(n=20, min=0, max=20)\n\ny <- runif(n=20, min=0, max=20)\n\nplot(ecdf(x), do.points = FALSE, verticals=T, xlim=c(0, 20))\nlines(ecdf(y), lty=3, do.points = FALSE, verticals=T)\nks.test(x,y)\n\n?ks.test\n?ecdf\n\n\n#8.6.   Working with the Pearson's Chi-Squared Tests00:01:40#####\n\nmat <- matrix(c(2047, 2522, 3512, 1919), nrow = 2, dimnames = list(c(\"smoke\",\"non-smoke\"), c(\"male\",\"female\")))\nmat\nmosaicplot(mat, main=\"Portion of male and female smokers/non-smokers\", color = TRUE)\nchisq.test(mat)\n\n? chisq.test\n\n\n#8.7.   Understanding the Wilcoxon Rank Sum and Signed Rank Tests       00:01:48#####\n\nlikes <- c(17,40,57,30,51,35,59,64,37,49,39,41,17,53,21,28,46,23,14,13,11,17,15,21,9,17,10,11,13,16,18,17,27,11,12,5,8,4,12,7,11,8,4,8,7,3,9,9,9,12,17,6,10)\nhist(likes)\nwilcox.test(likes, mu = 30)\nlikes2 = c(28,152,197,25,62,39,32,202,85,74,125,32,67,29,37,297,101,45,24,63,17,92,46,60,317,85,46,61,56,59,91,54,133,87,200,28,97,28,30,103,77,78,80,159,39,46,151,278,75,124,213,35,145,68,30,71,58,52,36,61,48,31,165,93,74,30,86,88,145,21,47,167,63,55,36,215,52,84,24,189,65,44,101,36,39,98,140,32,65,33,84,61,45,40,160,64,65,41,36,165)\nhist(likes2)\nboxplot(list(likes, likes2))\nwilcox.test(likes, likes2)\n\n? wilcox.test\n\n#8.8.   Conducting One-way ANOVA            00:02:39#####\n\ndata_scientist = c(95694,82465,85001,74721,73923,94552,96723,90795,103834,120751,82634,55362,105086,79361,79679,105383,85728,71689,92719,87916)\nsoftware_eng = c(78069,82623,73552,85732,75354,81981,91162,83222,74088,91785,89922,84580,80864,70465,94327,70796,104247,96391,75171,65682)\nbi_eng = c(62895,72568,67533,66524,60483,69549,62150,53320,66197,79189,64246,76079,53821,69444,75194,73011,71056,63592,61502,59758)\n\nsalary <- c(data_scientist, software_eng, bi_eng)\nprofession <- c(rep(\"Data Scientist\",20), rep(\"Software Engineer\",20), rep(\"BI Engineer\",20))\n\nboxplot(salary ~ profession,xlab='Profession', ylab = \"Salary\")\noneway.test(salary ~ profession)\nsalary.aov <- aov(salary ~ profession)\n\nsummary(salary.aov)\nmodel.tables(salary.aov, \"means\")\nsalary.posthoc =TukeyHSD(salary.aov)\nsalary.posthoc\n\n\n#8.9.   Performing Two-way ANOVA            00:03:01#####\n\nengineer = read.csv(\"engineer.csv\", header = TRUE)\npar(mfrow=c(1,2))\nboxplot(Salary~Profession, data = engineer,xlab='Profession', ylab = \"Salary\",main='Salary v.s. Profession')\nboxplot(Salary~Region, data = engineer,xlab='Region', ylab = \"Salary\",main='Salary v.s. Region')\nboxplot(Salary~Profession * Region, \n        data = engineer,xlab='Profession * Region', \n        ylab = \"Salary\",\n        cex.axis=0.7,\n        main='Salary v.s. Profession * Region')\ninteraction.plot(engineer$Region, engineer$Profession, engineer$Salary, \n                 type=\"b\", col=c(1:3),leg.bty=\"o\", \n                 leg.bg=\"beige\", lwd=2, \n                 pch=c(18,24,22), \n                 xlab=\"Region\", ylab=\"Salary\", main=\"Interaction Plot\")\nsalary.anova = aov(Salary~Profession * Region, data=engineer)\nsummary(salary.anova)\nTukeyHSD(salary.anova)\nplot(TukeyHSD(salary.anova))\n\n?MANOVA\n\n## Investigating data relationship with regression\n\ngdp = read.csv(\"GDP.csv\", header=TRUE)\nplot(GDP ~ Export, data=gdp)\nfit <- lm(GDP~ Export, data=gdp)\nsummary(fit)\nanova(fit)\nabline(fit, col=\"red\")\n\n?glm\n```\n\n#9. Rule and Pattern Mining with R          00:26:02#####\n#9.1.   Transforming Data into Transactions     00:05:11#####\n\ninstall.packages(\"arules\")\nlibrary(arules)\norder.dt <- fread(\"purchase_order.tab\", header=TRUE, sep='\\t')\nproduct_by_user <- aggregate(Product ~ User,\n                             data=order.dt,\n                             FUN=function(e) list(unique(e) ))\ntrans = as(product_by_user $Product, \"transactions\")\ntrans\ntr_list = list(c(\"Apple\", \"Bread\", \"Cake\"),\n               c(\"Apple\", \"Bread\", \"Milk\"),\n               c(\"Bread\", \"Cake\", \"Milk\"))\nnames(tr_list) = paste(\"Tr\",c(1:3), sep = \"\")\ntrans = as(tr_list, \"transactions\")\ntrans\ntr_matrix = matrix(\n  c(1,1,1,0,\n    1,1,0,1,\n    0,1,1,1), ncol = 4)\ndimnames(tr_matrix) =  list(\n  paste(\"Tr\",c(1:3), sep = \"\"),\n  c(\"Apple\",\"Bread\",\"Cake\", \"Milk\")\n)\ntrans2 =  as(tr_matrix, \"transactions\")\ntrans2\n\n\n#9.2.   Displaying Transactions and Associations    00:03:02#####\n\nLIST(trans) %>% head(3)\nsummary(trans)\ninspect(trans[1:3])\nfilter_trans = trans[size(trans) >=3]\ninspect(filter_trans[1:3])\nimage(trans[1:2000])\nitemFrequencyPlot(trans, topN=10, type=\"absolute\")\nhelp(itemFrequency)\n\n\n#9.3.   Mining Associations with the Apriori Rule   00:04:18#####\n\nrules <- apriori(trans, parameter = list(supp = 0.001, conf = 0.1, target= \"rules\"))\n\nsummary(rules)\ninspect(rules)\nrules <- sort(rules, by=\"confidence\", decreasing=TRUE)\ninspect(rules)\nhead(interestMeasure(rules, c(\"support\", \"chiSquare\", \"confidence\", \"conviction\",\"cosine\", \"coverage\", \"leverage\", \"lift\",\"oddsRatio\"), Groceries))\n\n\n#9.4.   Pruning Redundant Rules             00:02:14#####\n\nrules.sorted = sort(rules, by=\"lift\")\nsubset.matrix = is.subset(rules.sorted, rules.sorted)\nsubset.matrix[lower.tri(subset.matrix, diag=T)] = NA\nredundant = colSums(subset.matrix, na.rm=T) >= 1\nrules.pruned = rules.sorted[!redundant]\ninspect(rules.pruned)\nhelp(is.superset)\nhelp(is.subset)\n\n\n#9.5.   Visualizing Association Rules           00:02:35#####\n\ninstall.packages(\"arulesViz\")\nlibrary(arulesViz)\nplot(rules.pruned)\nplot(rules.pruned,method=\"grouped\")\nplot(rules.pruned,method=\"graph\")\n\n\n#9.6.   Mining Frequent Itemsets with Eclat     00:03:08#####\n\nfrequentsets=eclat(trans,parameter=list(support=0.01,maxlen=10))\nsummary(frequentsets)\ninspect(sort(frequentsets,by=\"support\"))\n\n\n#9.7.   Creating Transactions with Temporal Information 00:02:52#####\n\ninstall.packages(\"arulesSequences\")\nlibrary(arulesSequences)\ntraffic <- read.csv('web_traffic.csv', heade=TRUE)\ntraffic$sequence <- as.numeric(traffic$IP)\ntraffic <- traffic[order(traffic$sequence, traffic$Timestamp),]\ntraffic_data<-data.frame(item=traffic$Page)\ntraffic.tran<-as(tmp_data,\"transactions\")\ntransactionInfo(traffic.tran)$sequenceID <- traffic$sequence\ntransactionInfo(traffic.tran)$eventID<-traffic$Timestamp\ntraffic.tran\ninspect(head(traffic.tran))\nsummary(traffic.tran)\nzaki=read_baskets(con = system.file(\"misc\", \"zaki.txt\", package = \"arulesSequences\"), info = c(\"sequenceID\",\"eventID\",\"SIZE\"))\nas(zaki, \"data.frame\")\n\n\n#9.8.   Mining Frequent Sequential Patterns with cSPADE 00:02:42#####\n\nfrequent_pattern <-cspade(traffic.tran,parameter = list(support = 0.50))\ninspect(frequent_pattern)\nsummary(frequent_pattern)\nas(frequent_pattern, \"data.frame\")\n\n#10.    Time Series Mining with R               00:29:51#####\n#10.1.  Creating Time Series Data               00:05:11#####\n\ntw2330 = read.csv('tw2330_finance.csv', header=TRUE)\nstr(tw2330)\nm <- ts(tw2330$Total.Income, frequency= 4, start=c(2008, 1))\nclass(m)\nm\nm2 <- window(m, start=c(2012, 2), end=c(2014, 2)) \nm2\nm_ts <- ts(tw2330[,-1], start=c(2008,01), frequency = 4)\nclass(m_ts)\nhead(m_ts)\nm_ts[,\"EPS\"]\ninstall.packages(\"xts\")\nlibrary(xts)\nm.xts <- as.xts(m)\nhead(m.xts)\nsub.m.xts <- window(m.xts, start = \"2010 Q1\", end = \"2012 Q1\")\nsub.m.xts\n\n\n#10.2.  Plotting a Time Series Object           00:02:26#####\n\nplot.ts(m)\nplot.ts(m_ts, plot.type = \"multiple\",)\nplot.ts(m_ts, plot.type = \"single\", col=c(\"red\",\"green\",\"blue\", \"orange\"))\nplot.ts(m_ts[,\"EPS\"])\nplot.xts(m.xts)\n\n\n#10.3.  Decomposing Time Series             00:02:11#####\n\nm.sub = window(m, start=c(2012, 1), end=c(2014, 4)) \nm.sub\nplot(m.sub)\ncomponents <- decompose(m.sub)\nnames(components)\ncomponents$seasonal\nplot(components)\ncomp1 <- stl(m.sub, s.window=\"periodic\")\nnames(comp1)\nplot(ccomp1)\n\n\n#10.4.  Smoothing Time Series               00:05:21#####\n\nm.pre <- HoltWinters(m)\nm.pre\nplot(m.pre)\nm.pre$SSE\ninstall.packages(\"fpp\")\nlibrary(fpp)\nsummary(fit)\nplot.ts(m)\nlines(fitted(fit), col=\"red\")\n\n\n#10.5.  Forecasting Time Series             00:02:30#####\n\nlibrary(forecast)\nincome.pre <- forecast.HoltWinters(m.pre, h=4)\nsummary(income.pre)\nplot(income.pre)\nacf(income.pre$residuals)\nBox.test(income.pre$residuals)\nlibrary(fpp)\nfit <- hw(m,seasonal=\"additive\")\nplot(fit)\n\n\n#10.6.  Selecting an ARIMA Model                00:03:18#####\n\nset.seed(123)\nts.sim <- arima.sim(list(order = c(1,1,0), ar = 0.7), n = 100)\nplot(ts.sim)\nts.sim.diff <- diff(ts.sim)\nplot(ts.sim.diff)\nacf(ts.sim.diff)\npacf(ts.sim.diff)\ntsdisplay(ts.sim.diff)\nggtsdisplay(ts.sim.diff)\n\n\n#10.7.  Creating an ARIMA Model             00:02:19#####\n\nlibrary(forecast)\nfit <- Arima(ts.sim, order=c(1,1,0))\nfit\naccuracy(fit)\nauto.arima(ts.sim, ic=\"bic\")\nfit2 <- arima(ts.sim)\nsummary(fit2)\n\n\n#10.8.  Forecasting with an ARIMA Model     00:02:11#####\n\nfit.predict <- forecast.Arima(fit)\nsummary(fit.predict)\nplot.forecast(fit.predict)\nacf(fit.predict$residuals)\nBox.test(fit.predict$residuals)\ntsdiag(fit)\n\n\n#10.9.  Predicting Stock Prices with an ARIMA Model 00:04:24#####\n\ninstall.packages(\"quantmod\")\nlibrary(quantmod)\ngetSymbols(\"FB\",src=\"yahoo\", from=\"2015-01-01\")\nplot(FB)\nfit <- auto.arima(FB$FB.Close, ic=\"bic\")\nfit\nplot(as.ts(FB$FB.Close) )\nlines(fitted(fit), col=\"red\")\nfit.forecast <- forecast.Arima(fit)\nfit.forecast\nplot(fit.forecast)\nFB_Return<-diff(FB$FB.Close)/lag(FB$FB.Close,k=-1)*100\nhead(FB_Return)\nhist(FB_Return)\n\n#11.    Supervised Machine Learning             00:41:11#####\n#11.1.  Fitting a Linear Regression Model with lm   00:05:34#####\n\nhouse <- read.csv('house_rental.csv', header=TRUE)\n\nglmfit <- glm(Price ~ Sqft, data=house)\nglmfit\n\nplot(Price ~ Sqft, data=house)\nabline(glmfit, col=\"red\")\n\nglmfit2 <- glm(Price ~ poly(Sqft,2), data=house)\nglmfit2\n\nplot(Price ~ Sqft, data=house)\nlines(sort(house$Sqft), glmfit2$fit[order(house$Sqft)], col = \"red\")\n\nrlmfit = rlm(Price~Sqft, data=house)\nrlmfit\n\nplot(Price ~ Sqft, data=house)\nabline(rlmfit, col=\"red\")\n\n\n#11.2.  Summarizing Linear Model Fits           00:02:53#####\n\nsummary(glmfit)\n\nsummary(glmfit2)\n\nstepAIC(glmfit)\n\n\n#11.3.  Using Linear Regression to Predict Unknown Value    00:03:57#####\n\nnewdata <- data.frame(Sqft=c(800, 900, 1000)) \n\npredict(glmfit2 ,newdata)\n\n\n#11.4.  Measuring the Performance of the Regression Model00:03:23#####\n\npredicted <- predict(fit, house$Sqft)\n\nactual <- predict(glmfit, data.frame(Sqft=house$Sqft))\nrmse <- (mean((predicted - actual)^2))^0.5\nrmse\n\nmu <- mean(actual)\nrse <- mean((predicted - actual)^2) / mean((mu - actual)^2) \nrse\n\nrsquare <- 1 - rse\nrsquare\n\ntune(glm, Price~., data=house)\n\n\n#11.5.  Performing a Multiple Regression Analysis   00:04:17#####\n\n\n\n#11.6.  Selecting the Best-Fitted Regression Model with Stepwise Regression 00:02:42#####\n\n\n\n#11.7.  Applying the Gaussian Model for Generalized Linear Regression       00:02:19#####\n\n\n\n#11.8.  Performing a Logistic Regression Analysis   00:04:30#####\n\n\n\n#11.9.  Building a Classification Model with Recursive Partitioning Trees       00:03:58#####\n\ninstall.packages(\"rpart\")\nlibrary(rpart)\n\ncustomer = read.csv('customer.csv', header=TRUE)\n\nset.seed(33)\nidx <- sample(c(1,2),nrow(customer),prob = c(0.8,0.2), replace=TRUE)\ntrainset <- customer[idx == 1, ]\ntestset  <- customer[idx == 2, ]\n\nfit <- rpart(buy ~ gender + age + visit.times, data = trainset)\n\nfit\n\nprintcp(fit)\n\nplotcp(fit)\n\nsummaryfit)\n\n?rpart \n?printcp\n?summary.rpart\n\n\n#11.10. Visualizing Recursive Partitioning Tree 00:02:14#####\n\nplot(fit, margin= 0.1)\ntext(fit, all=TRUE, use.n = TRUE)\n\nplot(fit, uniform=TRUE, branch=0.6, margin=0.1)\ntext(fit, all=TRUE, use.n = TRUE)\n\n\n#11.11. Measuring Model Performance with a Confusion Matrix 00:01:38#####\n\npred = predict(fit, testset[,! names(testset) %in% c(\"buy\")], type=\"class\")\n\ntable(pred, testset[,c(\"buy\")])\n\nconfusionMatrix(pred, testset[,c(\"buy\")])\n\n\n#11.12. Measuring Prediction Performance Using ROCR 00:03:46#####\n\ninstall.packages(\"ROCR\")\nlibrary(ROCR)\n\npred2 <- predict(fit,testset[, !names(testset) %in% c(\"buy\")], probability=TRUE)\n\npred.to.roc = pred2[,c(\"yes\")]\n\npred.rocr = prediction(pred.to.roc, testset$churn)\n\nperf.rocr = performance(pred.rocr, measure = \"auc\", x.measure = \"cutoff\") \nperf.tpr.rocr = performance(pred.rocr, \"tpr\",\"fpr\")\n\nplot(perf.tpr.rocr, colorize=T,main=paste(\"AUC:\",(perf.rocr@y.values)))\n\n\n#12.    Unsupervised Machine Learning           00:30:16#####\n#12.1.  Clustering Data with Hierarchical Clustering    00:06:10#####\n\nhotel <- read.csv('taipei_hotel.csv', header=TRUE)\nstr(hotel)\n\nplot(hotel$lon, hotel$lat)\n\nhotel.dist <- dist(hotel[,c('lat', 'lon')] , method=\"euclidean\")\nhc <- hclust(hotel.dist, method=\"ward.D2\")\nhc\n\nplot(hc, hang = -0.01, cex = 0.7)\n\nhc2 = hclust(hotel.dist, method=\"single\")\nplot(hc2, hang = -0.01, cex = 0.7)\n\n? dist\t\n? hclust\n\ndv = diana(hotel, metric = \"euclidean\")\n\nsummary(dv)\n\nplot(dv)\n\n\n#12.2.  Cutting Tree into Clusters              00:01:44#####\n\nfit <- cutree(hc, k = 3)\n\nfit\n\ntable(fit)\n\nplot(hotel$lon, hotel$lat, col=fit)\n\nplot(hc)\nrect.hclust(hc, k = 3 , border=\"red\")\n\nplot(hc)\nrect.hclust(hc, k = 3 , which =2, border=\"red\")\n\n\n#12.3.  Clustering Data with the k-means Method 00:02:09#####\n\nset.seed(22)\nfit <- kmeans(hotel[,c(\"lon\", \"lat\")], 3)\nfit\n\nplot(hotel$lon, hotel$lat, col = fit$cluster)\n\nhelp(kmeans)\n\n\n#12.4.  Clustering Data with the Density-=Based Method  00:03:11#####\n\ninstall.packages(\"dbscan\")\nlibrary(dbscan)\n\nfit <- dbscan(hotel.dist, eps = 0.01, minPts = 3)\nfit\n\nplot(hotel$lon,hotel$lat, col=fit$cluster)\n\nnewdata = data.frame(lon= 121.51, lat=25.13)\n\npredict(fit, hotel[,c('lon', 'lat')], newdata)\n\ninstall.packages(\"fpc\")\nlibrary(fpc)\n\n\n#12.5.  Extracting Silhouette Information From Clustering   00:01:50#####\n\ninstall.packages(âclusterâ)\nlibrary(cluster)\n\nset.seed(22)\nkm <- kmeans(hotel[,c('lon', 'lat')], 3)\n\nhotel.dist <- dist(hotel[,c('lat', 'lon')] , method=\"euclidean\")\nkms <- silhouette(km$cluster, hotel.dist)\nsummary(kms)\n\nplot(kms)\n\n\n#12.6.  Comparing Clustering Methods                00:02:12#####\n\ninstall.packages(\"fpc\")\nlibrary(fpc)\n\nhotel.dist <- dist(hotel[,c('lat', 'lon')] , method=\"euclidean\")\nsingle_c <- hclust(hotel.dist, method=\"single\")\n\nhc_single <- cutree(single_c, k = 3)\n\ncomplete_c <- hclust(hotel.dist,method=\"complete\")\nhc_complete <- cutree(complete_c, k = 3)\n\nset.seed(22)\nkm <- kmeans(hotel[,c('lon', 'lat')], 3)\n\ncs = cluster.stats(hotel.dist, km$cluster)\n\ncs[c(\"within.cluster.ss\",\"avg.silwidth\")]\n\nsapply(list(kmeans = km$cluster, hc_single = hc_single, hc_complete = hc_complete), function(c) cluster.stats(hotel.dist, c)[c(\"within.cluster.ss\",\"avg.silwidth\")])\n\nset.seed(22)\nkm <- kmeans(hotel[,c('lon', 'lat')], 3)\nkm$withinss\nkm$betweenss\n\n\n#12.7.  Recognizing Digits Using the Density-Based Clustering Method 00:01:52#####\n\ninstall.packages(\"png\")\nlibrary(png)\n\nimg2 = readPNG(\"handwriting.png\", TRUE)\nimg3 = img2[,nrow(img2):1]\nb = cbind(as.integer(which(img3 < -1) %% 28), which(img3 < -1) / 28)\nplot(b, xlim=c(1,28), ylim=c(1,28))\n\nset.seed(18)\nfit = kmeans(b, 2)\nplot(b, col=fit$cluster)\nplot(b, col=fit$cluster,  xlim=c(1,28), ylim=c(1,28))\n\nds = dbscan(b, 2)\nds\n\nplot(ds, b,  xlim=c(1,28), ylim=c(1,28))\n\nhelp(package=\"png\")\n\n\n#12.8.  Grouping Similar Text Documents with k-means Clustering Method 00:02:14#####\n\n\n\n#12.9.  Performing Dimension Reduction with Principal Component Analysis (PCA)  00:02:51#####\n\neco.freedom <- read.csv('index2015_data.csv', header=TRUE)\n\neco.measure <- eco.freedom[,5:14]\n\neco.pca = prcomp(eco.measure, center = TRUE, scale  = TRUE)\neco.pca\n\nsummary(eco.pca)\n\npredict(eco.pca, newdata=head(eco.measure, 1))\n\neco.princomp = princomp(eco.measure,\n                        center = TRUE,\n                        scale  = TRUE)\nswiss.princomp\n\nsummary(eco.princomp)\n\npredict(eco.princomp, eco.measure[1,])\n\n\n#12.10. Determining the Number of Principal Components Using a Scree Plot   00:01:50#####\n\nscreeplot(swiss.pca, type=\"barplot\")\n\nscreeplot(eco.pca, type=\"line\")\n\nhelp(screeplot)\n\ninstall.packages(\"nFactors\")\nlibrary(nFactors)\ninstall.packages(\"nFactors\")\nlibrary(nFactors)\nev = eigen(cor(eco.measure))\nap = parallel(subject=nrow(eco.measure),var=ncol(eco.measure),rep=100,cent=.05) \nnS = nScree(x=ev$values, aparallel=ap$eigen$qevpea)\nplotnScree(nS)\n\n\n#12.11. Determining the Number of Principal Components Using the Kaiser Method  00:01:19#####\n\neco.pca$sdev \n\neco.pca$sdev ^ 2\n\nwhich(eco.pca$sdev ^ 2> 1)\n\nscreeplot(eco.pca, type=\"line\")\nabline(h=1, col=\"red\", lty= 3)\n\n\n#12.12. Visualizing Multivariate Data using a biplot                    00:02:54#####\n\n\nplot(eco.pca$x[,1], eco.pca$x[,2], xlim=c(-6,6), ylim = c(-4,3))\ntext(eco.pca$x[,1], eco.pca$x[,2], eco.freedom[,2], cex=0.7, pos=4, col=\"red\")\n\nrownames(eco.pca$x) =  as.character(eco.freedom[,2])\nbiplot(eco.pca)\n\ninstall.packages(\"devtools\")\nlibrary(devtools)\ninstall_github(\"ggbiplot\", \"vqv\")\nlibrary(ggbiplot)\ng <- ggbiplot(eco.pca, obs.scale = 1, var.scale = 1, ellipse = TRUE,  circle = TRUE)\nprint(g)\n",
    "created" : 1489921537942.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "579871367",
    "id" : "6E74A89C",
    "lastKnownWriteTime" : 1488002773,
    "last_content_update" : 1488002773,
    "path" : "~/Data Science with R/DSRMS.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}