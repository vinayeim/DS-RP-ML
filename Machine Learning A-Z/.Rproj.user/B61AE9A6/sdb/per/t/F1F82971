{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Regression\"\nauthor: \"Vinay Kumar\"\ndate: \"March 24, 2017\"\noutput: html_document\n---\n\n\n## Simple Liner Regression\n\n\n**Formula**\n\n$y = b_0 + b_1*X_1$\n\n**Variable Defination**\n\n* y = Dependent variable(DV).\n* X = Independent variable(IV).\n* $b_1$ = Cofficient for the Independent variable.\n* $b_0$ = Constant term.\n\n```{r=Simple Liner Regression}\n\n```\n\n\n**Example for Simple Linear Regression**\n\nIn this example where we have experience and salary So experience is going to be out horizontal axis and the salary is all vertical axis and we want to understand how people's salary depends on their experence\n\n*So here what Regression does:*\n\n*Formula* : $y = b_0 + b_1*X_1$\n\n*In our case the Formula* : $Salary = b_0 + b_1*Experience$\n\n*Constant $b_0$* : The Constant means the point where the line crosses the vertical axis and let's say it's the Basic (or) Starting salary let's assume 30000 was the starting salary if the person have no exprence joined in the company.\n\n*Cofficient $b_1$* : This is the slope of the line and the steeper the line the more you get more money you get per extra year of expereience.\n\n**Ordinary Least Squares**\n\n*how to Find the best fitting line (or) how the simple linear regression find the line for you.*\n\nThe ordinary least squares (OLS) or linear least squares is a method for estimating the unknown parameters in a linear regression model, with the goal of minimizing the sum of the squares of the differences between the observed responses (values of the variable being predicted) in the given dataset and those predicted by a linear function of a set of explanatory variables. Visually this is seen as the sum of the squared vertical distances between each data point in the set and the corresponding point on the regression line â€“ the smaller the differences, the better the model fits the data. The resulting estimator can be expressed by a simple formula, especially in the case of a single regressor on the right-hand side.\n\n\n*Example*\n\nA person with 10 years of experience is earning $100000. But the linear line at the bottom it acatually tells us where the person should be sitting according to the model in terms of salary.\n\nSo here there two observations they are:\n\n1. Actual observations : $y_i$\n2. Model observations : $\\hat{y_i}$\n\nThe difference between the Actual and modeled observation for the level of independent varable.\nto calculate the best fitting line we will take the sum distances between Actual and model then take some of the squares.\n\n*Formula* : SUM$(y_i - \\hat{y_i})^2$ --> min\n\nit will calculate the deference form each observation then draw the fitted line and that is called the ordinary least squares method.\n\n<https://en.wikipedia.org/wiki/Ordinary_least_squares>\n",
    "created" : 1490329958553.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1969613882",
    "id" : "F1F82971",
    "lastKnownWriteTime" : 1490347047,
    "last_content_update" : 1490347047892,
    "path" : "~/Machine Learning A-Z/Part 2 - Regression/Section 4 - Simple Linear Regression/simple linear regression.Rmd",
    "project_path" : "Part 2 - Regression/Section 4 - Simple Linear Regression/simple linear regression.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}